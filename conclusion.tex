\chapternonum{Conclusion}

%----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%tenses!!!!!!!!!!!
%-------------
%HIP


This thesis is divided into two parts, first part is dedicated to the Compact Muon Solenoid~(CMS) detector and especially the CMS silicon strip tracker. The second part presents a group of searches for supersymmetry~(SUSY) with the Run~2 CMS data. In the first part we study two different topics, which are related to highly ionizing particles (HIP) and simulation of the CMS tracker, respectively. 

Before the beginning of the Large Hadron Collider~(LHC) operation, the highly ionizing particles were identified to cause dead-time in cluster reconstruction leading to some hit inefficiency in the silicon strip tracker. In the beam tests of the tracker modules, both modules with iverter resistor value of 100~$\Omega$ and reduced value of 50~$\Omega$ were studied. It was shown that the dead-time decreases with resistance and therefore for modules with reduced inverter resistor value the mean dead-time is lower than for the nominal one. This observation lead to the decision to equip tracker modules with 50~$\Omega$ resistors instead of 100~$\Omega$. On the other hand, it was observed the decrease of the resistance enhanced the baseline distortions and therefore increased the number of reconstructed fake clusters. But this effect was found to cause only negligible increase in the strip ocupancy. 

When an increase of hit inefficiency in the strip tracker was observed in 2015-2016, a HIP was immediately identified as a possible explanation of it. Following this suggestion, I studied the HIP effect with collision events using a special data format in which no subtraction and suppression were applied at the back-end electronics level and therefore full information about all channels was available. In these data, I confirmed that the HIP induces a large charge on few channels and drives the remaining channels belonging to the same front-end electornics, the APV chip, towards lower charges up to the point when the signal is too small to be converted into light. Reaching this zero light level also induces a decreased spread of the charges on all channels within one APV when excluding the channels reading a large charge. With these data I was able to measure the rate of the HIP events, i.e. the number of events with a HIP interaction to all events, to be around $4 \times 10^{-3}$ for the first layer of Tracker Outer Barrel (TOB). I also evaluated the dead-time for this layer to be up to around 250~ns. In the studies at the PSI beam test, the mean and maximal dead-time for the TOB modules was evaluated to be 100~ns and 275~ns, respectively, therefore eround the vale estimated with the 2016 data.  

%TODO do I need the sentence starting "Reaching this" ?

However the HIP effect was not responsible for the observed inefficiencies and after the largest source of inefficiency was found and fixed, a new set of data provided an opportunity to study the HIP effect in a greater detail decomposed of the previous issue. Because of the data-taking conditions, these data permitted to perform a  measurement of the HIP probability in a cleaner environment. The HIP probability was computed per per pp interaction, i.e. rescaled by the fill peak pileup, in order to have a possibility to apply the probability on any LHC fill. This probability for each layer/wheel/ring of the silicon strip tracker was measured to be of the order of~$10^{-4}-10^{-3}$\% depending on the tracker layer/wheel/ring. The probabilty of the HIP events was already evaluated in the past, from simulations and beam test data, but these prebabilities were derived per particle and therefore they cannot easilly be compared to the current measurement. Moreover during the beam test, different particle composition and spectra were present compared to the CMS environment.

The measured HIP probability  $10^{-4}-10^{-3}$\% means that in case of the 250~ns dead-time and 25~ns bunch spacing, the pileup of the order of $10^4-10^5$ would cause that the APV chip is never fully efficient. The current pileup during the summer 2018 is of the order of 50 interactions per bunch crossing and therefore far from the fully inefficient chip scenario. Knowing the dead-time, the HIP probability and fixing the pilep to 31, which is the pileup of a representative run used for the 2018 hit efficiency measurement, it is possible to estimate the order of magnitude of the hit inefficiency resulting from HIP. The inefficiency was calculeted to be 0.1-1\% in agreement with the 2018 hit efficiency measurement.


Unfortunately, because of the data-taking conditions it was impossible to measure the dead-time. The first data provide better opportunity to measure dead-time, but in them it is diffcult to precisely fix the time of the HIP occurence. To measure dead-time, the best option would be to have data without any trigger rules, in order to have possibility to track given HIP from event to event. On top of it, it would be needed to reconsider the analysis strategy. For example as we would not be interested in the HIP probability anymore, we could spot the HIP event by presence of a large peak, which appears only for few ns and therefore should better fix the time of the HIP event. Moreover it would be needed to make sure, that the fake clusters from baseline distortions are not considered as real clusters and therefore it could seem that the chip is efficient. This should be partly solved by the tracking. 

These data also permitted to study the cluster charge, multiplicity and width  in presence of a HIP event or right after. I found out that the fake cluster multiplicity is increased after the HIP event compared to a standard event not influenced by a HIP. However the HIP probability is low and therefore in average the fake cluster multiplicity resulting from the HIP is of the same order of magnitude as the fake cluster multiplicity when no HIP had occurred. I observed that fake clusters induced by the HIP originate from the baseline distortions, i.e. the large spread in the channels charges. For the first layer of TOB, I also estimated a lower limit on the probability that a track is reconstructed with at least one fake cluster originating from a HIP event. This probability was computed to be 0.002\% and therefore is negligible.
 
%Both of the data-takings had its limitations. In the first one we had difficulties to measure the HIP probability, in the second one the deadtime. In case we 

%which kind of data would be perfect?
%	-theoretically the best option would be data without trigger rules. Then we measured probabilities but we miss deadtime. so we would need subdeetctors in to have tracking. And to link the track to the module with HIP. To measure dead-time we would need trains and to track a given module and asseswhn clusters start to be seen in that module with the help of tracking again. The analyiss would need to be more complex. (Think about that a bit more). Then there is still ambiguity in HIP selection - we would need to require HIP which is alreadys een in the first event in the train and then we would need to trigger all events in a time window (dead time estimated to 250ns, so at least this window)

The HIP probability cannot be decreased but we might investigate if the dead-time, and consequently the hit inefficiency, could be reduced. An option how to decrease a dead-time was identified and studied in the past~\cite{website:hitLoss}. This option is to maximize the pedestal subtracted data in order to decrease the chance that the channels charges are shifted beyond the measurable range. The pedestal is the mean strip activity when no particle is present and in the standard data-taking mode, not full pedestals are subtracted by the back-end electronics, but the pedestals minus offset, resulting in the positive mean channel charges after pedestal subtraction. This offset can be further maximized, but it was found out that this option only increases the fake cluster multiplicity without improving the hit efficiency. Moreover this option reduces the dynamic range, resulting in larger charges to be truncated faster. As this test was performed before the fix of the largest source of inefficiency, it could be an option to repeat it  now.

%dcommon powering scheme of inverters

%	-first studied the hi effect as a possible explanation of the inefficiencies
%	-first study with the CMS data, studied the evolution of baseline and raw digis standard deviation and with their help designed the selection of the HIP, it was found out that the HIP leads to the low baseline and low rms raw what was also observed in the 2nd study provided opportunity to study data not affecte dby the ineffeicincies
	%-the HIP effect affects the cluster charge and multiplicity and in the first study a decrease in the cluster multiplicity form average, i.e. dead time, is observed for aroun 250 ns.
	%-Due to the data-taking properties the deadtime cannot be studied in data after vfp fix, but with these data we have shown that 
	%there is an increased multiplicity of fake clusters after a HIP event. But in average this multiplicity is of the same order of magnitude as the cluster multiplicity of fake clusters when no HIP is present. We have also estimated a lower bound on the probability that a fake cluster from a HIP event is reconstructed into track to be 0.002\%. We observed that these fake clusters originate from the baseline distortiond induced by the HIP event. 
	%-We have computed that the hip probability per pileup for each layer/wheel/ring of the silicon strip tracker and it has been found to be of the order of~$10^{-4}-10^{-3}$\% depending on the tracker layer/wheel/ring. This means that in case of dead time for 250ns and bunch spacing of 25ns the PU of order of $10^4-10^5$  would cause that the APV chip is never fully efficient (ask prob of 10 events before have a HIP) and we are far from that now.

For the future upgrade of the silicon strip tracker for the High Luminosity LHC project (HL-LHC), we must pay attention to the HIP effect already at the design level. Ideally, it would be desired that the electronics is designed in a way that if a large charge is read by one channel, it does not affect the other channels belonging to the same chip as it is the case now. The HIP probability computed in this thesis is related to the module geometry and its distance from the interaction point and therefore cannot be easily extrapolated to the future detector. Assuming that the tracker would remain the same and for a pileup of around 200 interactions per bunch crossing, the HIP rate per chip can be expected to be of the order of $10^{-2}-10^{-1}$ \%. In the reallity, the detector will be upgraded and therefore the HIP probability and induced dead-time will change depending on the design of the tracker modules and the readout electronics.

   
%-HLLHC expected pu 140-200
 
%is there something to be improved
%	-different design of the APV chip. The common powering of inverters causes the drop o baseline but o the other hand it stabilizes the baseline. The resistance of the inverter resistor was changed in order to improve the sensitivity to the HIP event, but it enhances the instabilities of baseline, the basline distrotion, leading to a possible enhancement of the fake cluster reconstruction.

%does it need to be monitored
%	-the HIP probability is now low and tehrefore does not cause a large problems. But once the PU is increasing it can become dangerous, but as I mentioned the PU would have to be of the order of... which is far from teh expected PU at LHC.
 
%how to improve ?
%	-increase of lumi, increase of PU > for HL-LHC there is a development of new chips and therefore in their design we need to pay attention to the HIP effect (does somebody already thinking about that)

%could we prevent by changing baseline ref value ?
%	-the HIP was tried to be mitigated by maximizing CMN. But it was found out taht this maximization elads ony to the appearance of the so-called "anomalous cluster" (more details in HitEffLoss page). This check was though done before the APV change and should be redone. But the problem is that due to the HIP tehre are also fragments causing clusters plus fakes from distrotions which would not be normally reconstructed as they are below zero. So would it be the right thing to do?  %slide 27: https://indico.cern.ch/event/470862/contributions/1146979/attachments/1281240/1903454/common_mode_maximization.pdf

%will we suffer of this at HL-LHC ?
%	-we will ahve a HIP effect but it consequences, the induced dead time and probability will depend on the design of modules.

%something about the APV conditions?
%	-Preamp Feedback Voltage Bias (VFP) Change
%	-not much to say...


%the common biasing scheme of inverters -> stabilize baseline but xtalk effect -> could reduce the HIP problem - check 
%----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


%SIMU
The simulation of the CMS silicon strip tracker uses real tracker conditions in order to provide realistic results. These conditions change with the tracker operating conditions, e.g. the temperature, but also evolve as a result of tracker ageing, i.e. the radiation damage, for example the cross talk, i.e. the charge shared to the first and second neighboring strips. As the majority of these conditions was never updated since the beginning of the Run~1, we decided therefore to revise the simulation and study the impact of the outdated conditions on the simulation of the clusters in the tracker. I found out that the change of conditions (except noise and gains) has a negligible impact on the cluster properties in many cases, but the change of the cross talk parameters largely impacts the cluster width and seed charge.

After having identified that the cross talk parameters cause large discrepancies in distribution of the cluster shape between data and simulation, a cosmic data-taking period was arranged in absence of  magnetic field, dedicated to re-measure the cross talk parameters. As for the HIP studies, no subtraction and suppression of channels charges were done on the beck-end electronics level. In these data I spotted that the cross talk parameters evolve as a function of time when the particle arrives to a given module. Therefore the cross talk needs to be measured at the correct timing, i.e. as when a particle originating from the pp interaction would reach a given module. Due to the trigger conditions, there was only sufficient statistics to measure the cross talk parameters in the barrel. I measured that the cross talk decreased for all barrel geometries compared to the previous measurement. The sharing to the first neighboring strips in barrel decreased by around 18-27\%, the change being larger for geometries closer to the interaction point i.e. with larger fluence and consequently larger radiation damage. The sharing to the second neighboring strips diminished by around 24\%. To determine the cross talk parameters in disks and endcaps, I corrected the old cross talk parameters based on the change of the cross talk parameters in barrel and data to simualtion comparisons of the mean cluster seed charge in collision events. 


%But it is not know if previous measurement of cross talk paramters took into account the timing dependency and tehrefore it is not guaranteed that it was realistic.
	%- measurement done (differences between barrel and endcaps!) \& cross talk was found to decrease compared to the values originally int he MC
	%- example of TOB2 measurement before and after
	%- found evolution of xtalk as a function of time
	%-cross talk: Said somewhere that it was done last time in 2010 ?

The cross talk parameters influence profoundly the cluster shape, but not the total cluster charge. The change of cross talk might cause that the clusters which were slightly above the clustering threshold before do not have to pass it anymore now and therefore are not reconstructed anymore and vice versa. In addition, with the change of the cross talk parameters, the cluster position and its resolution change in simulation, resulting in small changes in tracking. Therefore the effect of a change of the cross talk may propagate up to the physics analysis, i.e. in searches for appearing/disappearing tracks. The object discriminators, mainly the b-tagging discriminators, which are strongly dependent on the tracking, are also influenced by it. But the impact is not large and for other physics objects and analyses, not largely dependent on tracking, it is expected to be negligible.

It was shown that the cross talk parameters evolve as a function of the fluence and therefore they should be remeasured and updated regularly. Also more frequent measurements of the cross talk parameters could help us to understand why the cross talk is decreasing. The literature is stating that with increasing fluence, the interstrip capacitance should increase and the interstrip resistance decrease. Both these changes should lead to the increase of cross talk, what was not observed. Note that the cross talk was measured only in the deconvolution mode, so it should be  updated for the peak mode as well. Nevertheless the peak mode is not used in standard data-taking, it might be important for some perticular studies, but mainly it could help us investigate the decrease of the cross talk, by disentangling the effects of the deconvolution from other effects.

The cosmic data recorded in absence of magnetic field, which were used for the cross talk measurement, posed several constraints and difficulties. First, due to the trigger conditions there is insufficient statistics in the disks and endcaps. Even if the trigger would be re-designed, due to the $\eta$ distribution of the cosmics, the data-taking would have to be long in order to collect sufficient statistics, which is difficult to arrange in the tight CMS schedule. The second large issue is related to the tracker timing when it samples the collected signal. The tracker has no special timing configuration for the cosmics and therefore the collision timing is always used.  As the cross talk depends on timing, we need to use for the cross talk measurement only cosmics which arrived to the given module at the same time as a particle produced in the pp collision. Another ambiguity comes from the computation of particle arrival time, which extrapolates all particles to the interaction point, even though they did not have to pass through there. Both the pseudorapidity coverage and the timing issues would be solved if it is possible to arrange a new data-taking of non suppressed collision data without magnetic field. But such data-taking is not compatible with the CMS program and priorities.
%The collision timing maximizes the cluster seed charge for particles created during the pp collision coming from the interaction point to the given module, and for cosmics no such maximization is done.


%does it need to be remeasured
%	- As shown the cross-talk evolves with fluence and tehrefore it must be remeasured from time to time.  Interesting feature is that the evolution is opposite than expected and therefore it would be interesting to monitor the cross talk in the vision of understanding the feature.

%do we need a different data
%	-the cruzet cosmics is good, but ideally we would need also some data from endcaps and this is difficult with cosmics as we would need quite a lot of time. The ideal case would be zero tesla collision data in the virgin raw, but this is scifi. These data would help to collect large stats everywhere, plus we would not have problems with timing.


%how can we improve the emasurement and the physics
%	-we could potentionally measure if there is left-right assymetry of cross talk (in past and this work observed average cluster (seed) charge dependency on the cluster position burt between 3); if the cross talk is dependent o the cluster position and track position within a sensor. But the description now is in general good and as mentioned there is only a small dependency of the tracking on the cross talk. In general only the physics analyses which are using tracking i.e. searches fro the appearing/disappearing tracks; or b-tagging discriminators are influenced.
%	-also we should update the cross talk in the peak data and check if the evolution is the same as in deco, the crosss talk decreases, this could give us a hint what is going on

After the update of the cross talk parameters and conditions such as gains and noise, which were updated by other members of the tracker local reconstruction group, the description by the simulation of the in-time clusters in data was largely improved. It was identified that there are still several parameters which are outdated and could be  updated in future, however these parameters do not largely change the cluster description. Because several parts in the simulation have only a negligible impact, i.e. the diffusion, it would be good to evaluate what parts of the simulation are really needed in order to reduce the time needed for the event simulation.

Several improvements could be still included in the simulation of the out-of-time clusters. Firstly, the pulse shape has changed due to the change of the APV parameter and therefore the pulse shape should be updated in order to reduce the simulated charge by a correct fraction. Secondly, it was shown that the cross talk depends on timing. The cross talk parameters were derived for in-time clusters and are not correct for out-of-time clusters. Ideally, the cross talk parameters should be parametrized as a function of the particle arrival time to the module. The out-of-time clusters need also to be well simulated as these clusters might be used by the tracking algorithm to reconstruct a track.

Few other interesting studies could be performed with the taken cosmic data and here I discuss few examples. In the past it was observed that there is a left-right asymmetry between the charge sharing. This effect could be as well studied with these data. Moreover, it is possible to study the cross talk and cluster properties as a function of the position where the track intercepts the strip plane, close and far from the strip. In the past it was also observed that there is an evolution of the cluster seed charge as a function of the strip number within one APV chip, this study could be repeated with these data. All these studies could help to better understand the formation of the signal, the features of the sensors such as non-uniformities in the electric field, and also the changes in the sensors properties reasulting from their irradiation. 


%Overall to prevent large discrepacnies between data and simulation, the paraaters and conditions in the simulation should be more frequently updated

%can we introduce some new things in simu
%	-for now we have achieved improvement and there are no things to be introduced into simu. After updating the conditions everything seems to be better described except the fraction of saturated clusters. This need to be resolved and changes in simulation might be done. On the other hand there are parts of simulation, where the effect is really small, e.g. diffucsion and if we would target the simplification of the code i order to reduce the simulation time, certainly few things could be simplified
 
%are there some problematic things in simualtion
% -in my opinion there is one large problem in simulation taht the parameters are not updated frequently. For some parameters it is not a problem, but for some it is crucial. For example the timing curve changed with the change of APV parameter and was not updated. The pulse shape largely influences teh clsuter properties and can pose really problems
%	- mainly the OOT will not be corectly described, firstly of incorrect pulse shape, but also because of the dependence of the cross talk on the particle arrival time. This could be considered for implementation in the smulation.
%	- then the puls shape is difefrent for the main strip and neighbor, what will cause also incorrect description of oot as we fixed it now for intime

%do we need to change something more frequently or completely because the sensor changes with radiation
%	- as shown except of cross talk also the noise and database conditions change fast with the aging of sensors.


%pulse shape problematics -> the OOT is not well described
%xtalk independent of time in simu
%	-already talked about


In the future tracker for the HL-LHC, the clustering will be largely different, only the binary information from the strip/macro-pixel will be sent, therefore the cluster seed charge and cluster charge will not be available and just the cluster width will be known. As the cluster width depends on cross talk, it will also be needed to mesure the cross talk in the new tracker. Moreover it will be important to determine and monitor the cross talk in order to design and maintain the threshold for strip/macro-pixel charge to be correctly set to one or zero.

%TODO: how is it with AC and DC coupling?

%measure it at HLLHC?
%	-for sure it needs to be determined what is teh cross talk and what it evolution will be (and if we are able to do so, measured from time to time) We need to make sure that the clusters do nto fail the binary criterium because they are too wide, because of too large cross talk. 
%	-soem sensors are going to be DC coupled, things will be different with xtalk
%	-ensure large interstrip resistance (think about xtalk during design) 

%what are the limitations ?
%why outdated ? talk about irradation
%	-fluence causes the aging of modules (defects, surface damage)

%Do we know why it was modified?
%	- not completely uderstood why xtalk is evolving in this way, but the evolution was expected due to surface damages on the sio-si interface

%How to understand the change of cross talk, which goes in opproite direction, can we disentangle it by more frequent measurements?
%	- it might be the cose but it does not have to

%Radiation Damage (https://indico.cern.ch/event/577879/contributions/2740332/attachments/1572803/2483960/CMS-OT_aldi.pdf) - slide 23 reference

%Bulk damage
%	Primary lattice defects (I and V) form higher order defects (V2, VO,...) or even defect clusters, with energy levels in the band gap of Si
%	Depending on energy level and cross section they contribute to leakage current, effective doping concentration, trapping

%Surface damage
%	-Ionizing radiation generates e/h pairs also in SiO2 
%	e much higher mobility than h -> positive charge up of oxide
%	Additional, interface traps with dynamic characteristics
%	Theses lead to increased surface currents, altered electric field in surface region, accumulation of electrons at surface


 %	- what remains unchanged - many parameters were found otdated but not updated week dependence -> should be some update later, but just for consistency

%----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


%SUSY
%motivation, conclusion
	%-the standard model of the particle physics was over years found to be excelently describing the nature, nut there are several shortcomings
	%-the most popular extension of sm du to its ability to addrss many shortcommings
%what was done and my contribution
	%-search for the supersymmetric partner of teh stop
	%-three posible decay channels, top neutralino, bottom chargino and mix of them
	%-search in one lepton final state + jets + met
	%- limits already at runI but runII due to cnage in energy and larger integrated luminosity enabled to probe stop masses far beyond the runI possibilities
	%-three analyses - my largest contribution to the full 2016
	%-the largest contribution to the full 2016 is estimation of the z to nunu background

%perspectives at HLLHC
%	-the relatively light stops already excluded
%	-the naturalness can be achieved with heavier ones therefore we must keep searching
%	-at the HL-LHC integrated lumi of 3000 fb-1 by CMS to be 2TeV
%	-further probing could be achieved by increase of the center of mass energy
%	-beyond this the more spohisticated analysis techniqus can eb designed to further probe the stop masses - like W or top tagging, new disriminating variables or machine learning teqniques

%perspectives:	What about the search at low mass ?
%perspectives:	How to improve ?
%erspectives:	How to interpret in a realistic model ? Do we have other strong constraints from other measurements/ serches ? ...

The second part of the thesis is discussing that the standard model (SM) of particle physics suffers from several shortcomings such asthe hierarchy problem or absence of the dark matter candidate, but in general over years that it proved to be capable to well describe the majority of observed physics phenomena. Due to these shortcomings theories beyond the SM were proposed and one of them, the supersymmetry, became the most promising one because of its ability to address large part of the SM issues. In this thesis I present searches for the supersymmteric partner of the top quark, the stop, with the CMS Run~2 data. These searches target the stop pair production, with three different possibilitis for the decays of the stops: the two stops decay to a top quark and a neutralino, or the two stops decay to a bottom quark and a chargino, or each stop decays differently as a mixture of the two previous cases. In all cases, the targeted signal final states contain one lepton, jets and missing transverse energy. 

I was involved in three publications of one search: one based on the data recorded in 2015 corresponding to an integrated luminosity $\int \mathcal{L}$ of 2.3~fb$^{-1}$, the second one based on the data recorded at the beginning of 2016 ( $\int \mathcal{L}=12.9$~fb$^{-1}$) and the last one corresponding to $\int \mathcal{L}= 35.9$~fb$^{-1}$ collected during the full 2016 pp data-taking period. I was mainly involved was in the last analysis, where I was responsible for the estimation of one of the SM backgrounds, in which a Z boson decays to two neutrinos. In this thesis I also exploited a technique for tagging of merged jets originating from a W boson and I showed that the gain of implementing such technique is growing with the integrated luminosity. 

The searches for the stop pair production in different final states were already performed with the Run~1 data. The analyses exclude stop masses in terms of simplified model spectra with stops decaying to  atop quark and a neutralino up to around 755 GeV for a neutralino mass below 200 GeV. With the increase of the center of mass energy and then also the integrated luminosity, it was soon possible to further probe the stop masses. No excess was found in the full 2016 data ($\int \mathcal{L}=35.9$~fb$^{-1}$) with respect to the background and therefore exclusion limits were derived. This analysis excluded the stop masses up to 1120 GeV for a massless neutralino in terms of simplified model spectra where both stops decay to a top quark and a neutralino. In the case where both stops decay to a bottom quark and a chargino or in the case of mixed decay, the stop masses were excluded up to 1000 GeV  and 980 GeV, respectively.

Despite the stop exclusion in the TeV range, there is still room for the natural supersymmetry and therefore the effort to search for the stops is not diminishing. According to the CMS predictions, with an integrated luminosity of 3000~fb$^{-1}$ envisioned to be collected at the HL-LHC, it will be possible to probe the stop masses up to 2~TeV. An upgrade of the LHC to higher center of mass energies would permit us to go even further in the stop masses. In addition to an increas of the center of mass energy and the integrated luminosity, we can further improve the discovery potential by optimizing the analyses, for example with the use of the discussed tagging of W and other jets, machine learning categorization of events, and new discriminating variables.

In the CMS a large variety of SUSY searches is performed. The stops are being sought in several differenet final states and the other predicted SUSY particles are targeted by the searches as well. Several searches also allow violation of the R-partity. In general the SUSY searches are interpreted in terms of simplified spectra. A lot of effort is being done to reinterpret these searches in more sophisticated models. The CMS, and the collider experiments in general, do not provide the sole opportunity to search for the supersymmetry. For example, the supersymmetry can also be studied via direct dark matter measurement experiments.

%----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
