\chapternonum{Abstract}


%This thesis started in October 2015, the same year as the beginning of the Run~2 data-taking period (2015-2018) during which  the center-of-mass energy, the instantaneous luminosity and the bunch crossing frequency was increased compared to the Run~1 (2008-2013). Consequently, the possibilities for physics analyses were largely extended with a price of a larger fluence on the detector side, especially on the tracker which is the inner subdetector. With this increasing fluence, the detector suffers from a larger irradiation which could be a cause of performance issues. Moreover, the detector is also ageing with irradiation, leading to a change of some of its characteristics. Therefore it is very important to monitor closely the detector, to keep its stable performance and consequently the potential physics reach.

%Particle physics is described by the standard model whose  last piece, the Higgs boson, was discovered in 2012 by the CMS and ATLAS collaborations. Although the standard model is now complete and in general describes excellently the physics phenomena, it suffers from several shortcomings. This issue makes us believe that the standard model is an effective theory at low energy of a more fundamental theory which is to be determined. Over years, many theories  were proposed and one, referred to as supersymmetry, became of a special interest due to its capability to address many of the standard model shortcomings. 

%Supersymmetry introduces a new partner to each standard model particle and therefore extensive searches for these particles have been performed by the CMS collaboration as well as other collaborations. One of these particles is the supersymmetric partner of the top quark, the stop, which is expected to have a mass around 1~TeV in natural supersymmetry and therefore be accessible at the LHC energies. No evidence for the stops was  found in Run~1, but the increase in luminosity as well as the center-of-mass energy in Run~2 allows us to probe the stop masses beyond the Run~1 exclusion. 

Cette thèse a commencé en octobre 2015, la même année que le début de la période de collecte de données Run~2 (2015-2018) au cours de laquelle l’énergie dans le centre de masse, la luminosité instantanée et la fréquence des collisions a augmenté par rapport à Run~1 (2008-2013). Par conséquent, les possibilités d'analyses physiques ont été largement étendues avec le prix d'une plus grande fluence du côté du détecteur, en particulier sur le tracker qui est le détecteur interne. Avec cette fluence croissante, le détecteur souffre d’une irradiation plus importante qui pourrait être à l’origine de problèmes de performance. De plus, le détecteur vieillit également avec l'irradiation, entraînant une modification de certaines de ses caractéristiques. Par conséquent, il est très important de surveiller de près le détecteur, afin de conserver ses performances stables et pas d'impact sur les analyses de physique.

La physique des particules est décrite par le modèle standard dont la dernière pièce, le boson de Higgs, a été découverte en 2012 par les collaborations CMS et ATLAS. Bien que le modèle standard soit maintenant complet et décrive en général très bien les phénomènes physiques, il souffre de plusieurs défauts. Ce problème nous fait croire que le modèle standard est une théorie efficace à basse énergie d’une théorie plus fondamentale à déterminer. Au fil des ans, de nombreuses théories ont été proposées et l'une d'entre elles, appelée supersymétrie, a suscité un intérêt particulier en raison de sa capacité à répondre à de nombreuses lacunes du modèle standard.

Supersymétrie introduit un nouveau partenaire pour chaque particule de modèle standard et, par conséquent, des recherches approfondies sur ces particules ont été effectuées par la collaboration CMS, ainsi que par d’autres collaborations. L'une de ces particules est le partenaire supersymétrique du quark top, le stop, qui devrait avoir une masse d'environ 1~TeV en supersymétrie naturelle et donc être accessible aux énergies du LHC. Aucune preuve des stops n'a été trouvée dans Run~1, mais l'augmentation de la luminosité ainsi que l'énergie dans le centre de masse dans Run~2 nous permettent de sonder les masses de stop au-delà de l'exclusion de Run~1.

\vspace*{1cm}

%The description of the Compact Muon Solenoid~(CMS) detector is given in the first chapter, Chapter~\ref{sec:detch}, together with a brief introduction of the Large Hadron Collider. This chapter focuses on the silicon strip tracker, whose deeper understanding is required for the following chapters. This chapter also presents the reconstruction of the physics objects corresponding to particles passing through the detector. In following Chapter~\ref{sec:HIPch}, I was interested in the highly ionizing particles in the silicon strip tracker. Already before the beginning of the Large Hadron Collider~(LHC) operation, the highly ionizing particles were studied and identified to cause dead-time in the front-end electronics leading to a hit inefficiency in the silicon strip tracker. In the beam tests of the tracker modules, it was found that by decreasing the inverter resistor value of the front-end electronics,  the dead-time can be shortened as shown in Table~\ref{tab:tableDeadtimes2} and therefore the design of the electronics was adjusted to mitigate this issue. Later no analysis with LHC collisions had been performed yet. So when an increase of the hit inefficiency in the strip tracker was observed in 2015-2016, a HIP was immediately identified as a possible explanation of it. I studied the HIP effect with collision events recorded in 2016 using a special data format in which no subtraction and suppression were applied at the back-end electronics level and therefore full information about all channels was available. They allowed me to  confirm that the HIP induces a large charge on few channels and drives the remaining channels belonging to the same front-end electronics, the APV chip, towards lower charges up to the point when the signal is too small to be converted into light what can be observed in Fig.~\ref{fig:figures/peakinmodule}. Reaching this zero light level also induces a decreased spread of the charges on all channels within one APV, when excluding the channels reading a large charge. With these data I was able to measure the rate of the HIP events, i.e. the number of events with a HIP interaction to all events, to be around $4 \times 10^{-3}$ for the first layer of Tracker Outer Barrel~(TOB). I also evaluated the dead-time for this layer to be up to around 250~ns. It is of the same order as the dead-times evaluated in  previous studies performed at the PSI beam test, namely a  mean and maximal dead-times for the TOB modules of 100~ns and 275~ns, respectively.  

La description du Compact Muon Solenoid~(CMS)  détecteur est donnée dans le premier chapitre, Chapitre~\ref{sec:detch}, avec une brève introduction du Large Hadron Collider~(LHC). Ce chapitre se concentre sur le tracker à bande de silicium, dont la compréhension approfondie est requise pour les chapitres suivants. Ce chapitre présente également la reconstruction des objets physiques correspondant aux particules traversant le détecteur. En suivant le Chapitre~\ref{sec:HIPch}, je me suis intéressée aux particules hautement ionisantes~(HIP) dans le tracker à bande de silicium. Déjà avant le début de l'opération Large Hadron Collider, les particules hautement ionisantes ont été étudiées et identifiées pour causer un temps mort dans l'électronique frontale, entraînant une inefficacité dans le tracker à bande de silicium. Dans les tests de faisceau des modules tracker, il a été constaté que, en diminuant la valeur de la résistance de résisteur de inverter de l’électronique frontale, le temps mort peut être diminué comme indiqué dans le Tableau~\ref{tab:tableDeadtimes2} et donc le désign de l'électronique a été ajustée pour atténuer ce problème. Plus tard, aucune analyse avec des collisions du LHC n'a encore été réalisée. Ainsi, lorsqu'une augmentation de l'inefficacité du hit dans le tracker a été observée en 2015-2016, un HIP a immédiatement été identifié comme une explication possible. J'ai étudiée l'effet HIP avec des événements de collision enregistrés en 2016 en utilisant un format de données spécial dans lequel aucune soustraction et suppression n'a été appliquée au niveau de l'électronique arrière-plan et donc des informations complètes sur tous les canaux étaient disponibles. Ils m'ont permis de confirmer que le HIP induit une charge importante sur quelques canaux et pilote les canaux restants appartenant à la même électronique frontale, la puce APV, vers des charges plus faibles jusqu'au moment où le signal est trop petit pour être converti en lumière ce qui peut être observé sur la Fig.~\ref{fig:figures/peakinmodule}. Atteindre ce niveau de lumière zéro induit également une réduction des différences entre charges des tous les canaux dans un APV, en excluant les canaux lisant une charge de HIP. Grâce à ces données, j'ai pu mesurer le taux d'événements HIP, c'est-à-dire le nombre d'événements ayant une interaction HIP  à  tous les événements, autour de 4 $\times 10^{-3} $ pour la première couche de Tracker Outer Barrel~(TOB). J'ai également évalué le temps mort pour cette couche à environ 250 ~ ns. Il est du même ordre que les temps morts évalués dans des études antérieures effectuées au test du faisceau PSI, à savoir un temps mort moyen et maximal pour les modules TOB de 100~ns et 275~ns, respectivement.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Sensor type and $R_{inv}$~[$\Omega$] & $\Gamma_{mean}$~[ns]  & $\Gamma_{max}$~[ns] \\
\hline
\hline
TIB 100 $\Omega$ & 99.5 $\pm$ 12.0 & 200 $\pm$ 25 \\
TIB 50  $\Omega$ & 69.6 $\pm$ 9.4 & 250 $\pm$ 25 \\
TOB 100  $\Omega$ & 122.5 $\pm$ 12.6 & 275 $\pm$ 25 \\
TOB 50 $\Omega$  & 100.5 $\pm$ 3.6 & 275 $\pm$ 25 \\
\hline
TIB $\Gamma_{mean}(50~\Omega )/\Gamma_{mean}(100~\Omega)$ &  0.70 $\pm$ 0.13  & \\
TOB $\Gamma_{mean}(50~\Omega )/\Gamma_{mean}(100~\Omega)$ &  0.82 $\pm$ 0.09 & \\
\hline
\end{tabular}
%\caption[Table caption text]{The mean~($\Gamma_{mean}$) and maximum~($\Gamma_{max}$) dead-time of the APV chip induced by the HIP events for fully suppressed~($\sigma_{raw}<1$~ADC) baseline events. The dead-times were evaluated for two different module geometries~(TIB or TOB) as well as for two inverter resistor values~(100 or 50~$\Omega$). These results were obtained with PSI beam test data~\cite{Bainbridge:2004jc}. }
\caption[Table caption text]{La moyenne~ ($\Gamma_{mean} $) et le maximal temps mort~($\Gamma_{max}$) de la puce APV induit par les événements HIP pour les baselines qui sont entièrement supprimé~($\sigma_{raw} <1 $~ADC). Les temps morts ont été évalués pour deux géométries de module différentes~(TIB ou TOB) ainsi que pour deux valeurs de résistance d'inverter~(100 ou 50 ~ $\Omega$). Ces résultats ont été obtenus avec des données de test de faisceau PSI~\cite{Bainbridge: 2004jc}.}
\label{tab:tableDeadtimes2}
\end{center}
\end{table}


    \insertFigure{figures/peakinmodule}
                 {0.47}
                 {Exemple de distribution de raw digis~(rose), piédestal soustrait digis~(bleu), baseline soustrait digis~(rouge) et grappes~(vert) en fonction du numéro de bande dans un module. Le troisième APV dans le module montre un comportement induit par un événement HIP: une faible variation de charge pour les raw digis et un grand signal observé pour quelques canaux.}       % Width, in fraction of the whole page width
                 %{Example of distribution of raw digis~(pink), pedestal subtracted digis~(blue), baselines~(red) and clusters~(green) as a function of the strip number in one module. The third APV in the module shows a behavior induced by a HIP event: a low charge variation for the suppressed raw digis and a large observed signal for few channels.} 

%However the HIP effect was not responsible for the observed inefficiencies and after the largest source of inefficiency was found and fixed, a new set of data provided an opportunity to study the HIP effect in a greater detail disentangled of the previous issue. Because of the data-taking conditions, these new data permitted to perform a  measurement of the HIP probability in a cleaner environment. The HIP probability was computed per pp interaction, i.e. rescaled by the peak pileup of the fill, in order to have the possibility to apply this probability to any LHC fill. The HIP probability in the silicon strip tracker, shown in Fig.~\ref{fig:figures/probPerPU2}, was measured to be of the order of~$10^{-4}-10^{-3}$\% depending on the tracker layer/wheel/ring. The HIP probabilities, already evaluated in the past from simulations and using beam test data, were derived per particle and therefore they cannot easily be compared with the current measurement. Moreover during the beam test, a different particle composition and energy spectra were present compared to the CMS environment.

Cependant, l'effet HIP n'était pas responsable des inefficacités observées et, une fois que la plus grande source d'inefficacité a été trouvée et corrigée, un nouvel ensemble de données a permis d'étudier l'effet HIP de manière plus détaillée. En raison des conditions de prise de données, ces nouvelles données ont permis de mesurer la probabilité HIP dans un environnement plus propre. La probabilité HIP a été calculée par interaction pp, c'est-à-dire redimensionnée par le pileup de pointe, afin de pouvoir appliquer cette probabilité à tout remplissage LHC. La probabilité de HIP dans le tracker de bande de silicium, représentée sur la Fig.~\ref{fig:figures/probPerPU2}, a été mesurée de l'ordre de ~10$^{-4} -- 10^{-3}$\% selon sur la couche/roue/anneau. Les probabilités HIP, déjà évaluées par le passé à partir de simulations et utilisant des données de test de faisceau, ont été dérivées par particule et ne peuvent donc pas être facilement comparées à la mesure actuelle. De plus, lors du test de faisceau, une composition de particules et des spectres d'énergie différents étaient présents par rapport à l'environnement CMS.

    \insertTwoFigures{figures/probPerPU2}
                 {figures/probFinalLayerPU} % Filename = label
                 {figures/probFinalPURings} % Filename = label
                 {0.47}       % Width, in fraction of the whole page width
                 { Probabilité moyenne d'un événement HIP par pileup~(PU) pour les couches (gauche, droite) de TIB, TOB et roues (gauche) ou anneaux (droite) des partitions TID et TEC du tracker de silicium, calculées à partir du cycle de données 281604.}
                 %{The average probability of a HIP event per PU for layers (left, right) of TIB, TOB, and wheels (left) or rings (right) of the TID and TEC partitions of the silicon strip tracker, computed from the data run 281604.}

%The HIP probability itself cannot be decreased but we might investigate if the dead-time, and consequently the hit inefficiency, could be reduced. An option how to decrease the dead-time was identified and studied in the past~\cite{website:hitLoss}. This option is to maximize the pedestal subtracted data in order to decrease the chance that the channel charges are shifted beyond the measurable range. The pedestal is the mean strip activity when no particle is present and in the standard data-taking mode, not pedestals themselves are subtracted by the back-end electronics, but the pedestals minus a fixed offset, resulting in the positive mean channel charges after this subtraction. This offset can be further maximized, but it was found out that this option only increases the fake cluster multiplicity without improving the hit efficiency. Moreover this option reduces the dynamic range, resulting in larger charges to be truncated faster. However this test was performed before the fix of the largest source of inefficiency and it could be worth  to repeat it  now.

La probabilité HIP elle-même ne peut pas être réduite, mais nous pourrions rechercher si le temps mort et, par conséquent, l’inefficacité de hit, pourraient être réduits. Une option permettant de réduire le temps mort a été identifiée et étudiée dans le passé ~\cite{website: hitLoss}. Cette option permet de maximiser les données piédestal soustraites afin de réduire le risque que les charges du canal soient décalées au-delà de la plage mesurable. Le piédestal est l’activité de bande moyenne quand aucune particule n’est présente et dans le mode de prise de données standard, mais les piédestal eux-mêmes ne sont pas soustraits par l’électronique back-end, mais les piédestaux moins un décalage fixe. Ce décalage peut encore être optimisé, mais il a été constaté que cette option ne fait qu’accroître la multiplicité des clusters fausses  sans améliorer l’efficacité des hit. De plus, cette option réduit la plage dynamique, ce qui se traduit par des charges plus importantes à tronquer plus rapidement. Cependant, ce test a été effectué avant la résolution de la plus grande source d’inefficacité et il pourrait être utile de le répéter maintenant.

%A measured HIP probability of $10^{-4}-10^{-3}$\% means that in case of a dead-time of 250~ns obtained from the first data and bunch spacing of 25~ns, an APV chip would never be fully efficient if the pileup were of the order of $10^4-10^5$. The current pileup during the summer 2018 is of the order of 40 interactions per bunch crossing and therefore far from the fully inefficient chip scenario. The inefficiency resulting from the HIP was recently calculated, using the dead-time of 250~ns and the measured HIP probability, to be of the order of 0.1-1\%, for a pileup of 30 as in the representative run used for the 2018 hit efficiency measurement. This is of similar order as the measured 2018 hit inefficiency. Therefore the main source of the measured inefficiency now seems to come from the HIP effect. This inefficiency is not considered during the event simulation. 


Une probabilité HIP mesurée de $10^{-4}-10^{-3}$ signifie que dans le cas d'un temps mort de 250~ns obtenu à partir des premières données et de l'espacement des paquets des particules de 25~ns, une puce APV ne serait jamais pleinement efficace si le pileup était de l'ordre de $10^4 - 10^5$. Le pileup actuel au cours de l'été 2018 est de l'ordre de 40 interactions par événement et donc loin du scénario de puce totalement inefficace. L'inefficacité résultant du HIP a récemment été calculée, en utilisant le temps mort de 250~ns et la probabilité HIP mesurée, de l'ordre de 0.1-1\%, pour un pileup de 30 comme dans le run représentative utilisé pour le 2018 mesure de l'efficacité de hit. Cet ordre est similaire à l'inefficacité de 2018 mesurée. Par conséquent, la principale source de l’inefficacité mesurée semble désormais provenir de l’effet HIP. Cette inefficacité n'est pas prise en compte lors de la simulation d'événement.

%The second set of data also permitted to study the cluster charge and multiplicity, presented in Fig.~\ref{fig:figures/avClusterMultiplicitySecondT2}, in presence of a HIP event or right after. I found out that the fake cluster multiplicity is increased after the HIP event compared to a standard event not influenced by a HIP. However the HIP probability is low and therefore in average over the runs the fake cluster multiplicity resulting from the HIP is of the same order of magnitude as the fake cluster multiplicity when no HIP had occurred. I observed that fake clusters induced by the HIP originate from the baseline distortions. For the first layer of TOB, I also estimated a lower limit on the probability that a track is reconstructed with at least one fake cluster originating from a HIP event. This probability was computed to be 0.002\% and therefore is negligible. These fake clusters are not included in the simulation either, but as discussed, their impact is negligible and therefore it is reasonable to omit them from simulation.

Le deuxième ensemble de données a également permis d'étudier la charge et la multiplicité des grappes, présentées dans la Fig.~\ref{fig:figures/avClusterMultiplicitySecondT2}, en présence d'un événement HIP ou juste après. J'ai découverte que la multiplicité des clusters fausses est augmentée après l'événement HIP par rapport à un événement standard non influencé par un HIP. Cependant, la probabilité HIP est faible et, par conséquent, en moyenne pendant le run, la multiplicité de grappes fausses résultant du HIP est du même ordre de grandeur que la multiplicité de grappe fausses en l'absence de HIP. J'ai observé que les fausses grappes induites par le HIP proviennent des distorsions de baseline. Pour la première couche de TOB, j'ai également estimé une limite inférieure sur la probabilité qu'une piste soit reconstruite avec au moins un faux cluster provenant d'un événement HIP. Cette probabilité a été calculée pour être 0.002 \% et est donc négligeable. Ces fausses grappes ne sont pas non plus incluses dans la simulation, mais comme on l’a vu, leur impact est négligeable et il est donc raisonnable de les omettre de la simulation.


    \insertTwoFigures{figures/avClusterMultiplicitySecondT2}
                 {figures/avClusterMultiplicitySecond} % Filename = label
                 {figures/avClusterChargeSecond} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 {La multiplicité de grappe moyenne (gauche) et la charge de grappe moyenne (droite) en fonction du nombre de paquet croisement pour le run 281604. En triangles roses, la multiplicité de grappes moyenne de l’événement HIP et les événements plus tard tandis que les carrés bleus sont pour les événements non-HIP. }
                 %{ The average cluster multiplicity (left) and average cluster charge (right) as a function of the bunch crossing number for the run 281604. In pink triangles is the average cluster multiplicity of the HIP event and events later while the blue squares are for the non-HIP events.  } % Caption



%Unfortunately, because of the data-taking conditions of the second data-taking period, it was impossible to measure the dead-time in them. The first data provided a better opportunity to measure the dead-time, but with the  difficulty to precisely fix the time of the HIP occurrence. There are not any simple options of data-taking conditions suitable to  measure the dead-time in future, in this paragraph I propose two more complex ideas of  data-takings which could be considered. The first option would be to arrange data-taking in which events would be recorded every 25~ns during at least 15 bunch crossings not respecting any trigger rule, in order to have the possibility to track a given HIP in time. On top of it, it would be needed to reconsider the analysis strategy. For example, as we would not be interested in the HIP probability anymore, we could spot the HIP event by presence of a large peak, which appears only for few ns and therefore should better fix the time of the HIP event. But this approach could overload the system and therefore it would be needed to evaluate if such data-taking would be reasonable and how we could possibly avoid the overload. The second option would be to have a special fill and partially violate the trigger rules as well. In such fill we would need trains of two bunches and in each train we would need different time gap between the two bunches: in the first one 25~ns, the second one 50~ns, the third one 75~ns, etc. In these data we would have the time of HIP event fixed by the first bunch crossing in train and in the second one we could measure the hit efficiency. The dead-time would be the time gap between two bunch crossings in the same train in which the same hit efficiency is measured. In both these proposed data-takings, it would be needed to make sure, that the fake clusters from baseline distortions are not considered as real clusters by tracking and therefore that the fake clusters do not artificially increase the hit efficiency. 

Malheureusement, en raison des conditions de prise de données de la deuxième période de collecte de données, il était impossible de mesurer le temps mort. Les premières données ont fourni une meilleure occasion de mesurer le temps mort, mais avec la difficulté de déterminer précisément le temps de l'occurrence du HIP. Il n’existe pas d’options simples de conditions de prise de données permettant de mesurer le temps mort à l’avenir. Dans ce paragraphe, je propose deux idées plus complexes de prises de données qui pourraient être envisagées. La première option consisterait à organiser la prise de données dans laquelle les événements seraient enregistrés tous les 25~ns pendant au moins 15 croisements de paquet, sans respecter aucune règle de déclenchement, afin de pouvoir suivre un HIP donné dans le temps. De plus, il serait nécessaire de reconsidérer la stratégie d’analyse. Par exemple, comme nous ne serions plus intéressés par la probabilité HIP, nous pourrions repérer l’événement HIP en présence d’un grand pic, qui n’apparaît que pour quelques instants et devrait donc mieux déterminer le temps de l’événement HIP. Mais cette approche pourrait surcharger le système et il faudrait donc évaluer si une telle prise de données serait raisonnable et comment nous pourrions éventuellement éviter la surcharge. La deuxième option serait d'avoir un remplissage spécial et de violer partiellement les règles de déclenchement également. Dans un tel remplissage, nous aurions besoin de trains de deux paquets et dans chaque train, nous aurions besoin d'un intervalle de temps différent entre les deux paquets: dans le premier 25~ns, le second 50~ns, le troisième 75~ns, etc. Dans ces données, nous aurions le temps de l'événement HIP fixé par la première croisement en train et dans la seconde, nous pourrions mesurer l'efficacité de hit. Le temps mort serait l'intervalle de temps entre deux croisements de paquets dans le même train dans lequel la même efficacité de hit est mesurée. Dans ces deux recueils de données proposés, il serait nécessaire de s'assurer que les fausses grappes issues des distorsions de baseline ne sont pas considérées comme de bonnes grappes par traçage et que, par conséquent, les fausses grappes n'augmentent pas artificiellement l'efficacité.

%In the future upgrade of the silicon strip tracker for the High Luminosity LHC project (HL-LHC), we must pay attention to the HIP effect already at the design level. Ideally, it would be desired that the electronics is designed in a way that if a large charge is read by one channel, it does not affect the other channels belonging to the same chip as it is the case now. The HIP probability computed in this thesis is related to the module geometry and its distance from the interaction point and therefore cannot be easily extrapolated to the future detector. Assuming that the tracker would remain the same and for a pileup of around 200 interactions per bunch crossing, the HIP rate per chip could be expected to be of the order of $10^{-2}-10^{-1}$ \%. In the reality, the detector will be upgraded and therefore the HIP probability and induced dead-time will change depending on the design of the tracker modules and the readout electronics.

Dans la future mise à niveau du tracker à bande de silicium pour le projet High Luminosity LHC (HL-LHC), nous devons prêter attention à l’effet HIP dès la conception. Idéalement, il serait souhaitable que l’électronique soit conçue de telle manière que si une charge importante est lue par un canal, cela n’affecte pas les autres canaux appartenant à la même puce que c’est le cas actuellement. La probabilité HIP calculée dans cette thèse est liée à la géométrie du module et à sa distance par rapport au point d'interaction et ne peut donc pas être facilement extrapolée au futur détecteur. En supposant que le tracker resterait le même et pour un pileup d'environ 200 interactions par croisement de paquets, le taux de HIP par puce pourrait être de l'ordre de 10$^{-2}-10 ^{-1}$\% . En réalité, le détecteur sera mis à niveau et, par conséquent, la probabilité HIP et le temps mort induit changeront en fonction de la conception des modules de trajectographe et de l’électronique de lecture.

\vspace*{1cm}

%SIMU


The third chapter, Chapter~\ref{ch:simu} focuses on the  simulation of the CMS silicon strip tracker and the tracker conditions used in simulation in order to provide realistic results. These conditions change with the tracker operating conditions, e.g. the temperature, but also evolve with tracker ageing resulting from the radiation damage, for example the cross talk, i.e. the charge shared with the neighboring strips. As the majority of these conditions was never updated since the beginning of the Run~1, we decided therefore to revise the simulation and study the impact of the outdated conditions on the simulation of the clusters in the tracker. I found out that the change of conditions (except noise and gains) has a negligible impact on the cluster properties in many cases, but the change of the cross talk parameters largely impacts the cluster width and seed charge.

After having identified that the cross talk parameters cause large discrepancies in cluster shape distributions between data and simulation as shown in Fig.~\ref{fig:figures/seedwidthTOBXT2}, a cosmic data-taking period was arranged in absence of magnetic field, dedicated to re-measure the cross talk parameters. As for the HIP studies, no subtraction and suppression of channel charges were applied on the back-end electronics level. In these data I spotted that the cross talk parameters evolve as a function of the time when the particle arrives to a given module. Therefore the cross talk needs to be measured at the correct timing, i.e. as when a particle originating from the pp interaction would reach a given module. Due to the trigger conditions, there was only sufficient statistics to measure the cross talk parameters in the barrel. The newly measured parameters together with the current parameters used in simualtion are shown in Table~\ref{tab:measuredXtalk2}. I measured that the cross talk decreased for all barrel module geometries compared to the previous measurement. The sharing to the first neighboring strips in barrel decreased by around 18-27\%, the change being larger for geometries closer to the interaction point i.e. with larger fluence and consequently larger radiation damage. The sharing to the second neighboring strips diminished by around 24\%. To determine the cross talk parameters in disks and endcaps, I corrected the old cross talk parameters based on the change of the cross talk parameters in barrel and data to simulation comparisons of the mean cluster seed charge in collision events. The results of cross talk measurement in disks and endcaps are summarized in Table~\ref{tab:measuredXtalkTODTEC2}. Updating the cross talk parameters in simulation by newly measured values, we found out that the newly measured parameters improved largely description of cluster shape in data by simulation as shown on example in Fig.~\ref{fig:figures/widthTOBXTm2}.

    \insertTwoFigures{figures/seedwidthTOBXT2} % Filename = label %TDO continue here
                 {figures/clusterwidthTOBl1to4XT}
                 {figures/clusterseedchargeRescaledTOBl1to4XT} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { Distribution of the on-track cluster width (left) and cluster seed charge (right) in data and simulation for the OB2 geometry and for different values of the cross talk $XT$. The first of the three XT numbers corresponds to the fraction of charge induced on the seed strip, the second and third number represent the fractions of charge induced on each first and second neighboring strips, respectively. The simulated distributions are rescaled to the number of clusters in data. The bottom plot represents the data to simulation ratios. }

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Geometry & Type & $x_{0}$ & $x_{1}$ & $x_{2}$ \\
\hline
\hline
IB1 & current measurement & $ 0.836 \pm 0.009 $ & $0.070 \pm 0.004 $ & $0.012 \pm 0.002 $ \\
IB1 & currently in simulation & $ 0.775 $ & $ 0.096 $ & $0.017 $  \\
\hline
IB2 &  current measurement & $0.862 \pm 0.008 $ & $0.059 \pm 0.003 $ & $0.010 \pm  0.002 $  \\
IB2 & currently in simulation &  $0.830 $ & $0.076 $ & $ 0.009$   \\
\hline
OB2 &  current measurement & $0.792 \pm 0.009 $ & $0.083 \pm 0.003 $ & $0.020 \pm 0.002$  \\
OB2 & currently in simulation &   $0.725 $ & $0.110 $ & $ 0.027 $  \\
\hline
OB1 &  current measurement &  $0.746 \pm 0.009 $ & $0.100 \pm 0.003 $ & $0.027 \pm 0.002 $  \\
OB1 & currently in simulation &  $0.687 $ & $0.122 $ & $ 0.034 $ \\
\hline
\end{tabular}
\caption[Table caption text]{The cross talk measured in 2018 CRUZET VR data and the cross talk values used currently in simulation for barrel geometries. }
\label{tab:measuredXtalk2}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Geometry & Type & $x_{0}$ & $x_{1}$ & $x_{2}$ \\
\hline
\hline
W1a &  new values & 0.8571 & 0.0608 & 0.0106 \\
W1a &  currently in simulation & 0.786 & 0.093 & 0.014 \\
\hline
W2a &  new values & 0.8861 & 0.049 & 0.008 \\
W2a &  currently in simulation & 0.7964 & 0.0914 & 0.0104 \\
\hline
W3a &  new values & 0.8984 & 0.0494 & 0.0014 \\
W3a &  currently in simulation & 0.8164 & 0.09 & 0.0018 \\
\hline
W1b &  new values & 0.8827 & 0.0518 & 0.0068 \\
W1b &  currently in simulation & 0.822 & 0.08 & 0.009 \\
\hline
W2b &  new values & 0.8943 & 0.0483 & 0.0046 \\
W2b &  currently in simulation & 0.888 & 0.05 & 0.006 \\
\hline
W3b &  new values & 0.8611 & 0.0573 & 0.0121 \\
W3b &  currently in simulation & 0.848 & 0.06 & 0.016 \\
\hline
W4 &  new values & 0.8881 & 0.0544 & 0.0015 \\
W4 &  currently in simulation & 0.876 & 0.06 & 0.002 \\
\hline
W5 &  new values & 0.7997 & 0.077 & 0.0231 \\
W5 &  currently in simulation & 0.7566 & 0.0913 & 0.0304 \\
\hline
W6 &  new values & 0.8067 & 0.0769 & 0.0198 \\
W6 &  currently in simulation & 0.762 & 0.093 & 0.026 \\
\hline
W7 &  new values & 0.7883 & 0.0888 & 0.0171 \\
W7 &  currently in simulation & 0.7828 & 0.0862 & 0.0224 \\
\hline
\end{tabular}
\caption[Table caption text]{The updated cross talk for rings of TID and TEC. }
\label{tab:measuredXtalkTODTEC2}
\end{center}
\end{table}


    \insertTwoFigures{figures/widthTOBXTm2} % Filename = label %TDO continue here
                 {figures/clusterwidthTOBl1to4XTm}
                 {figures/clusterseedchargeRescaledTOBl1to4XTm} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { Distribution of the on-track cluster width (left) and seed charge (right)  in data and simulation for the OB2 geometry, for the current (default) and newly measured (updated) cross talk parameters~($XT$).  The simulated distributions are rescaled to the number of clusters in data.  The bottom plots represent the data to simulation ratios. }


The cross talk parameters influence profoundly the cluster shape, but not the total cluster charge. The change of the cross talk might cause that the clusters which were slightly above the clustering threshold before do not have to pass it now and therefore are not reconstructed anymore and vice versa. In addition, with the change of the cross talk parameters, the cluster position and its resolution change in simulation, resulting in small changes in tracking. Therefore the effect of a change of the cross talk may propagate up to some of the physics analyses, e.g. in searches for appearing/disappearing tracks. The object discriminators, mainly the b-tagging discriminators, which are strongly dependent on the tracking, are also influenced by it. The impact on the other physics objects and analyses, not largely dependent on tracking, is expected to be negligible.

It was shown that the cross talk parameters evolve as a function of the fluence and therefore they should be remeasured and updated regularly. Also more frequent measurements of the cross talk parameters could help us to understand why the cross talk is decreasing with respect to the previous measurement. The literature is stating that with an increasing fluence, the inter-strip capacitance should increase and the inter-strip resistance decrease. Both these changes should lead to the increase of cross talk, what was not observed. However the inter-strip resistance and capacitance are not the only factors influencing the signal formation, but there is more complex network of capacitances and resistances which has its impact on the cluster shape.  Note that the cross talk was measured only in the deconvolution mode, so it should be updated for the peak mode as well. Nevertheless, the peak mode is not used in standard data-taking, but it could help us to investigate the decrease of the cross talk, by disentangling the effects of the deconvolution from other effects.

The cosmic data recorded in absence of magnetic field, which were used for the cross talk measurement, posed several constraints and difficulties. First, due to the trigger conditions there is insufficient statistics in the disks and endcaps. Even if the trigger would be re-designed, due to the $\eta$ distribution of the cosmics, the data-taking would have to be long in order to collect sufficient statistics, which is difficult to arrange in the tight CMS schedule. The second large issue is related to the tracker timing when it samples the collected signal. The tracker has no special timing configuration for the cosmics and therefore the collision timing is always used.  As the cross talk depends on timing, we need to use for the cross talk measurement only cosmics which arrive to the given module at the same time as a particle produced in the pp collision. Another ambiguity comes from the computation of  the particle arrival time, which extrapolates all particles to the interaction point, even though they do not have to pass through there. Both the pseudorapidity coverage and the timing issues would be solved if it is possible to arrange a new data-taking of non suppressed collision data without magnetic field. But such data-taking is not compatible with the CMS program and priorities.

After the update of the cross talk parameters and conditions such as gains and noise updated by other members of the tracker local reconstruction group, the description by the simulation of the in-time clusters in data was largely improved. It was identified that there are still several parameters which are outdated and could be  updated in future, however these parameters do not largely change the cluster description. In this thesis on one hand I found out that the simulation is simplified and sometimes not realistic, but on the other hand developing more sophisticated models would not largely improve the description of the clusters in data by simulation. I also identified that already some existing parts of the simulation have only a negligible impact, i.e. the diffusion, and it could be good to evaluate what parts of the simulation are really needed in order to reduce the time needed for the event simulation.

Several improvements could be still included in the simulation of the out-of-time clusters. First, the pulse shape has changed due to the change of the APV parameter and therefore the pulse shape should be updated in order to reduce the simulated charge by a correct fraction. Secondly, it was shown that the cross talk depends on timing. The cross talk parameters were derived for in-time clusters and are not correct for out-of-time clusters. Ideally, the cross talk parameters should be parameterized as a function of the particle arrival time to the module. The out-of-time clusters need also to be well simulated as these clusters might be used by the tracking algorithm to reconstruct the tracks of the particles.

Few other interesting studies could be performed with the taken cosmic data and here I discuss few examples. In the past it was observed that there is a left-right asymmetry in the charge sharing. This effect could be as well studied with these data. Moreover, it is possible to study the cross talk and cluster properties as a function of the position where the track intercepts the strip plane, close and far from the strip. In the past it was also observed that there is an evolution of the cluster seed charge as a function of the strip number within one APV chip, this study could be repeated with these data. All these studies could help to better understand the formation of the signal, the features of the sensors such as non-uniformities in the electric field, and also the changes in the sensors properties resulting from their irradiation. 


In the future tracker for the HL-LHC, the clustering will be largely different, only the binary information from the strip/macro-pixel will be sent, therefore the cluster seed charge and cluster charge will not be available and just the cluster width will be known. As the cluster width depends on cross talk, it will also be needed to measure the cross talk in the new tracker. Moreover it will be important to determine and monitor the cross talk in order to design and maintain the threshold for strip/macro-pixel charge to be correctly set to one or zero.



\vspace*{1cm}

After an introduction of both SM and SUSY in Chapter~\ref{sec:SUSYch}, the search for the stop is presented in Chapter~\ref{sec:stopch} in the single lepton final state with the data of Run~2. This last part of the thesis is discussing a search for physics beyond the standard model. Despite its capability to well describe the majority of observed physics phenomena, the standard model (SM) of particle physics suffers from several shortcomings such as the hierarchy problem or the absence of a dark matter candidate. Due to these shortcomings, theories beyond the SM were proposed and one of them, the supersymmetry, became the most promising one because of its ability to address large part of the SM issues. In this thesis I presented a search for the supersymmetric partner of the top quark, the stop, with the CMS Run~2 data. This search targets the stop pair production, with three different possibilities for the decays of the stops: the two stops decay each to a top quark and a neutralino, or the two stops decay to a bottom quark and a chargino, or each stop decays differently as a mixture of the two previous cases. In all cases, the targeted signal final states contain one lepton, jets and missing transverse energy. 

I was involved in three public releases of this search: one based on the data recorded in 2015 corresponding to an integrated luminosity $\int{\mathcal{L}}$ of 2.3~fb$^{-1}$~\cite{Sirunyan:2016jpr}, the second one based on the data recorded at the beginning of 2016 ( $\int{\mathcal{L}}=12.9$~fb$^{-1}$)~\cite{CMS:2016vew} and the last one corresponding to $\int{\mathcal{L}}= 35.9$~fb$^{-1}$ collected during the full 2016 pp data-taking period~\cite{Sirunyan:2017xse}. I was mainly involved in the last analysis, where I was responsible for the estimation of one of the SM backgrounds, in which a Z boson decays to two neutrinos. In this thesis I also exploited a technique for tagging of merged jets originating from a W boson and I showed that the gain of implementing such technique is growing with the integrated luminosity. 

The searches for the stop pair production in different final states were already performed with the Run~1 data. The analyses exclude stop masses in terms of simplified model spectra with stops decaying to  a top quark and a neutralino up to around 755~GeV for a neutralino mass below 200 GeV. With the increase of the center of mass energy and then also the integrated luminosity, it was soon possible to further probe the stop masses. As it can be seen in Fig.~\ref{fig:figures/resultPlot2}, no excess was found in the full 2016 data ($\int{\mathcal{L}}=35.9$~fb$^{-1}$) with respect to the SM background predictions and therefore exclusion limits were derived. This analysis excluded stop masses up to 1120 GeV for a massless neutralino in terms of simplified model spectra where both stops decay to a top quark and a neutralino as shown in Fig.~\ref{fig:figures/limitT2tt}. In the case where both stops decay to a bottom quark and a chargino or in the case of mixed decay, the stop masses were excluded up to 1000 GeV  and 980 GeV, respectively, presented in Figs.~\ref{fig:figures/limitT2bW2} and \ref{fig:figures/limitT2tb}.

    \insertFigure{figures/resultPlot2} % Filename = label
                 {0.75}       % Width, in fraction of the whole page width
                 { The presentation of the observed data (black dots), background estimates and expected signal for three different signal models in the 31 signal regions. The lost lepton background is shown in green, the $1 \ell$ background from $W+jets \to 1\ell$ (i.e. not from top) in yellow, the $Z \to \nu \bar{\nu}$ background in violet and the $t\bar{t} \to 1\ell$ contribution in red. The T2tt model ($\tilde{t}_{1} \to t \tilde{\chi}^{0}_{1}$ ) with a stop mass of 900~GeV and an LSP mass of 300~GeV is shown in blue dashed line. The T2tb ($\tilde{t}_{1} \to t \tilde{\chi}^{0}_{1} /\tilde{t}_{1} \to b \tilde{\chi}^{\pm}_{1}$) and T2bW ($\tilde{t}_{1} \to b \tilde{\chi}^{\pm}_{1}$) scenarios for a stop mass of 600~GeV and a neutralino mass of 300~GeV are shown in pink and green dashed lines, respectively. The red dashed line separates the signal regions of the nominal and optimized compressed analyses~\cite{Sirunyan:2017xse}. }

    \insertFigure{figures/limitT2tt}
                 {0.49}       % Width, in fraction of the whole page width
                 { The exclusion limits at 95\% CL on the T2tt model corresponding to an integrated luminosity of 35.9~fb$^{-1}$ ~\cite{Sirunyan:2017xse}. The interpretation is provided in the context of SMS, in the plane of the stop mass vs the LSP mass. The color code shows the 95\% CL upper limit on the cross section assuming a branching ratio of 100\% in $ \tilde{t}_{1} \to t  \tilde{\chi}^{0}_{1} $. The red and black contours represent the expected and observed exclusion limits, respectively. The blue and magenta lines in the left plot show the expected limits of $1 \ell$ and $0 \ell$ analyses, respectively.  The area below the black thick curve indicates the excluded region of the signal points.  }


    \insertFigure{figures/limitT2bW2} % Filename = label
                 {0.5}       % Width, in fraction of the whole page width
                 { The exclusion limits at 95\% CL on the T2bW model corresponding to an integrated luminosity of 35.9~fb$^{-1}$~\cite{Sirunyan:2017xse}. The interpretation is provided in the context of the SMS, in the plane of the stop mass vs LSP mass. The color code shows the 95\% CL upper limit on the cross section. The red and black contours represent the expected and observed exclusion limits, respectively. The area below the black thick curve indicates the excluded region of the signal points.  }


    \insertFigure{figures/limitT2tb}
                 {0.49}       % Width, in fraction of the whole page width
                 { The exclusion limits at 95\% CL on the T2tb model corresponding to an integrated luminosity 35.9~fb$^{-1}$ ~\cite{Sirunyan:2017xse}. The interpretation is provided in the context of SMS, in the plane of the stop mass vs the LSP mass. The color code shows the 95\% CL upper limit on the cross section assuming a branching ratio of 50\% in $ \tilde{t}_{1} \to t  \tilde{\chi}^{0}_{1} $ and 50\% in $ \tilde{t}_{1} \to b  \tilde{\chi}^{\pm}_{1} $. The red and black contours represent the expected and observed exclusion limits, respectively. The blue and magenta lines in the left plot show the expected limits of $1 \ell$ and $0 \ell$ analyses, respectively.  The area below the black thick curve indicates the excluded region of the signal points.  }



Despite the stop exclusion in the TeV range, there is still room for the natural supersymmetry and therefore the effort to search for stops is not diminishing. According to the CMS predictions, with an integrated luminosity of 3000~fb$^{-1}$ envisioned to be collected at the HL-LHC, it will be possible to probe the stop masses up to 2~TeV. An upgrade of the LHC to higher center of mass energies would permit us to go even further in the stop masses. 

Before migrating to future projects, there are several directions how to improve the presented stop analysis for its future update based on data with a larger integrated luminosity. On the side of the SM backgrounds it is desired to use the data-driven methods for the background estimations in order to minimize a dependence on the simulation and consequently reduce the systematic uncertainties. The data-driven estimates are based on control regions, these control regions should be defined very similarly as the signal regions to avoid uncertainties due to extrapolations in variables.  These extrapolations rely on the shapes of variables in simulation, and if not well modeled they largely increase the systematic uncertainties on the background estimates. On the side of the signal we discussed that the simplified event simulation is not reliable in the region of the 2D mass plane where $\Delta m(\mathrm{stop, neutralino}) \sim m(\mathrm{top})$ at low neutralino mass. This could be solved by using the full simulation of the detector in this particular kinematic region. Then also the analysis strategy can be reoptimized. It is possible to reoptimize the signal regions by finding optimal cuts but also trying to find better discriminating variables. Another option is to use more object taggers. Nowadays there is a variety of taggers targeting the W boson and top quark identification, not only targeted as a  single merged jet, but also as completely resolved jets or partially merged jets. We could also consider not to perform a cut and count analysis but move to machine learning techniques providing discriminant between signal and background we can cut on. 


Within the CMS collaboration a large variety of SUSY searches is performed. The stops are being sought in several different final states and the other predicted SUSY particles are targeted by dedicated searches as well. Many analyses search for SUSY within the context of the minimal supersymmetric standard model but there are also several searches which go beyond, e.g. searches allowing violation of the R-parity. In general the SUSY searches are interpreted in terms of simplified spectra. A lot of efforts are being done to reinterpret these searches in more realistic models. Another part of activities is the combination of searches to reach a better discovery/exclusion potential. The CMS experiment and in general the collider experiments do not provide the sole opportunity to search for supersymmetry. For example, supersymmetry can also be studied via direct dark matter detection experiments. 

In summary, there are still many options for natural SUSY in term of parameter space and also many options how to search for it. Collider experiments are one of these options, and with increasing luminosity, energy and optimization of analyses they are providing great opportunities to go beyond the current limits and further probe the uncovered phase-space.

%----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
