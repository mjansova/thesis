\clearpage

\setcounter{secnumdepth}{4}
\chapterwithnum{Search for top squark pair production in pp collisions at $\sqrt{s}$=13~TeV in Run 2 using single lepton events~\label{sec:stopch}}
\setcounter{secnumdepth}{4}

%\section{Introduction and motivation}

The supersymmetry is considered to be the most popular extension of the standard model, due to its capability to address many shortcomings of the SM. Among others, it provides a natural solution to the hierarchy problem and a dark matter candidate. Within supersymmetry each SM particle has a SUSY partner with the same quantum numbers except of spin which is differing by one half. This chapter focuses on the direct pair production of the SUSY partners of the top quarks, referred to as ``top squarks'' or ``stops''~($\tilde{t}$). There are two scalar stops $\tilde{t}_{R}$ and  $\tilde{t}_{L}$ as there are a left and a right components of the SM fermion fields. These two stops mix into mass eigenstates $\tilde{t}_{1}$ and $\tilde{t}_{2}$ labeled by increasing mass. 

In this chapter only a SUSY model, in which the R-parity is conserved leading to the LSP being the lightest neutralino~($\tilde{\chi}^{0}_{1}$) and the stops being produced  in pair, is considered. Depending on the mass difference between the stop and the neutralino $\Delta m = m_{\tilde{t}_{1}} - m_{\tilde{\chi}^{0}_{1}}$, several decay modes of the stops are possible. The stops can decay via two, three or four body decays to final states with b- or c-quarks. In case of three (four) body decays the top quark (top quark and W boson) in the decay chain are produced off-shell. This chapter describes the search for top squark pair production in pp collisions at $\sqrt{s}$=13~TeV using single lepton events~\cite{Sirunyan:2017xse} corresponding to integrated luminosity of 35.9~fb$^{-1}$ collected during 2016. This analysis, further referred to as ``full 2016 analysis'', is covering two and three body decays with b-quarks, W bosons and neutralinos in the final state. As I was also involved in the previous versions of this analysis~\cite{Sirunyan:2016jpr, CMS:2016vew}, corresponding to data from 2015 of the integrated luminosity 2.3~fb$^{-1}$ referred to as ``2015 analysis'' and data from beginning of 2016 of the integrated luminosity 12.9~fb$^{-1}$ referred to as ``beginning of 2016 analysis'', respectively, several differences between the three versions are briefly discussed, but in general the analyses are largely similar. In the following sections I especially highlight some chosen personal contributions.

In the Section~\ref{sec:stopologies} the signal topologies targeted by full 2016 analysis are presented. The following Section~\ref{sec:strategy} defines the strategy of the analysis.  Then in Section~\ref{sec:estimations} the background estimation techniques in the search region are discussed, starting from the estimation of background referred as ``$Z \to \nu \bar{\nu}$'' which I provided during my PhD thesis. Within the subsections on the relevant backgrounds, the systematic uncertainties on the background estimates are identified. Finally Section~\ref{sec:results} summarizes the results of this analysis and provides their interpretation. 

%-delta M around top mass challenging kinematics -> looks like SM tt -> this region is called stealthy region
%-for t2bW when W becomes on-shell also difficult kinematics

\section{Signal topologies~\label{sec:stopologies}}

In this analysis the targeted final states have one leptonically and one hadronically decaying W boson, resulting in one charged lepton in the final states. The advantage of this one lepton channel is the low occurrence of the standard model backgrounds and its relatively high branching ratio, the stop pair decays in around 32\% of cases to the final states with one prompt lepton ($e,\mu$) or leptonically decaying tau lepton. The analysis focuses on the kinematic region where $\Delta m > m_W+m_b$ and on three different decay modes of the stop pair.  

The first of the three considered decay chains of the stop pair is shown in the left part of Fig.~\ref{fig:figures/stopdecays} and referred to as ``T2tt'', in which both stops decay to a top quark and a neutralino, followed by the decay of each top quark to a W boson and a b-quark:

\eq{t2tt}
{
    \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t \bar{t} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1} \to b \bar{b} W^{+} W^{-} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1} \to b \bar{b} \ell \nu_{\ell} 2j \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}.
}

The second possibility depicted in the right part of Fig.~\ref{fig:figures/stopdecays} is referred to as ``T2bW'' and in this case both stop quarks decay via an intermediate chargino:

\eq{t2bW}
{
    \tilde{t}_{1} \bar{\tilde{t}}_{1} \to b \bar{b} \tilde{\chi}^{+}_{1} \tilde{\chi}^{-}_{1} \to b \bar{b} W^{+} W^{-} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1} \to b \bar{b} \ell \nu_{\ell} 2j \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1} .
}
In the T2bW model the chargino mass is not scanned due to the computing limitations and it was chosen to be fixed halfway between the mass of the stop and the mass of the neutralino. 

    \insertTwoFigures{figures/stopdecays}
                 {figures/T2tt} % Filename = label
                 {figures/T6bbWW} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { The SMS diagrams for the direct stop pair production. In the left diagram each stop decays to $t  \tilde{\chi}^{0}_{1}$, while in the right one it decays to $ b \tilde{\chi}^{\pm}_{1} $~\cite{website:SUSYdiagrams}. }


The third decay, shown in Fig.~\ref{fig:figures/T4tbW} and referred as ``T2tb'' combines the previous two decay modes of the stop: one of the stops decay to a top quark and a neutralino, while other one decays to a bottom quark and a chargino:

\eq{t2tb}
{
    \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t b \tilde{\chi}^{0}_{1} \tilde{\chi}^{+}_{1} \to b \bar{b} W^{+} W^{-} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1} \to b \bar{b} \ell \nu_{\ell} 2j \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1} .
}
In the T2tb model, the chargino and neutralino are taken as almost mass degenerate, the chargino mass is fixed by relation $m_{\tilde{\chi}_{1}^{\pm}} = m_{\tilde{\chi}_{1}^{0}} + 5~\mathrm{GeV}$. Because of this small difference between the chargino and neutralino masses, the W boson is produced off-shell, its decay products have low transverse momenta and are typically not selected or even reconstructed. 

    \insertFigure{figures/T4tbW} % Filename = label
                 {0.5}       % Width, in fraction of the whole page width
                 { The SMS diagram for the direct stop pair production considering the mixed decay of the stop pair $t b \tilde{\chi}^{0}_{1} \tilde{\chi}^{+}_{1}$~\cite{website:SUSYdiagrams}. }


All decay modes considered by this analysis lead to states with two b-jets, two W bosons and two neutralinos, but the final state kinematics differ depending on the decay mode and $\Delta m$. The $\Delta m$ plane is shown in Fig.~\ref{fig:figures/dmplane}. In the part of the plane where $\Delta m < m_t$, often referred to as ``compressed spectra'' region or ``corridor region'', the decay products are softer and subsequently not selected. On the other hand when the $\Delta m$ is large, boosted topologies can be expected. 

    \insertFigure{figures/dmplane} % Filename = label
                 {0.99}       % Width, in fraction of the whole page width
                 { The plane of stop ($\tilde{t}_{1}$) versus LSP~($\tilde{\chi}^{0}_{1}$) masses~\cite{Aad:2014kra}. }


\section{Analysis strategy~\label{sec:strategy}}

Because of the two neutralinos, the signal processes (T2tt, T2bW and T2tb)  can have final state signatures largely differing from the SM background ones. The knowledge about the signal and background signatures is used to define a ``baseline search region'', in which the SM backgrounds are suppressed. 

In the baseline search region there are three groups of backgrounds present. The leading background is referred to as the ``lost lepton'' background which has a final state with two W bosons, both decaying into leptons and one of the charged leptons being lost due to acceptance or other criteria. This background, mainly coming from $\mathrm{t\bar{t}} \to 2 \ell$ and single top quark process,  is reduced by applying veto on the misreconstructed second lepton, but even after this it remains the largest background. The sub-leading background is the ``one lepton'' background, which is predominantly composed of $W+jets \to 1\ell$ and $t\bar{t} \to 1 \ell$ processes with one W boson decaying leptonically. The last relevant background is denoted as ``$Z \to \nu \bar{\nu}$'' and comes from processes such as $t\bar{t}Z$ and $WZ$ in which the Z boson decays to two neutrinos and one of the W bosons decays leptonically.  

This baseline search region is then divided,  with help of discriminating variables, to smaller signal regions. The goal is to define regions in which the signal to background ratio is enhanced and only the regions leading to the high signal to background ratio are used. Also these variables can discriminate between different signals and backgrounds and can be helpful in defining signal regions where some of the signals are enriched or some of the backgrounds are more reduced. The remaining background in each signal region is estimated from data-driven techniques or simulations. 

The hypothesis tests are performed on the signal, estimated background and observed data yields in the signal regions. In case that no excess from the SM expectation is observed, the results are interpreted in term of exclusion limits on given signal models. For the purpose of analysis optimization and limit setting, the procedure of hypothesis tests in the LHC searches is briefly described in the Section~\ref{sec:stats}. Then the triggers and objects used in this analysis are introduced in Subsections~\ref{sec:trigger}~and~\ref{sec:objects}.
 
In the following Section~\ref{sec:variables}, the variables designed to define the baseline search region and its categorization in signal regions are introduced and justified on the examples of signal topologies. The backgrounds behavior with regard to the choice of the variables is discussed as well. In the end, the final definition of the baseline and signal regions for the presented analysis is revealed. Once the most discriminating variables are identified, the final definition of the baseline and signal regions is shown in Section \ref{sec:baseline} and \ref{sec:sr}. 

The introduced analysis strategy is general for all types of signal topologies. Only for corridor region of T2tt a separately optimized ``corridor'' analysis was designed to better target a very challenging signal final state scenario, whose kinematics is very similar to that of the SM processes. To be able to combine results of the nominal and corridor analyses, the sensitivity of both analysis for different $\Delta m$ was evaluated and it was decided to use compressed analysis for T2tt signals in $\Delta m$ range between 100 and 225~GeV and the nominal one everywhere else.

%binned approach in variables which tend to reduce signal
%single top?


\subsection{Hypothesis testing in LHC searches~\label{sec:stats} }

The hypothesis testing and limit setting procedure chosen by the CMS collaborations is a modified frequentist method referred to as the ``$\mathrm{CL_{s}}$'' method~\cite{Read:2002hq, CMS-NOTE-2011-005}. There are two hypotheses to be tested, the background only hypothesis $H_{0}$ also referred to as the ``null hypothesis'' and the signal plus background hypothesis $H_{1}$ usually called as ``alternative hypothesis''. These hypotheses are functions of $b+\mu s$, where $b$ and $s$ are the expected background and signal yields and $\mu$ is the signal strength modifier. In case of background only hypothesis $H_{0}$ the signal strength $\mu$ is zero. The signal and background estimates entering into the hypothesis testing are burdened by many systematic uncertainties, which are in the $\mathrm{CL_{s}}$ method treated as nuisance parameters and denoted collectively as $\theta$. The expected signal and background yields are dependent on these nuisance parameters: $s \to s(\theta)$ and $b \to b(\theta)$.

\textbf{Observed limits}

To compute the observed limits a likelihood function $\mathcal{L}(data|\mu, \theta)$ can be built as follows~\cite{CMS-NOTE-2011-005}

\eq{likelihood}
{
\mathcal{L}(data|\mu, \theta) = \operatorname{Poisson}(data| \mu s(\theta) +b(\theta))~p(\tilde{\theta}|\theta),
}
where $p(\tilde{\theta}|\theta)$ are the probability distribution functions for the nuisance parameters $\theta$ with $\tilde{\theta}$ being the default value of the nuisance parameter, $data$ for this purpose refers to the information about the observed data yield or generated pseudo-data. In case of a binned likelihood, the term $\operatorname{Poisson}(data| \mu s(\theta) +b(\theta))$ can be expressed as


\eq{binnedlikelihood}
{
\operatorname{Poisson}(data| \mu s(\theta) +b(\theta))  = \prod_{i} \frac{\mu s_{i}+b_{i}}{n_{i}!} e^{-\mu s_{i}-b_{i}},
}
which is a product  of Poisson probabilities to observe $n_{i}$ data events in the bin $i$. Given the likelihood function, the compatibility of $data$ with the $H_{0}$ or $H_{1}$ hypothesis can be tested with the help of the test statistics $\tilde{q}_{\mu}$ which has the form of a profile likelihood ratio

\eq{pLR}
{
\tilde{q}_{\mu} = -2 \mathrm{ln } \frac{ \mathcal{L}(data| \mu , \hat{\theta}_{\mu}) } {\mathcal{L}(data| \hat{\mu} ,~\hat{\theta})},
}
where the signal strength $\mu$ is a free parameter, $\hat{\theta}_{\mu}$ are the conditional maximum likelihood estimators of the parameters $\theta$ given the parameter $\mu$ and the measured data. The $\hat{\mu}$ and $\hat{\theta}$ are the parameter estimators at the global maximum of the likelihood. 
%The condition $\hat{\mu} \geq 0 $ expresses that the rate of signal cannot be negative and $\hat{\mu} \leq \mu $ is constraint to consider only one-sided confidence interval.
	  
The observed value of the profile likelihood ratio expressed in Eq.~\ref{eq:pLR} $\tilde{q}_{\mu}^{obs}$, can be computed using the measured data and the maximum likelihood estimators of $\theta$ which best describe the observed data, can be evaluated for both the $H_{0}$ and $H_{1}$ hypotheses. The $\mathrm{CL_{s}}$  method defines two p-values, the first one is $p_{\mu}$ testing the compatibility of the observation with $H_{1}$ and the second one, $1-p_{b}$, testing the compatibility of the observation with the background-only hypothesis~$H_{0}$. The $\mathrm{CL_{s}}$ for a given signal strength $\mu$ is then defined as

\eq{cls}
{
\mathrm{CL_{s}}(\mu) = \frac{p_{\mu}}{1-p_{b}},
}
where $p_{\mu}$ can be written as

\eq{pmu}
{
p_{\mu} = P(\tilde{q}_{\mu} \geq \tilde{q}_{\mu}^{obs}|s+b)
}
and $1-p_{b}$ as

\eq{pb}
{
1-p_{b} = P(\tilde{q}_{\mu} \geq \tilde{q}_{\mu}^{obs}|b).
}

For $\mu=1$, the $H_{1}$ is excluded at ($1-\alpha$) $\mathrm{CL_{s}}$ confidence level in case that $\mathrm{CL_{s}} \leq \alpha$. Typically, the exclusion limits are derived for $\alpha = 0.05$. In the case that $\mu$ parameter is not fixed to one, the exclusion limits are set on the cross section given a fixed branching ratio of a given signal model. In such procedure the signal strength $\mu$ starts at zero and it is increased up to the point where $\mathrm{CL_{s}}$ = 95\%. If for a given signal point the cross section is limited to a lower value than the theoretical cross section, the signal point is excluded. A contour line is drawn at the place where the limit on the cross section is equal to the theoretical cross section.

\textbf{Expected limits}

The expected limits can be determined via a similar method using generated background-only pseudo-data instead of observed data. 

Assuming that the test statistics has a Gaussian tail, the p-value can be converted to the significance $Z$ with the convention

\eq{significance}
{
 p = \int_Z^{\infty} \frac{1}{\sqrt{2\pi}} e^{-x^{2}/2} dx.
}
If considering the background-only hypothesis, the significance of 5 standard deviations corresponding to $p_{b} = 2.8 \times 10^{-7} $ suggests that a $H_{0}$ hypothesis is excluded at $5 \sigma$ in favor of the $H_{1}$ hypothesis, therefore the discovery is claimed. %TODO check that


\subsection{Triggers, data and simulated samples~\label{sec:trigger}}

In the full 2016 analysis several kinds of triggers, which are single lepton, \MET, $\mu-e$ double lepton  and single photon triggers, are used. The single lepton data are triggered with single lepton and \MET triggers, where the \MET triggers help to select data with lower \pt leptons, which would not pass the single lepton trigger. In the analysis an ``OR'' of these triggers is used. The trigger requirements can be seen in Table~\ref{tab:triggerobj}. The trigger efficiency in the search regions of interest is more than 99\%. The data triggered by the double lepton trigger are used for checking the kinematics of the lost lepton background. Similarly the data triggered by the single photon trigger are used to cross-check the \MET resolution in  the one lepton background. 

The SM background samples are generated by leading-order or next-to-leading-order generators and processed by the full simulation of CMS. The LO samples generated by MADGRAPH\_aMC@NLO 2.2.2.~\cite{Alwall:2014hca} generator configured to the LO with MLM matching~\cite{Alwall:2007fs} using the LO NNPDF3.0~\cite{Ball:2014uwa} parton distribution functions are  $t\bar{t}$, W+jets, $t\bar{t}Z$ and $\gamma$+jets. The single top samples are generated by the NLO POWHEG 2.0~\cite{Alioli:2010xd} generator. The MADGRAPH\_aMC@NLO 2.2.2. in the NLO mode with FxFx~\cite{Frederix:2012ps} matching and NLO NNPDF3.0 parton distribution functions is used to generate majority of the rare processes, such as $WZ$ and $ZZ$. The full hadronization, parton showering and modeling of the underlying events is then performed by PYTHIA 8.205~\cite{Sjostrand:2014zea}.

The description of the initial state radiation (ISR) jets in the $t\bar{t}$ and $t\bar{t}Z$ samples generated by the MADGRAPH\_aMC@NLO 2.2.2. is improved by reweighting these samples based on number of the ISR jets $N_{ISR}$ in order that the jet multiplicity agree between data and simulation. For the $N_{ISR}$ between one and six, the reweighting factor varies between 0.92 and 0.51. The systematic uncertainty on this factor is taken to be half a deviation from the unity.

The signal samples are produced by the MADGRAPH\_aMC@NLO 2.2.2. generator in the LO and injected into fast simulation to simulate the response of CMS detector to the signal events.  The same $N_{ISR}$ reweighting procedure as discussed above is applied on the signal samples. 

\begin{table}[h]
\begin{center}
   \begin{tabular}{| l | l |}
      \hline
      \MET trigger & \MET$>120\GeV$ and $H_{T}^{miss}=\abs{\sum(\ptvec^{\text{\,jets}})+\ptvec^{\mathrm{^{\text{\,lep}}}}}>120\GeV$ or \\
      Single lepton trigger                         & isolated electron (muon):       \pt$^{\mathrm{lep}} > 25 (22) \GeV$, $|\eta| < 2.1 (2.4)$ \\
      \hline
      Selected lepton \rule{0pt}{4ex}   & electron (muon): \pt$^{\mathrm{lep}} > 20 \GeV$, $|\eta| <1.442 (2.4)$, medium ID \\
      Selected lepton isolation & \pt$^{\mathrm{sum}} < 0.1 \times
      p_{T}^{\mathrm{lep}}$,  $\Delta R = \min[0.2,\max(0.05, 10 \GeV / p_{T}^{\mathrm{lep}})]$ \\
      Jets and b-tagged jets \rule{0pt}{4ex}   & \pt$ > 30 \GeV$, $|\eta| < 2.4$ \\
      \ \ \ b tagging efficiency & medium (tight) WP: 60-70 (35-50)\% for jet \pt 30-400 $\GeV$ \\
      \ \ \ b tagging mistag rate & medium (tight) WP : $\sim1\%$ ($\sim0.2\%$) for light-flavor quarks\\
      Veto lepton \rule{0pt}{4ex}   & muon or electron with \pt$^{\mathrm{lep}} > 5 \GeV$, $|\eta| <2.4$ \\
      \ \ \ Veto lepton isolation & \pt$^{\mathrm{sum}} < 0.1 \times p_{T}^{\mathrm{lep}}$,  $\Delta R = \min[0.2,\max(0.05, 10 \GeV / p_{T}^{\mathrm{lep}})]$ \\
      Veto track \rule{0pt}{4ex}   & charged particle-flow candidate, \pt$ > 10 \GeV$, $|\eta| <2.4$ \\
      \ \ \ Veto track isolation & \pt$^{\mathrm{sum}} < \min\left(0.1 \times p_{T}^{\mathrm{lep}}, \mathrm{6} \GeV\right)$, $\Delta R = 0.3$ \\
      \hline
    \end{tabular}

\caption[Table caption text]{ The triggers and the objects definition. The $p_{T}^{lep}$ is the \pt of lepton and the $p_{T}^{sum}$ is the scalar sum of the \pt of all PF candidates which are located in a cone around a lepton, but excluding the \pt of lepton. The $p_{T}^{sum}$ for the selected and veto lepton is computed from the charged and neutral PF particles, while for the veto track only charged particles are used. The label ``light flavor'' denotes that the jet originates from u,d or s quarks or gluons~\cite{Sirunyan:2016jpr}. }
\label{tab:triggerobj}
\end{center}
\end{table}




%TODO start here

\subsection{Objects and event selection~\label{sec:objects}}


In this section the objects definitions and the criteria on their selection for the full 2016 analysis are introduced.

\begin{description}
\item[Primary vertex]
The primary interaction vertex in the event must pass the good quality criteria and it is identified as the vertex which leads to the largest sum of $p_{T}^{2}$ of physics objects belonging to this vertex.

\item[Lepton]
The ``selected'' lepton for this analysis is a large \pt electron or muon isolated from any activity in the detector. The selected lepton must originate from the primary vertex and have in both cases \pt>20~GeV, $|\eta|<1.4442$ for electron and $|\eta|<2.4$ for muon. Within this acceptance criteria the selection efficiency is more than 85\% for electrons and 95\% for muons. This analysis also defines second ``veto'' lepton which passes looser isolation criteria, \pt threshold or which passes looser identification than the selected one. The isolation criterion is specific for most of the CMS SUSY analyses and is defined as a function of sum of the \pt of PF candidates which are within a cone around the lepton. The isolation cone distance parameter depends on \pt of lepton and the cone distance parameter  starts at 0.2 for \pt<50~GeV and then the distance parameter is 10.0/\pt for \pt in interval 50<~\pt~<200~GeV. For the \pt>200~GeV the cone distance parameter is of 0.05. This procedure ensures that the vetoing of signal events due to the jet fragments in proximity is decreased.   

\item[Jets and b-jets]
The jets are reconstructed by the anti-kt~\cite{Cacciari:2008gp} algorithm with a cone distance parameter of 0.4. The jet \pt is required to be higher than 30~GeV and the jet must be in the range $|\eta|<2.4$. Either a medium or a tight b-tagging working point~(WP) of CSVv2 is used for definition of signal regions.  

\item[Veto on isolated tracks]
Majority of the tau leptons decay into one charged track which is predominantly a charged hadron. Therefore the events with taus can be suppressed by vetoing events which have one isolated charged track.

\item[Veto on hadronic tau]
This additional tau veto, using tau identification criteria, helps to veto the taus which do not pass the isolated track selection. It mainly helps to reduce the lost lepton background.
\end{description}

The presented analysis selects events with one single lepton, jets, b-jets and \MET in the final state. On the other hand the events which have in the final state hadronically decaying tau leptons or a second ``veto'' lepton passing looser lepton criterion are vetoed. The summary of the object selection criteria is presented in Table~\ref{tab:triggerobj}.

\subsection{Search variables~\label{sec:variables}}

The discriminating variables are built from the selected objects described above. Definitions of variables connected with the presented stop analyses are introduced in the following subsections.

\subsubsection{Number of leptons~($N_{\ell}$)}

The presented search focuses on final states with one charged lepton and therefore a requirement on the number of leptons to be equal to one must be imposed. The lepton is either an electron or a muon and the events with tau leptons are rejected.

\subsubsection{Number of jets~($N_{J}$)}

As shown in the example in blue in Fig.~\ref{fig:figures/T2ttV}, at least four jets are expected in the signal final states. But because of the low mass difference between the chargino and the neutralino in the T2tb model, the two jets from the hadronically decaying W boson have low \pt and thus they may not be selected, therefore only two jets are expected. In case of high $\Delta m$ the decay products of the stops can be boosted and thus two or more jets can be merged into one, resulting to a  reduced number of selected jets in final state. The baseline selection requires to have at least two jets, but this variable is also later used for the definition of signal regions targeting different topologies.

%If the jets from hadronically decaying W boson are not selected, only the two b-jets are present in the final state of this signal topology, what is being one of the motivation to search for final states with less jets than expected four.
%TODO jets PT, eta, jet wp

    \insertFigure{figures/T2ttV}
                 {0.7}       % Width, in fraction of the whole page width
                 { Diagram of the T2tt signal model supporting the choice of $N_{J}$ (blue), \MET (red),  $M_{T}$ (magenta), min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ (brown) and $M_{\ell b}$ (orange) as discriminating variables~\cite{CMS:2016vew}. }

\subsubsection{Number of b-jets~($N_{b}$)}

In the diagram of Fig.~\ref{fig:figures/T2ttV} it can also be noticed that two jets in green are originating from b-quarks. For this reason the number of selected b-tagged jets is one of the discriminating variables, which is in the baseline search region  chosen to be larger or equal to one. In the signal regions where $W+jets \to 1\ell$  background is dominant, the tight b-tagging working point is used to further suppress it, as the jets are expected to be mainly composed of light flavors.

\subsubsection{Missing transverse energy~($E_{T}^{miss}$)}

The the red circles of diagram in Fig.~\ref{fig:figures/T2ttV} show that in the final states of the signal processes are two neutralinos and one neutrino originating from the leptonically decaying W boson. These particles escape detector and lead to the missing momentum in the transverse plane. The missing transverse momentum referred as ``missing transverse energy'' defined in Section~\ref{sec:MET} is very powerful in rejecting the SM backgrounds because they tend to have smaller values of the \MET as the sources of the \MET are more limited than in case of the signal processes. Therefore the baseline selection requires \MET to be larger than 250~GeV. The distribution of the \MET in the baseline search region for all relevant backgrounds and three selected signal scenarios is shown in the left plot of Fig.~\ref{fig:figures/METMT}. The one lepton background is decomposed into two processes which are later referred to as ``1l from top'' populated by $t \bar{t} \to 1\ell$ and ``1l not from top'' populated by $W+jets \to 1\ell$  processes. The \MET was also found to be a good variable for the definition of the signal regions.

Because of the presence of two neutrinos and one misreconstructed lepton in the lost lepton background, the \MET of this background can be very large. This also applies for the processes of the $Z \to \nu \bar{\nu}$ background, where the source of the \MET are three neutrinos.

\subsubsection{Transverse mass of the lepton-\MET system~($M_{T}$)}

The $M_{T}$ variable is defined as

\eq{MT}
{
 M_{T} = \sqrt{2 p_{T}^{\ell} E_{T}^{miss} (1 - \mathrm{cos}(\phi)) } ,
}
where $p_{T}^{\ell}$ is the transverse momentum of the lepton and $\phi$ is the angle between the lepton and the direction of \MET in the transverse plane. The combination of a selection on the $M_{T}$ and \MET ensures a drastic reduction of the SM background. The $M_{T}$ variable is  designed to suppress backgrounds where both the lepton and \MET come from one W boson, in that case the \MET corresponds to the neutrino of this W boson. In such case the $M_{T}$ has an endpoint at the W boson mass. As shown in the left diagram of Fig.~\ref{fig:figures/T2ttV}, for the signal, the leptonically decaying W boson is not the only source of the \MET and therefore it has no endpoint.


The baseline search region requires events with $M_{T}$ larger than 150~GeV, suppressing mainly the one lepton background. The lepton and \MET in both  $W+jets \to 1\ell$  and $t \bar{t} \to 1\ell$ originate from one W boson and thus their $M_{T}$ should have endpoint at W boson mass~($\sim$80~GeV). It can be seen in the right plot of Fig.~\ref{fig:figures/METMT}, that indeed the bulk of the one lepton events have low $M_{T}$, but there are tails with high $M_{T}$. In case of $t \bar{t} \to 1\ell$ the top quark constraints the kinematics of the W boson and therefore the tail in $M_{T}$  is mainly caused by the \MET resolution. The situation is different for $W+jets \to 1\ell$ , where the kinematics permits off-shell production of W boson and consequently $m_{W^{*}}> 80$~GeV. For the lost lepton and $Z \to \nu \bar{\nu}$ backgrounds, there is no endpoint as the \MET does not originate from one W boson.

    \insertTwoFigures{figures/METMT}
                 {figures/LogMET2j} % Filename = label
                 {figures/LogMT2j} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { Data, background and signal distributions of the \MET (left) and $M_{T}$ (right) variables after applying the baseline selection, except of the cut on the $M_{T}$ in case of the right plot~\cite{website:stopSupp}. }

\subsubsection{Minimal azimuthal angle between the direction of one of the two leading jets and the \MET~(min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ )}

For background events where a neutrino is the only source of the \MET, it is probable that this neutrino is close to the b-quark  originating from the same top decay. This appears for $t\bar{t} \to 1\ell$ process as shown in the left plot of Fig.~\ref{fig:figures/DPHIMLB}. In case of signal, as depicted in magenta in diagram of Fig.~\ref{fig:figures/T2ttV}, there are more sources of the \MET and therefore there is no constraint on this variable. The chosen baseline requirement on the min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ to be larger than 0.8 was found to considerably reduce the suggested $t\bar{t} \to 1\ell$ but also $t\bar{t} \to 2\ell$  background processes but lead to a relatively small reduction of the signal.

    \insertTwoFigures{figures/DPHIMLB}
                 {figures/LogMDPhi2j} % Filename = label
                 {figures/LogMlb2j} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { Data, background and signal distributions of min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ (left) and $M_{\ell b}$ (right) variables after applying the baseline selection, except of the cut on the shown variable~\cite{website:stopSupp}. }

\subsubsection{Invariant mass of the system composed by the selected lepton and the closest b-tagged jet~($M_{\ell b}$)}

In case that a lepton and a b-jet originate from one top quark as displayed in Fig.~\ref{fig:figures/T2ttV}, there is a bound on the $M_{\ell b}$ variable which is

\eq{Mlb}
{
 m_{t} \sqrt{1 - \frac{m_{W}^{2}}{m_{t}^{2}} } \approx 153~\mathrm{GeV} ,
}
where $m_{t}$ is the mass of the top quark and $m_{W}$ the W boson mass. This limit is true for backgrounds coming from $t\bar{t}$ decays, but also for the T2tt signal. On the other hand there is no limit for the $W+jets \to 1\ell$  background and the T2bW signal. Therefore this variable is not suitable for the baseline region definition, as putting a constraint on it could considerably reduce signal but it can be used later to define signal regions which discriminate among the signal scenarios as well as suppress part of the backgrounds. The distribution of the  $M_{\ell b}$ variable for all relevant backgrounds and three different signals is shown in Fig.~\ref{fig:figures/DPHIMLB} after the baseline selection. It can be clearly noticed that the low $M_{\ell b}$ part of distribution is dominated by the lost lepton background and is more sensitive to the T2tt like signal, while the high $M_{\ell b}$ part is mainly populated by $W+jets \to 1\ell$  and T2bW like signal.

\subsubsection{Modified topness~($t_{mod}$)}

The topness variable~\cite{Graesser:2012qy} is a $\chi^{2}$-like variable developed to suppress background coming from the $t \bar{t} \to 2\ell$ process with one misreconstructed lepton. Its aim is to evaluate how well an event agrees with the $t \bar{t} \to 2\ell$ hypothesis, given that one lepton is lost. It was discovered that removing several terms from the topness variable, improves the signal discrimination in this analysis. The new variable called ``modified topness''~($t_{mod}$) is defined as


\eq{tmod}
{
 t_{mod} = \mathrm{ln(\mathrm{min}S)},~\mathrm{where}~S(\vec{p}_{W}, p_{\nu, z} ) = \frac{(m_{W}^{2}- (p_{\nu}+p_{\ell})^2 )^2 }{a_{W}^{4}} + \frac{(m_{t}^{2}- (p_{b_{2}}+p_{W})^2 )^2 }{a_{t}^{4}},
}
with $m_{W}$ and  $m_{t}$ being the masses of the W boson and the top quark, ${p}_{W}$, ${p}_{\nu}$, ${p}_{\ell}$, ${p}_{b_{2}}$ being the momenta of the W boson, the neutrino, the lepton and the b-jet. The parameters $a_{W} =5$~GeV and $a_{t}=15$~GeV are the resolution parameters. The first term in Eq.~\ref{eq:tmod} aims to reconstruct the mass of the W boson whose lepton was selected and the purpose of the second term is to reconstruct the top mass of the leg, for which the lepton was lost. There are different options how to chose the  b-jet in the event. For this analysis the following procedure was chosen: The modified topness is computed for three jets having the highest CSVv2 discriminator values, and then the jet which leads to the smallest modified topness is chosen.

Different backgrounds and signal models with different $\Delta m$ populate the $t_{mod}$ distribution shown in Fig.~\ref{fig:figures/Logtmod2j} differently, therefore this variable is valuable for the definition of signal regions. It can be noticed that the $t_{mod}$ variable has a double peak shape. The left peak is populated by the events, where the objects were correctly identified to originate from given W boson and top quark, while in the one on the right a wrong object(s) was taken for the computation of the modified topness.

    \insertFigure{figures/Logtmod2j}
                 {0.5}       % Width, in fraction of the whole page width
                 { Data, background and signal distributions of the $t_{mod}$ variable after applying the baseline selection, except of the cut on the shown variable~\cite{website:stopSupp}. }

\subsubsection{$\mathbf{M_{T2}^{W}}$~\label{sec:mt2w}}

The purpose of the $M_{T2}^{W}$ variable is similar to that of the modified topness, i.e. to reduce the lost lepton background. This variable similarly tries to reconstruct the event under the $t \bar{t} \to \ell \ell$ hypothesis with one lost lepton. Its definition is

\begin{equation}
\begin{split}
M_{T2}^{W} = \mathrm{min}\{m_{y},~\mathrm{consistent~with:}~[p_{1}^{2}=0,~(p_{1}+p_{\ell})^{2} = p_{2}^{2} =m_{W}^{2},~ \vec{p}_{T}^{1}+\vec{p}_{T}^{2}= \vec{E}_{T}^{miss} \\
(p_{1} + p_{\ell} + p_{b_{1}})^{2} =  (p_{2} + p_{b_{1}})^{2} = m_{y}^{2}]\},
\end{split}
\end{equation}

where $m_{y}$ is the fitted mass of mother particle and $p_{1}$, $p_{2}$, $p_{b_{1}}$, $p_{b_{2}}$, $p_{\ell}$ are momenta of the neutrino from the W boson whose lepton was selected, neutrino and lost lepton system, two b-jets, and lepton. The $m_{W}$ is the mass of W boson. The b-jets $b_{1}$ and $b_{2}$ are selected from three jets with the highest CSVv2 discriminator as the jets which lead to the lowest $M_{T2}^{W}$.

The $M_{T2}^{W}$ distribution has an endpoint at the top mass for $t\bar{t}$ events with lost lepton. It would be possible to use this variable for the baseline selection by requiring a high values of $M_{T2}^{W}$, but it was found out that signals with low $\Delta m$ also lead to small $M_{T2}^{W}$. In the 2015 and beginning of 2016 analyses~\cite{Sirunyan:2016jpr, CMS:2016vew}, the $M_{T2}^{W}$ was used for definition of signal regions. The $M_{T2}^{W}$ has very similar behavior as the $t_{mod}$ and with more statistics collected it was found out that the $t_{mod}$ is more discriminating in the full analysis phase-space than the $M_{T2}^{W}$ variable and therefore in current analysis the $M_{T2}^{W}$ is not used anymore.


\subsubsection{Number of W-tagged jets~($N_{W}$)}

In case of high  $\Delta m$ signals the top quarks originating from the stop decays are expected to be significantly boosted. The boost of the top quarks within the SM is less likely and therefore tagging of boosted objects could help the signal discrimination. As shown in Fig.~\ref{fig:figures/boostedTopologies} when the momentum of hadronically decaying top quark is low, three resolved anti-kt jets with cone distance parameter of 0.4 (ak4) are observed. With growing boost of the top quark, the jets originating from the W boson or all three jets of the top quark can be merged into one fat jet. The merged jets can be tagged by special W  or top-tagging techniques.

    \insertFigure{figures/boostedTopologies}
                 {0.99}       % Width, in fraction of the whole page width
                 { A schema of the jet configuration of hadronically decaying top quark  depending on its boost. }


I have worked on evaluating the potential increase of the sensitivity that W-tagging techniques could bring to the stop analysis. The study has been performed in the context of the 2015 single lepton stop analysis~\cite{Sirunyan:2016jpr} using data collected in 2015 at center-of-mass energy of 13~TeV  corresponding to the integrated luminosity of 2.3~fb$^{-1}$. 


The standard jets used in the analysis are clustered by the ak4 algorithm. The boosted W jets are expected to be ``fat'' and therefore need to be re-clustered with a different cone distance parameter. In the study two possibilities were exploited, ak8 jets with a cone distance parameter  of 0.8 and ak10 jets with a cone distance parameter of 1.0. The merged W jet should have a large transverse momentum and a mass around the mass of W boson. To discriminate if the jet is a merged jet originating from a W boson, a requirement on its mass can be posed. The raw mass of the jet is contaminated by the initial state radiations, underlying events and pileup and therefore it must be cleaned from these contributions. Several ``grooming techniques'' of the jet mass are exploited by CMS, but in this study only the pruned mass is used, as it was the technique recommended by CMS for the W-tagging~\cite{website:Wtagging}. The pruning procedure~\cite{Ellis:2009su} rejects large angle and soft constituents during the re-clustering iterations. Another variable used in W-tagging is called the N-subjettiness~($\tau_{N}$) which aims to discriminate how much the jet is consistent with the N-jet hypothesis. For example the merged jet of W boson should contain two jets and therefore be consistent with 2-jet hypothesis, not the 3-jet one. The N-subjettiness ratio $\tau_{21} = \tau_{2}/\tau_{1}$ has high discriminating power for merged W jets and therefore in this study a requirement on $\tau_{21}$ is used to tag merged W-jets. The distributions of the pruned mass and  $\tau_{21}$ is shown in Fig.~\ref{fig:figures/subjet}. In the plots it can be noticed, that  W+jets events mostly contain unmerged W bosons and therefore they have low pruned mass and lead to the broad $\tau_{21}$ distribution. The fraction of the pruned jets around 80~GeV, therefore merged W-jets, is enhanced in case of $t\bar{t}$ events, leading to low values of the $\tau_{21}$. Taking into account the discriminating variables, two definitions of the W-tagged jets referred to as ``ak8-W'' and ``ak10-W'' are presented in Table~\ref{tab:Wtags}.


    \insertFigure{figures/subjet}
                 {0.9}       % Width, in fraction of the whole page width
                 {Data and simulation distribution of the jet pruned mass (left) and the N-subjettiness ratio $\tau_{21}$ (right) for the W+jets events with the data-to-simulation ratios~\cite{Khachatryan:2014vla}.}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Name            & Clustering algorithm &      pruned mass $m_{W,p}$ [GeV]  &        $\tau_{21}$  & \pt [GeV]  \\
\hline
\hline
ak8-W  &        ak8                     &   60<$m_{W,p}$<100               & <0.5   & >200  \\
\hline
ak10-W  &        ak10                     &   60<$m_{W,p}$<100               & <0.5   & >200  \\
\hline
\end{tabular}
\caption[Table caption text]{ The definition of the ``ak8-W'' tagged and ``ak10-W'' tagged jet. }
\label{tab:Wtags}
\end{center}
\end{table}


This analysis defines four kinds of signal regions targeting signals with different kinematics. In this study, we focus only on the region groups with high $\Delta m $ referred to as ``High $\Delta m$'' and ``Boosted High $\Delta m$'' defined in Table~\ref{tab:SRnoW}. This study redefines the signal regions with the use of the number of W-tagged jets~($N_{W}$) instead of the \MET, but keeps the number of the signal regions the same. The proposed signal regions are shown in Table~\ref{tab:SRW}.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Name            & $N_{J}$  & $M_{T2}^{W}$~[GeV] & $E_{T}^{miss}$~[GeV]  \\
\hline
\hline
                & $\geq$4  & >200                & 250<$E_{T}^{miss}$<350   \\
High $\Delta m$ & $\geq$4  & >200                & 350<$E_{T}^{miss}$<450   \\
                & $\geq$4  & >200                & $E_{T}^{miss}$>450   \\
\hline
Boosted High $\Delta m$ & 3  & >200                & 250<$E_{T}^{miss}$<350   \\
                        & 3  & >200                & $E_{T}^{miss}$>350   \\
\hline
\end{tabular}
\caption[Table caption text]{ The signal regions of analysis~\cite{Sirunyan:2016jpr} . }
\label{tab:SRnoW}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Name            & $N_{J}$  & $M_{T2}^{W}$~[GeV]            & $E_{T}^{miss}$~[GeV]    & $N_{W}$ \\
\hline
\hline
                          & $\geq$4  & >200                & 250<$E_{T}^{miss}$<350   & --   \\
High $\Delta m$ W-tagging & $\geq$4  & >200                & $E_{T}^{miss}$>350       & 0    \\
                          & $\geq$4  & >200                & $E_{T}^{miss}$>350       & 1    \\
\hline
Boosted High $\Delta m$ W-tagging & 3  & >200              & $E_{T}^{miss}$>250 & 0  \\
                                  & 3  & >200              & $E_{T}^{miss}$>250 & 1  \\
\hline
\end{tabular}
\caption[Table caption text]{ Proposed signal regions using W-tagged jets multiplicity. }
\label{tab:SRW}
\end{center}
\end{table}

To evaluate the benefits of the W-tagging the expected significance in the default signal regions in Table~\ref{tab:SRnoW} are compared to the expected significance in the proposed W-tagging signal regions in Table.~\ref{tab:SRW}. The significance, described in Section~\ref{sec:stats}, is the expected significance when using generated pseudo-data with a signal strength of one instead of the observed data. For this comparison all relevant backgrounds as well as three different T2tt signal points with different stop and neutralino masses $(m_{\tilde{t}_{1}}, m_{\tilde{\chi}^{0}_{1}})$ are used. These signal point masses in GeV are (900,1), (800,300) and (650,450). In this study the significances are computed for two different luminosity options of 2.3~fb$^{-1}$ and 10~fb$^{-1}$ and two different options of W-tagging defined in Table~\ref{tab:Wtags}. The default signal regions and the newly proposed W-tagged signal regions has been differently combined, the best performing combination of the signal regions has been found to be the combination of the ``High $\Delta m$ W-tagging'' category with the ``Boosted High $\Delta m$'' category as shown in Table~\ref{tab:taggingResults}. In this table, the significances of the best performing combination of the signal regions for both ak8-W and ak10-W are shown together with significances of the default signal regions for three signal points and an integrated luminosity of 2.3~fb$^{-1}$. It can be noticed, that using ak10-W jets leads to a larger significance than ak8-W jets. Although there is an increase in the significance up to around 20~\% in some cases, the increase is not large enough to support the usage of complicated and highly consuming in computing resources W-taggers which have a significant gain only for high $\Delta m$ signals.  Compared to the full analysis, in this study many simplifications were done, only systematic uncertainty of 30\% was used and the background yields were taken directly from simulated samples. Therefore also large effort would be needed to provide background estimations and the systematics. Moreover it would be necessary to compute W-tagging efficiency and eventually  derive scale factors correcting differences between data and simulation.  But when studying the significances for integrated luminosity of  10~fb$^{-1}$  it was found that the gain in significance is up to around 30\% when using the W-tagging and therefore the W-tagging can be an interesting option for analyses based on data with larger integrated luminosity. 

%Table results
\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
$\mathcal{L} =$ 2.3~fb$^{-1}$             & ak4~\cite{Sirunyan:2017xse}      & ak8-W & ak10-W \\
\hline
Signal points                       & High $\Delta m$ +           &  High $\Delta m$ W-tagging +  &   High $\Delta m$ W-tagging +  \\
                       &  Boosted High $\Delta m$    & Boosted High $\Delta m$       &   Boosted High $\Delta m$  \\
\hline
\hline
(900,1) &     1.43 & 1.35 & 1.70  \\
\hline
(800,300) &   1.93 & 1.96 & 2.34  \\
\hline
(650,450) &   0.21 & 0.21 & 0.23  \\
\hline
\end{tabular}
\caption[Table caption text]{ Significances for three different T2tt signal points (first column) using the default SRs (second column) and then a mix of SRs defined by using ``ak8-W'' (third column) and ``ak10-W'' (fourth column) tagged jets multiplicity for the integrated luminosity of 2.3~fb$^{-1}$. }
\label{tab:taggingResults}
\end{center}
\end{table}

%(900,1) &     2.68 & 2.63 & 3.43  \\
%\hline
%(800,300) &   3.58 & 3.76 & 4.61  \\
%\hline
%(650,450) &   0.38 & 0.39 & 0.44  \\

%-expected significance, reject backfround hypothesis

%leptonic diagram~\cite{CMS:2016vew}
%MT2W - formula; explanation; supress lost lepton (already suppressed by requiring no additional lepton, but not enough); tries to reconstruct the event under tt2l and one undetected lepton assumption; for signal large delta M leads to alrge MT2W, while small delta M has lage MT2W; endpoint at top mass
%tmod - formula; chi2 like variable how well the event agrees with tt2l hypothesis, similar behavior to Mt2W; removing some of the terms from oifficial topness helps the discrimination; works better at low jet multiplicities than MT2W


\subsection{Baseline search region selection~\label{sec:baseline}}

With knowledge how the objects are selected or vetoed and  which kinematic variables help to suppress SM background while keeping high signal yields, the baseline search region is defined as follows:

\begin{itemize}
\item One selected primary vertex
\item Exactly one selected electron or muon
\item No veto lepton, isolated track or hadronic tau
\item At least two selected jets
\item At least on b-jet passing medium WP 
\item \MET>250~GeV
\item $M_{T}$>150~GeV
\item min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ > 0.8
\end{itemize}


\subsection{Signal regions~\label{sec:sr}}

In order to increase the sensitivity of the analysis to the signal, the baseline search region can be divided into signal regions with help of the discriminating variables defined in Section~\ref{sec:variables}. For the nominal search there are 8 signal region groups denoted from A to H, leading to 27 exclusive signal regions in total. To further suppress $W+jets \to 1\ell$ background, the tight b-tagging WP is used in regions where $M_{\ell b}>$175~GeV.  An additional group I of four signal regions is created for the T2tt compressed spectrum where 100<$\Delta m$<225~GeV. The definition of the signal regions is shown in Table~\ref{tab:SR}.


\begin{table}[h]
\begin{center}
\begin{tabular}{|l|lll|l|}
\hline
Group  &  $N_{J}$  & $t_{mod}$    &  $M_{\ell b}$ [GeV]     & \MET [GeV]                       \\
\hline
A      &  2-3      &   >10        &  $\leq$175              &  250-350, ~350-450, ~450-600, ~>600  \\
B      &  2-3      &   >10        &       >175              &  250-450, ~450-600, ~>600  \\
C      &  $\geq$4  &   $\leq$0    &  $\leq$175              &  250-350, ~350-450, ~450-550, ~550-650, ~>650  \\
D      &  $\geq$4  &   $\leq$0    &       >175              &  250-350, ~350-450, ~450-550, ~>550  \\
E      &  $\geq$4  &   0-10       &  $\leq$175              &  250-350, ~350-550, ~>550  \\
F      &  $\geq$4  &   0-10       &       >175              &  250-450, ~>450  \\
G      &  $\geq$4  &   >10        &  $\leq$175              &  250-350, ~350-450, ~450-600, ~>600  \\
H      &  $\geq$4  &   >10        &       >175              &  250-450, ~>450  \\
\hline
I      &  $\geq$5  &   --         &   --                    &  250-350, ~350-450, ~450-550, ~>550  \\
\hline
\end{tabular}
\caption[Table caption text]{The 27 signal regions of the nominal analysis divided into groups A to H. In the high $M_{\ell b}$ signal region groups B, D, F and H the b-tagged jet must pass the tight working point instead of the medium one. The group I is composed of 4 exclusive signal regions for the T2tt corridor analysis. }
\label{tab:SR}
\end{center}
\end{table}

In the compressed T2tt region the top quarks are produced off-shell, limiting the available momentum,  and therefore their decay products have very low \pt consequently leading to the low \MET. Obviously, such kind of signal has a different kinematics and the baseline selection and categorization into SRs must be adjusted to account for it. To increase the \MET in the event, the stop pair must be boosted against an initial state radiation~(ISR), leading to the requirement of  an additional jet in the event. The ISR jet should have the largest \pt, but such requirement does not help the overall signal discrimination as \MET>250~GeV is already required. The jet with the highest \pt must fail the medium b-tagging working point in order to increase the probability that it comes from the ISR. The stop system should be boosted in the opposite direction with respect to the ISR jet and thus the direction of objects originating from the stop pair decay, like the \MET and the lepton, should have a similar direction. As the \pt of objects in compressed region is not large, putting a limit on lepton \pt was found to be useful for the signal discrimination. Taking these considerations into account in the corridor region the following additional criteria are imposed

\begin{itemize}
\item At least five jets
\item One lepton with \pt$^{\ell}$<150~GeV
\item The leading jet is not b-tagged
\item $\Delta \phi(E_{T}^{miss}, \ell)$<2.0 
\item min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ is loosened to be larger than  0.5
\end{itemize}

In the previous versions of the analysis, the 2015 a beginning of 2016~\cite{Sirunyan:2016jpr, CMS:2016vew}, there is a lower number of signal regions, due to the smaller available data statistics. Also as discussed before in Section~\ref{sec:mt2w}, the $M_{T2}^{W}$ variable was used instead of $t_{mod}$ for definition of the majority of the signal regions. In the presented full 2016 analysis a new discriminating variable $M_{\ell b}$ was introduced. % and the signal regions were re-optimized with respect to the $N_{J}$ and \MET .


\section{Background estimations~\label{sec:estimations}}

In this section, the estimations of all relevant background yields in the signal regions together with systematic uncertainties on the background estimations are discussed. The lost lepton background, coming from the $t\bar{t} \to 2 \ell$ and single top ($tW$) processes where one of the two leptons is misreconstructed, is the largest contribution in the signal regions with low $M_{\ell b}$ values and at least four jets. This background is estimated in dilepton control regions~(CR). The one lepton background, represented by $W+jets \to 1\ell$  and $t\bar{t} \to 1\ell$ processes, is largely suppressed by the \MET and $M_{T}$ cuts. After these cuts, the yield of $t\bar{t} \to 1\ell$ process in the signal regions is small and can be estimated from simulations. The $W+jets \to 1\ell$  process leads to a larger contribution especially in regions with high $M_{\ell b}$ and is estimated from zero b-tag control regions. The $Z \to \nu \bar{\nu}$ background is the last relevant background group composed by the $t\bar{t}Z$ and $WZ$ processes, in which the Z boson decays to neutrinos and one W boson decays leptonically. The yields of $Z \to \nu \bar{\nu}$ in the signal regions are estimated from simulation, and corrected by an overall normalization factor measured in a three lepton control region.  

My main contribution to the full 2016 version of analysis was to provide the estimation of the $Z \to \nu \bar{\nu}$ background. In this section the estimation of this background is therefore described first and than built on the knowledge of the $Z \to \nu \bar{\nu}$ background estimation method, the other background estimates are presented.


\subsection{The $Z \to \nu \bar{\nu}$ background estimation}

The $Z \to \nu \bar{\nu}$ background, which arises from $t\bar{t}Z$ and $WZ$ processes with one leptonically decaying W boson and one Z boson decaying to neutrinos, represents overall the smallest relevant group of backgrounds, but it is largely enhanced in signal regions with high \MET and high $t_{mod}$ values. The main contribution to the $Z \to \nu \bar{\nu}$ background comes from the $t\bar{t}Z$ process, however at low jet multiplicities, the $WZ$ process becomes an important part of this background.  The previous versions of this analysis estimated this background purely from simulation, because of the low statistics in the three lepton control region. Now with a larger dataset, the normalization of these two processes is derived from a data-driven method, but the relative splitting of events between signal regions (shape) is taken from simulation. Then in the case that the shapes of the discriminating variables differ between data and simulation, the background estimation can be influenced by migration of events between the signal regions. Therefore the target is to define as many control regions and the signal ones, once there is sufficient statistics. 

\subsubsection{Principle of a data driven estimate}

To explain the data-driven background estimation technique, let's consider only one signal and one control region. This CR is a region enriched in events of the process under study and is typically defined similarly as signal region, just with the change of a certain requirement, in the case of the $Z \to \nu \bar{\nu}$ background, the selection on the lepton multiplicity is modified. For the $Z \to \nu \bar{\nu}$ background the three lepton control region is chosen, in which the Z boson decays to two leptons instead of two neutrinos. The relationship between the data~($N^{Data}$) and simulation~($N^{MC}$) yields in the SR and control CR can be written as follows

\eq{bkgEst}
{
\frac{N^{Data,~SR}}{N^{Data,~CR}}  = \frac{N^{MC,~SR}}{N^{MC,~CR}}.
}
Therefore the estimated background yield in the SR, $N^{Data~estimate,~SR}$, can be obtained from the simulated yield in signal region, $N^{MC,~SR}$, and the normalization factor, $NF^{CR}$, determined in the CR by rewriting the Eq.~\ref{eq:bkgEst} as

\eq{bkgEst2}
{
N^{Data~estimate,~SR}  = N^{MC,~SR} \times  \frac{N^{Data,~CR}}{ N^{MC,~CR}} = N^{MC,~SR} \times  NF^{CR}.
}
However, in the analysis there is not only one single signal region and thus the background yield estimation in Eq.~\ref{eq:bkgEst2} needs to be performed for each signal region separately. 

Ideally the same number of control regions as signal regions should be used to keep the kinematics as close as possible between the SR and CR. Consequently there should be the same number of normalization factors as of signal regions.  But the three lepton control regions suffer from the lack of statistics and therefore  only one control region can be used in the estimation of the $Z \to \nu \bar{\nu}$ background. However two normalization factors are determined, one for each process ($t\bar{t}Z$, $WZ$) separately  from a template fit of the $N_{b}$ distribution in the three lepton control region as described in the following subsection. For the $Z \to \nu \bar{\nu}$ background the Eq.~\ref{eq:bkgEst2} can be rewritten as

\eq{ZnunuEst}
{
N_{proc}^{Data~estimate,~SR}  = N_{proc}^{MC,~SR} \times NF_{proc,~3\ell}^{CR},
}
where $proc$ = $t\bar{t}Z$ or $WZ$.

%TODO start here

\subsubsection{Scale factors from the three lepton control region}

The normalization factors for the $t\bar{t}Z$ and  $WZ$ processes were determined in the  paper~\cite{Sirunyan:2017uyt} searching for BSM physics in final states with two leptons of same sign, \MET, and jets. For this purpose a three lepton control region is defined by the requirements:
\begin{itemize}
\item Two leptons passing the nominal isolation and identification criteria as defined in~\cite{Sirunyan:2017uyt}, the leading lepton should have \pt>25~GeV and the trailing one a \pt>20~GeV
\item A third lepton of \pt>10~GeV should pass the tight identification and must have an opposite sign and the same flavor as one of the leptons above; the formed lepton pair must have a mass within a $\pm$15~GeV window around the Z boson mass
\item $H_{T}$ = $\displaystyle\sum_{jets} p_{T}$>80~GeV
\item $N_{J} \geq 2$ with the \pt>40~GeV
\item No requirement on b-tagging
\item \MET>~30~GeV.
\end{itemize}

The $t\bar{t}Z$ and  $WZ$ processes are expected to have a different number of b-jets and  therefore a fit of the number of b-jet distribution allows to extract the normalization factors of $t\bar{t}Z$ and $WZ$. The zero b-jet category of CR is composed at $\sim$70\% by the $WZ$ process, while the $t\bar{t}Z$ process represents around 70\% of the rest of the b-jet distribution. In addition to these two processes, there is also a contribution to the control region from non-prompt leptons, $t\bar{t}H$,  $t\bar{t}W$, $WW$ processes as well as processes collectively denoted as ``rare SM'' and ``$X+\gamma$''. The rare SM contribution arises mainly from diboson (ZZ), triboson (WWW, WWZ, WZZ, ZZZ), Higgs (HZZ, VH where V=Z,W), same-sign WW from double-parton scattering (DPS WW), and rare top (tZq and $\mathrm{t\bar{t}t\bar{t}}$) processes decaying to three leptons. The ``$X+\gamma$'' group is composed by processes where one of the selected leptons is an electron from an unidentified photon conversion, such processes being predominantly $W\gamma$, $Z\gamma$, $t\bar{t}\gamma$ and $t\gamma$.

The distribution of the number of b-jets in data is simultaneously fitted by the discussed simulated components, letting only the normalization of all contributions free to vary. This fit procedure results in a normalization factor for $t\bar{t}Z$ of $1.14 \pm 0.30$ and for $WZ$ of $1.26 \pm 0.09$. The uncertainty on the normalization factors includes both the statistical uncertainty and an uncertainty on the shape of the b-jet distribution in the simulation. The post-fit distribution of number of b-jets in the three lepton control region is shown in Fig.~\ref{fig:figures/nbtagspostfit}.

    \insertFigure{figures/nbtagspostfit} % Filename = label
                 {0.4}       % Width, in fraction of the whole page width
                 { The post-fit distribution of the number of b-tagged jets in the three lepton control region~\cite{Sirunyan:2017uyt}. The distribution in data was simultaneously fitted by the SM background components to obtain the normalization factors for the $t\bar{t}Z$ and $WZ$ processes. The top panel shows data-to-simulation ratio with uncertainty composed by the statistical uncertainty and an uncertainty on the shape of the b-jet distribution in the simulation.  }

\subsubsection{Estimation of the $Z \to \nu \bar{\nu}$ background }

The final estimation of the $t\bar{t}Z$ and $WZ$ yields computed according to Eq.~\ref{eq:ZnunuEst} is shown in Table~\ref{tab:YZnunu}. The simulated events are corrected by the objects scale factors for the different lepton and b-tagging efficiencies in data and simulation which is a standard procedure for all background estimates. On top of this, the events in $t\bar{t}Z$ sample are reweighted in order to improve the modeling of jets coming from the ISR. The uncertainties on the estimates include both statistical and systematic uncertainties, which are described later in the following subsection.


\begin{table}[h]
\centering
%\noindent\hrulefill
%\smallskip\noindent
%\resizebox{0.7\linewidth}{!}{%
\scalebox{0.8}{
\begin{tabular}{|l|ccc|}
\hline
&
\textbf{ttZ}    &
\textbf{WZ}     &
\textbf{Total}  \\
\hline
\textbf{ A 250$<E_T^{miss}<$350}         & 3.33 $\pm$ 0.96       & 1.38 $\pm$ 0.58       & 4.71 $\pm$ 1.21       \\
\textbf{ A 350$<E_T^{miss}<$450}         & 1.85 $\pm$ 0.53       & 0.21 $\pm$ 0.53       & 2.05 $\pm$ 0.75       \\
\textbf{ A 450$<E_T^{miss}<$600}         & 1.15 $\pm$ 0.33       & 0.47 $\pm$ 0.39       & 1.62 $\pm$ 0.53       \\
\textbf{ A $E_T^{miss}>$600}     & 0.36 $\pm$ 0.11       & 0.36 $\pm$ 0.41       & 0.71 $\pm$ 0.40       \\
\textbf{ B 250$<E_T^{miss}<$450}         & 0.61 $\pm$ 0.18       & 0.93 $\pm$ 0.47       & 1.54 $\pm$ 0.52       \\
\textbf{ B 450$<E_T^{miss}<$600}         & 0.15 $\pm$ 0.05       & 0.20 $\pm$ 0.34       & 0.35 $\pm$ 0.33       \\
\textbf{ B $E_T^{miss}>$600}     & 0.09 $\pm$ 0.03       & 0.02 $\pm$ 0.26       & 0.11 $\pm$ 0.26       \\
\textbf{ C 250$<E_T^{miss}<$350}         & 14.28 $\pm$ 3.84      & 0.10 $\pm$ 0.62       & 14.38 $\pm$ 3.92      \\
\textbf{ C 350$<E_T^{miss}<$450}         & 3.83 $\pm$ 1.06       & 0.60 $\pm$ 0.48       & 4.43 $\pm$ 1.23       \\
\textbf{ C 450$<E_T^{miss}<$550}         & 1.06 $\pm$ 0.31       & 0.73 $\pm$ 0.39       & 1.79 $\pm$ 0.52       \\
\textbf{ C 550$<E_T^{miss}<$650}         & 0.40 $\pm$ 0.12       & 0.00 $\pm$ 0.00       & 0.40 $\pm$ 0.12       \\
\textbf{ C $E_T^{miss}>$650}     & 0.20 $\pm$ 0.06       & 0.00 $\pm$ 0.00       & 0.20 $\pm$ 0.06       \\
\textbf{ D 250$<E_T^{miss}<$350}         & 2.06 $\pm$ 0.56       & 0.95 $\pm$ 0.63       & 3.01 $\pm$ 0.91       \\
\textbf{ D 350$<E_T^{miss}<$450}         & 0.62 $\pm$ 0.18       & 0.55 $\pm$ 0.40       & 1.18 $\pm$ 0.41       \\
\textbf{ D 450$<E_T^{miss}<$550}         & 0.24 $\pm$ 0.07       & 0.21 $\pm$ 0.22       & 0.45 $\pm$ 0.24       \\
\textbf{ D $E_T^{miss}>$550}     & 0.09 $\pm$ 0.03       & 0.00 $\pm$ 0.00       & 0.09 $\pm$ 0.03       \\
\textbf{ E 250$<E_T^{miss}<$350}         & 7.88 $\pm$ 2.14       & 0.40 $\pm$ 0.44       & 8.27 $\pm$ 2.21       \\
\textbf{ E 350$<E_T^{miss}<$550}         & 3.34 $\pm$ 0.94       & 0.52 $\pm$ 0.39       & 3.87 $\pm$ 1.07       \\
\textbf{ E $E_T^{miss}>$550}     & 0.26 $\pm$ 0.08       & 0.03 $\pm$ 0.25       & 0.29 $\pm$ 0.26       \\
\textbf{ F 250$<E_T^{miss}<$450}         & 0.98 $\pm$ 0.27       & 0.13 $\pm$ 0.17       & 1.11 $\pm$ 0.33       \\
\textbf{ F $E_T^{miss}>$450}     & 0.21 $\pm$ 0.06       & 0.00 $\pm$ 0.00       & 0.21 $\pm$ 0.06       \\
\textbf{ G 250$<E_T^{miss}<$350}         & 2.89 $\pm$ 0.78       & 0.14 $\pm$ 0.15       & 3.03 $\pm$ 0.81       \\
\textbf{ G 350$<E_T^{miss}<$450}         & 2.51 $\pm$ 0.71       & 0.16 $\pm$ 0.18       & 2.67 $\pm$ 0.77       \\
\textbf{ G 450$<E_T^{miss}<$600}         & 1.80 $\pm$ 0.50       & 0.16 $\pm$ 0.16       & 1.96 $\pm$ 0.54       \\
\textbf{ G $E_T^{miss}>$600}     & 0.70 $\pm$ 0.20       & 0.00 $\pm$ 0.00       & 0.70 $\pm$ 0.23       \\
\textbf{ H 250$<E_T^{miss}<$450}         & 0.37 $\pm$ 0.11       & 0.00 $\pm$ 0.00       & 0.37 $\pm$ 0.11       \\
\textbf{ H $E_T^{miss}>$450}     & 0.33 $\pm$ 0.10       & 0.16 $\pm$ 0.17       & 0.49 $\pm$ 0.20       \\
\hline
\textbf{ I 250$<E_T^{miss}<$350}         & 3.66 $\pm$ 1.01       & 0.66 $\pm$ 0.43       & 4.33 $\pm$ 1.16       \\
\textbf{ I 350$<E_T^{miss}<$450}         & 1.57 $\pm$ 0.45       & 0.35 $\pm$ 0.34       & 1.93 $\pm$ 0.59       \\
\textbf{ I 450$<E_T^{miss}<$550}         & 0.58 $\pm$ 0.17       & 0.19 $\pm$ 0.20       & 0.77 $\pm$ 0.27       \\
\textbf{ I $E_T^{miss}>$550}     & 0.41 $\pm$ 0.13       & 0.17 $\pm$ 0.17       & 0.58 $\pm$ 0.22       \\
\hline
\end{tabular}}
\caption[Table caption text]{The estimated $t\bar{t}Z$ and $WZ$ yields in the signal regions A to I with their uncertainties. }
\label{tab:YZnunu}
\end{table}

\subsubsection{Systematic uncertainties on the $Z \to \nu \bar{\nu}$ background estimation}


There are several uncertainties entering into the background estimate, one is the uncertainty due to limited statistics in simulated and control data samples and others arise for example from the differences in modeling of the physics objects in data and simulation or uncertainties on theoretical quantities used for the production of simulated samples. Many uncertainties are common to all background estimates and therefore here again, due to my personal involvement, the systematic uncertainties for the $Z \to \nu \bar{\nu}$ background is described in a greater detail. Then the differences for other backgrounds are discussed.

\textbf{Theoretical uncertainties}

There are two kinds of systematic uncertainties affecting the background estimate. The first group is referred to as ``theoretical uncertainties'' and consists of uncertainties arising from the choice of the factorization and renormalization scale, PDFs and $\alpha_{S}$. As for the $Z \to \nu  \bar{\nu}$ processes the background yields in signal regions are taken from simulation and only normalized by an overall factor determined by the data-driven method, this background estimate is sensitive to the migrations of events between signal regions due to imprecise modeling of the theoretical parameters in the simulation. Because of the limited statistics in the signal regions, large fluctuations driven by the statistics were observed when varying the theoretical parameters. To avoid this, for evaluation of systematic uncertainties, the binning of signal regions in $M_{\ell b}$ and $t_{mod}$ was removed and only the binning in \MET and $N_{J}$ was kept. Effectively, we grouped signal regions across groups A--B and C--H, the corridor region is treated separately. To be sure that the uncertainty is not underestimated due to grouping in $M_{\ell b}$ and $t_{mod}$, the distributions of these variables were studied for different values of the theoretical parameters. Only small variations in shape of $M_{\ell b}$ and $t_{mod}$ were observed when varying the theoretical parameters. On the other hand large variations in shapes of \MET and $N_{J}$ were seen. In these grouped regions, the relative uncertainty on the background estimate was evaluated. The relative uncertainty was then used to compute the absolute uncertainties on the estimate in the corresponding signal regions.  The theoretical uncertainties are discussed one by one in the following paragraphs.


\begin{description}
\item[Factorization and renormalization scales]
The cross section of a given process depends on the choice of the factorization and renormalization scales. The theoretical $t\bar{t}Z$ cross section at 13~TeV is 839.3~fb~\cite{deFlorian:2016spz}, with a quoted scale uncertainty around 10\%. As the normalization of the background estimate was derived from data, the scale uncertainty should only influence the shapes and consequently the migration of events between the signal regions. In this analysis both renormalization and factorization scales are varied simultaneously, while keeping the cross section constant. The uncertainty was then evaluated from the change of background estimate when varying the scales. The scale uncertainty on the $Z \to \nu \bar{\nu}$ estimation in each signal region is shown in Table~\ref{tab:SysZnunu}, overall it is lower than 10\%.

%The central value for renormalization and factorization scales is set to 0=Mt+MV/2.
%For the NLO QCD part of the calculation scale uncertainties are estimated by independent variations of
%renormalization and factorization scales in the range /2R,F2, with 1/2R/F2,while for PDF and s uncertainties the PDF4LHC15 prescription is used. The resulting uncertainties are applied also to NLO EW correction effects
%str 156 handbook

\item[Parton distribution functions]
The NNPDF3.0~\cite{Ball:2014uwa} set of parton distribution functions~(PDF) was used for the production of simulated samples. To evaluate the uncertainty arising from the PDFs, 100 variations of PDFs were computed and each was used to calculate an event weight $y_{i,n}$, where $i$ denotes the i-th event and $n$ the n-th variation. Then the standard deviation of the PDF variations for an event $i$, computed as

\eq{pdfVar}
{
\sigma_{i} = \sqrt{\frac{1}{99} \sum_{n=1}^{100} (y_{i,n} - \bar{y}_{i})^{2}}, ~\mathrm{with}~ \bar{y}_{i} = \frac{1}{100} \sum_{n=1}^{100} y_{i,n},
}
was used to evaluate the PDF uncertainties. Here again only shape uncertainties are taken into account by keeping the normalization constant. The uncertainties due to PDFs go up to 10\% as shown in Table~\ref{tab:SysZnunu} for the $Z \to \nu \bar{\nu}$ estimate in all signal regions.

\item[Strong coupling constant $\alpha_{S}$]
The uncertainties on the background estimate coming from $\alpha_{S}$ are as well evaluated with help of the event weights. The $\alpha_{S}$ is was varied from its default value of 0.118  within its uncertainty to 0.117 and 0.119~\cite{Patrignani:2016xqp}.  The interest is in the shape uncertainties and therefore again the normalization was forced to be constant when varying $\alpha_{S}$. The final uncertainties are of the order of 3\% and are show in Table~\ref{tab:SysZnunu} for $Z \to \nu \bar{\nu}$ background estimate.
\end{description}

Figure~\ref{fig:figures/ttZUnc} shows the $t\bar{t}Z$ \MET distribution in grouped regions of A--B~(left) and C--H(right) with a band corresponding to combined scales, PDFs and $\alpha_{S}$ uncertainties. It can be noticed that the uncertainties are very small for low \MET, but grow with \MET up to around 10\%.

    \insertTwoFigures{figures/ttZUnc}
                 {figures/AB250lessMETlessInfCanFrank} % Filename = label
                 {figures/CDEFGH250lessMETlessInfCanFrank} % Filename = label
                 {0.49}       % Width, in fraction of the whole page width
                 { The predicted \MET distribution of the $t\bar{t}Z$ process with a band coming from combined uncertainties on scales, PDFs and $\alpha_{S}$ uncertainties. The regions A--B (left) and C--H (right) were grouped in the $M_{\ell b}$ and $t_{mod}$ variables. The ratio plots depict the maximal variations from the default. }

\textbf{Experimental uncertainties}

The second group of systematic uncertainties is referred to as ``experimental uncertainties''. Part of the uncertainties originate from a non-perfect modeling in the simulation of the light and b-jets b-tagging, or of the lepton ID/isolation efficiencies. To correct for that, the uncertainties on the background yield evaluated from the variations of scale factors within their uncertainties as prescribed by the corresponding physics objects groups of CMS. The impact on the yields is presented in Table.~\ref{tab:SysZnunu} for the $Z \to \nu \bar{\nu}$ background estimate. The uncertainties on the b-tagging and lepton scale factors are typically of order of few percent, except for a few outliers caused by limited statistics in the simulated samples.

Another source of uncertainties arises from the Jet Energy Scale~(JES). The Jet Energy Corrections~(JEC) must be applied on the reconstructed jets to have a correct energy of the jets. The uncertainties from variations of JES are of order of a few percent, although in few signal regions with small statistics they can reach up to three or four tens of percent. 

The uncertainty arising from differences in pileup profile in data and simulation was found to be extremely sensitive to the limited statistics and therefore, for the $Z \to  \nu \bar{\nu}$ processes, one global uncertainty was evaluated by varying the PU in the baseline search region only. The systematic uncertainty on the PU was determined to be lower than one percent in the baseline region, but a conservative estimate of 3\% was used to take into account variations due to categorization. 

As, for the $t\bar{t}Z$ process, the ISR reweighting was applied, the uncertainty arising from this procedure had to be evaluated by varying the ISR weight. The uncertainty introduced by the ISR reweighting was evaluated to be lower than 10\%. The systematic uncertainty on the luminosity was provided by the CMS and is  2.5\%~\cite{CMS-PAS-LUM-17-001}. The normalization uncertainty was computed by varying the normalization factors within their uncertainty.

The dominant uncertainty comes from limited statistics in the simulated samples. The statistical uncertainty varies from around 5\% to 90\% and in one signal region reaches up to 228\%.

The break-down of the systematic uncertainties on the $Z \to \nu \bar{\nu}$  background estimate in all signal regions is shown in Table~\ref{tab:SysZnunu}. The summarized typical order of magnitude of these uncertainties is presented in Table~\ref{tab:SysZnunuSum}. As only the normalization factors were derived by data-driven method and the shapes were taken from simulation, the uncertainties are very sensitive to the modeling of shape in the simulation. If there would be one CR per each SR, large part of uncertainties would largely cancel in the $N^{MC,~SR}/ N^{MC,~CR}$ ratio. For now, as the statistics in the three lepton control region is small, only CR can be used, but with the higher integrated luminosity, a division of the three lepton control regions will be possible.

%Like all experimentally-reconstructed objects, jets need to be calibrated in order to have the correct energy scale: this is the aim of the jet energy corrections (JEC). 

\begin{table}[h]
\begin{center}
\noindent\hrulefill
\smallskip\noindent
\resizebox{\linewidth}{!}{%
\begin{tabular}{|l|ccccccccccc|c|}
\hline
&
\textbf{MC stat.}  &
\textbf{$SF_{l}$}        &
\textbf{$SF_{b}$(light)}    &
\textbf{$SF_{b}$(heavy)}    &
\textbf{PU}         &
\textbf{PDFs}  &
\textbf{$\alpha_{S}$}       &
\textbf{Scales}   &
\textbf{ISR}   &
\textbf{JES}  &
\textbf{Norm.}        &
\textbf{Total}  \\
\hline
\textbf{ A 250$<E_T^{miss}<$350}         & 11.88         & 0.31          & 1.58          & 1.23          & 3.00          & 0.23          & 0.86          & 3.86          & 7.12          & 2.55          & 20.70         & 25.60         \\
\textbf{ A 350$<E_T^{miss}<$450}         & 25.04         & 0.35          & 3.63          & 0.77          & 3.00          & 3.33          & 0.61          & 1.31          & 9.09          & 0.37          & 24.38         & 36.61         \\
\textbf{ A 450$<E_T^{miss}<$600}         & 23.87         & 0.37          & 1.34          & 1.34          & 3.00          & 0.45          & 0.39          & 2.47          & 7.02          & 0.46          & 20.77         & 32.71         \\
\textbf{ A $E_T^{miss}>$600}    & 51.43         & 0.41          & 2.87          & 1.74          & 3.00          & 8.04          & 0.36          & 2.37          & 5.13          & 10.54         & 16.77         & 56.17         \\
\textbf{ B 250$<E_T^{miss}<$450}         & 29.98         & 0.50          & 1.67          & 1.23          & 3.00          & 1.17          & 0.76          & 2.86          & 3.97          & 1.01          & 14.74         & 34.01         \\
\textbf{ B 450$<E_T^{miss}<$600}         & 86.04         & 0.77          & 5.28          & 1.26          & 3.00          & 0.45          & 0.40          & 2.47          & 4.00          & 32.62         & 15.46         & 93.63         \\
\textbf{ B $E_T^{miss}>$600}    & 228.31        & 2.65          & 19.40         & 1.64          & 3.00          & 8.03          & 0.36          & 2.37          & 8.12          & 44.04         & 22.72         & 234.76        \\
\textbf{ C 250$<E_T^{miss}<$350}         & 4.56          & 0.45          & 0.06          & 0.68          & 3.00          & 0.81          & 0.60          & 1.77          & 0.40          & 4.78          & 26.18         & 27.26         \\
\textbf{ C 350$<E_T^{miss}<$450}         & 10.85         & 0.35          & 0.48          & 0.66          & 3.00          & 2.01          & 1.23          & 5.00          & 0.99          & 6.85          & 23.73         & 27.73         \\
\textbf{ C 450$<E_T^{miss}<$550}         & 20.70         & 0.31          & 4.53          & 0.26          & 3.00          & 1.89          & 1.34          & 4.86          & 2.03          & 4.52          & 18.54         & 29.25         \\
\textbf{ C 550$<E_T^{miss}<$650}         & 9.20          & 0.38          & 0.77          & 0.38          & 3.00          & 4.90          & 1.86          & 3.67          & 3.67          & 2.14          & 26.32         & 29.09         \\
\textbf{ C $E_T^{miss}>$650}    & 12.73         & 0.45          & 0.20          & 0.40          & 3.00          & 8.11          & 3.08          & 4.23          & 4.92          & 5.74          & 26.32         & 31.85         \\
\textbf{ D 250$<E_T^{miss}<$350}         & 19.82         & 0.31          & 0.72          & 0.41          & 3.00          & 0.81          & 0.60          & 1.76          & 0.82          & 9.76          & 20.24         & 30.20         \\
\textbf{ D 350$<E_T^{miss}<$450}         & 27.58         & 0.32          & 1.82          & 1.08          & 3.00          & 2.02          & 1.23          & 4.99          & 1.63          & 10.79         & 17.33         & 34.98         \\
\textbf{ D 450$<E_T^{miss}<$550}         & 47.40         & 0.33          & 4.96          & 0.22          & 3.00          & 1.88          & 1.33          & 4.85          & 2.24          & 17.87         & 17.31         & 54.17         \\
\textbf{ D $E_T^{miss}>$550}    & 18.94         & 0.44          & 0.06          & 0.83          & 3.00          & 7.53          & 2.44          & 2.55          & 7.31          & 7.15          & 26.32         & 35.14         \\
\textbf{ E 250$<E_T^{miss}<$350}         & 5.55          & 0.42          & 0.65          & 0.74          & 3.00          & 0.81          & 0.60          & 1.77          & 2.52          & 3.80          & 25.40         & 26.66         \\
\textbf{ E 350$<E_T^{miss}<$550}         & 10.40         & 0.40          & 0.01          & 0.83          & 3.00          & 1.97          & 1.26          & 4.96          & 0.66          & 7.23          & 23.72         & 27.63         \\
\textbf{ E $E_T^{miss}>$550}    & 87.72         & 0.77          & 7.05          & 0.02          & 3.00          & 7.55          & 2.47          & 2.51          & 2.26          & 1.66          & 24.61         & 91.86         \\
\textbf{ F 250$<E_T^{miss}<$450}         & 16.02         & 0.42          & 0.06          & 0.31          & 3.00          & 1.16          & 0.78          & 2.70          & 2.10          & 4.41          & 24.07         & 29.63         \\
\textbf{ F $E_T^{miss}>$450}    & 12.82         & 0.47          & 0.40          & 0.57          & 3.00          & 3.72          & 1.70          & 3.91          & 2.22          & 3.37          & 26.33         & 30.26         \\
\textbf{ G 250$<E_T^{miss}<$350}         & 5.75          & 0.39          & 0.01          & 1.71          & 3.00          & 0.81          & 0.60          & 1.77          & 2.09          & 3.44          & 25.42         & 26.68         \\
\textbf{ G 350$<E_T^{miss}<$450}         & 7.06          & 0.43          & 0.05          & 1.35          & 3.00          & 2.01          & 1.23          & 5.00          & 1.79          & 9.62          & 25.14         & 28.63         \\
\textbf{ G 450$<E_T^{miss}<$600}         & 9.01          & 0.37          & 0.19          & 1.27          & 3.00          & 2.63          & 1.54          & 4.72          & 0.51          & 5.15          & 24.77         & 27.64         \\
\textbf{ G $E_T^{miss}>$600}    & 7.00          & 0.47          & 0.19          & 1.01          & 3.00          & 8.01          & 2.34          & 1.74          & 0.68          & 15.62         & 26.32         & 32.70         \\
\textbf{ H 250$<E_T^{miss}<$450}         & 9.94          & 0.49          & 0.05          & 0.07          & 3.00          & 1.17          & 0.79          & 2.72          & 2.06          & 4.60          & 26.32         & 28.91         \\
\textbf{ H $E_T^{miss}>$450}    & 34.01         & 0.43          & 0.04          & 0.88          & 3.00          & 3.71          & 1.70          & 3.91          & 0.93          & 2.41          & 19.94         & 40.03         \\
\hline
\textbf{ I 250$<E_T^{miss}<$350}         & 9.76          & 0.38          & 0.78          & 0.47          & 3.00          & 0.21          & 0.32          & 3.91          & 4.30          & 5.32          & 23.38         & 26.72         \\
\textbf{ I 350$<E_T^{miss}<$450}         & 17.91         & 0.35          & 0.76          & 0.60          & 3.00          & 1.99          & 1.41          & 5.88          & 4.56          & 6.05          & 22.78         & 30.79         \\
\textbf{ I 450$<E_T^{miss}<$550}         & 25.04         & 0.51          & 4.83          & 0.35          & 3.00          & 1.89          & 1.62          & 4.90          & 5.38          & 4.89          & 21.64         & 34.80         \\
\textbf{ I $E_T^{miss}>$550}    & 29.62         & 0.40          & 0.22          & 0.68          & 3.00          & 1.82          & 1.55          & 7.48          & 4.76          & 6.34          & 20.76         & 37.98         \\
\hline
\end{tabular}}
\caption[Table caption text]{The break-down of the relative systematic uncertainties in \% on the $Z \to \nu \bar{\nu}$ background estimation in each signal region.   }
\label{tab:SysZnunu}
\end{center}
\end{table}


\begin{table}[h]
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Source}       & \textbf{Typical range of values} [\%]                       \\
\hline
\textbf{MC statistics}      &  5-88   \\
\hline
\textbf{Lepton SF}              &  <3  \\
\textbf{b-tagging SF (light flavor)}      &  1-19  \\
\textbf{b-tagging SF (heavy flavor)}      &  <2  \\
\textbf{Pileup}                          &  <3  \\
\textbf{PDFs}                 &  1-8  \\
\textbf{$\mathbf{\alpha_{S}}$}                 &  1-3  \\
\textbf{Scales}                 &  1-8  \\
\textbf{ISR}                 &  1-9  \\
\textbf{Jet energy scale}                 &  1-44  \\
\textbf{Normalization}                 &  15-26  \\
\hline
\textbf{Total}                 &  26-94  \\
\hline
\end{tabular}
\caption[Table caption text]{The typical order of magnitude of a given uncertainty for the $Z \to \nu \bar{\nu}$ background.  }
\label{tab:SysZnunuSum}
\end{center}
\end{table}

\subsection{The lost lepton background estimation}

The lost lepton background, composed of $t\bar{t}$, single top ($tW$), $t\bar{t}V$ and diboson processes decaying to two leptons where one lepton is misreconstructed, is the dominant background due to the large tails in \MET and $M_{T}$ distributions caused by the two neutrinos and the not selected lepton. The majority of the lost lepton background arises from the $t\bar{t} \to 2\ell$ process. The background yields in the signal regions are estimated according to Eq.~\ref{eq:bkgEst2} from the dilepton control regions. These control regions have a purity above 95\% in the $t\bar{t}$, $tW$, $t\bar{t}V$ and diboson processes and therefore no correction for contamination from other processes is needed. 

The control regions pass the same selection as the signal regions, except the veto on the second lepton, which is reversed. Because of the small yields in high \MET control regions, several control regions differing only by the \MET range had to be merged. Due to this combination of the control regions, the background estimate in a given signal region has to be corrected by corresponding interpolation factor. The lost lepton background estimate in signal region can be then written as

\eq{bkgEstLL}
{
N^{Data~estimate,~SR}_{lost~\ell}  = N^{MC,~SR}_{lost~\ell} \times  \frac{N^{Data,~CR}_{2\ell}}{ N^{MC,~CR}_{2\ell}} \times \frac{N^{MC,~SR,~E_{T}^{miss}~bin}_{lost~\ell}}{ N^{MC,~SR}_{lost~\ell}},
}
where the first term on the right side denotes the simulated yield in the signal region, the second term corresponds to data-to-simulation factor in the control region and the third factor is the simulation based interpolation factor, which is equal to one if a given control region was not merged with another one.

The described extrapolation of control regions in the \MET could lead to imprecise estimate if the \MET shapes in data and simulation differ. For this reason the shape of the \MET is checked in an region enriched top quark events. Data used for definition of this region are triggered by double lepton trigger with electron-muon pair in the final state. Then the data-to-simulation ratio in \MET bins is used as a correction of the simulated \MET spectra. 

The final estimate of the lost lepton background in the signal regions are shown in Table~\ref{tab:resultsAll}, together with uncertainties including both statistical and systematic uncertainties described in following subsection.

\subsubsection{Systematic uncertainties on the lost lepton background estimation}

As for the $Z \to \nu \bar{\nu}$ background, the uncertainties on the lost lepton background estimate include the uncertainties due to limited statistics in simulation, lepton and b-tagging scale factors, PU,  PDFs, $\alpha_{S}$, scales, ISR reweighting and JES, although many of them largely cancel in $N^{MC,~SR}_{lost \ell}/ N^{MC,~CR}_{2\ell}$ ratio used for the estimation of the lost lepton background yields in the SRs.

In addition to these uncertainties, the following uncertainties had to be evaluated:
\begin{itemize} 
\item Uncertainty arising from the trigger efficiency in the dilepton CR
\item Uncertainty on the \MET resolution which can cause a migration of events between signal regions: It cancels in $N^{MC,~SR}_{lost \ell}/ N^{MC,~CR}_{2\ell}$ ratio in case no \MET extrapolation is needed, but in the case that the \MET extrapolation is applied, the corresponding intrapolation factor is computed purely from the simulation. If the \MET resolution is different in data and simulation, there are different bin migrations in data and simulation and therefore the intrapolation factor is influenced by this difference
\item Uncertainty arising from different \MET shapes in data and simulation: Because of the extrapolation in \MET, the difference in \MET shape difference can again cause migration of events between the signal regions as described for the \MET resolution
\item Uncertainty on the efficiency of the hadronic tau and the isolated track vetos: As the control region is defined by requiring second lepton, the differences between efficiency for the hadronically decaying tau leptons between signal and control regions must be taken into account   %why we do not have this?
\end{itemize}

Here again the dominant uncertainty arises from the limited statistics in data and simulations which can go up to around 60\%.
%Additional correction factor obtained from studies of $\gamma$+jets processes is applied on background estimates to take into account differeneces in \MET resolution in data and simulation causing different migration between 
%An additional correction factor is used to take into account  ??!!
%gamma plus jets -> MET resolution > bin migrations - gamma pt spectrum reveighted to match neutrino pt spectrum, from reweighted events METmodified = reconstructed-MET + pTgamma -check that
%->ask about that -> not really clear from the papaer

\subsection{One lepton background estimation}


The  one lepton background is the sub-leading background composed of the $W+jets \to 1\ell$  and $t\bar{t} \to 1\ell$ processes. The $W+jets \to 1\ell$ background  is the largest part of the one lepton background and is estimated from control regions with zero b-jets. The control regions are defined in a same way as the signal regions, but no b-tagged jet is required. In regions where medium b-tagging WP is used, this requirement is inverted. In the majority of cases, a zero tight b-tags is required for definition of control regions corresponding to the $\geq$1 tight b-tag signal regions. But in some control regions of high $M_{\ell b}$, zero medium b-tags are used to reduce the systematics uncertainty and increase the purity. The  $t\bar{t} \to 1\ell$ represents only small contribution to the one lepton background and therefore is taken directly from the simulation.

In the zero b-jet control regions, the $M_{\ell b}$ variable needs to be redefined as there is no b-jet in the event. Instead of the closest b-jet, the jet with highest CSVv2 discriminator is used to define $M_{\ell b}$ in this case. Contrary to the previous background estimations there is enough statistics to define a corresponding zero b-tag control region to each signal region. On the other hand the control regions are not pure in the $W+jets \to 1\ell$  process and the background estimate have to account for the contamination by non-$W+jets \to 1\ell$  processes in the control regions. The $W+jets \to 1\ell$  background estimate in the signal regions can be then written as

\eq{bkgEstWJ}
{
N^{Data~estimate,~SR}_{W+jets,~\geq~1btag}  = N^{MC,~SR}_{W+jets,~\geq~1btag} \times  \frac{N^{Data,~CR}_{W+jets,~0btag}}{ N^{MC,~CR}_{W+jets,~0btag}},
}
where the first term on right side denotes the simulated yield in the signal region and second term corresponds to data-to-simulation factor in the control region for W+jets process. Because of the contamination in the control regions by other processes, the $N^{Data,~CR}_{W+jets,~0btag}$ is defined as follows


\eq{bkgEstWJT2}
{
N^{Data,~CR}_{W+jets,~0btag} = N^{Data,~CR}_{all,~0btag} \times \frac{N^{MC,~CR}_{W+jets,~0btag}}{ N^{MC,~CR}_{all,~0btag}} ,
}
where the second term on the right side is a simulation based purity factor which corrects the data yields in the control region for non-$W+jets \to 1\ell$  processes.

Because of the different $M_{\ell b}$ definition, it had to be investigated if the shape of this variable agrees between the control and signal regions. The study based on simulations confirmed, that the two shapes are in agreement. 

The contribution to the total background from $t\bar{t} \to 1\ell$ process is lower than 10\% and therefore it is reasonable to estimate the background yields directly from simulation. The $t\bar{t} \to 1\ell$ process appears in the signal regions mainly due to the \MET resolution. As this background is purely estimated from simulation the mismodeling of the \MET resolution could cause an imprecise estimate. The \MET resolution is checked with the help of $\gamma$+jets data and simulated samples. The \pt spectrum of the photon in data and simulation is reweighted to match the \pt spectrum of the neutrino in the $t\bar{t} \to 1\ell$. Then a modified \MET is computed as the sum of reweighted $p_{T}^{\gamma}$ of the originally reconstructed \MET. The data-to-simulation ratio for the modified \MET distribution can then reveal any \MET resolution mismodeling. It was found out that the difference between data and simulation for modified \MET is large, but no scale factors are derived and this mismodeling is only taken into account as a systematic uncertainty which was assessed to be 100\%. %really true? 
 
The final estimate of the one lepton background in the signal regions is shown in Table~\ref{tab:resultsAll}, together with uncertainties including both statistical and systematic uncertainties as described in the following subsection.

\subsubsection{Systematic uncertainties on the one lepton background estimation}

The uncertainties on the $W+jets \to 1\ell$  background estimate is very similar to the previous cases as they contain uncertainties due to the limited statistics in simulation, PU, JES, PDFs, scales, $\alpha_{S}$, b-tagging and lepton scale factors, although here as well many of them largely cancel in $N^{MC,~SR}_{W+jets,~\geq 1btag}/ N^{MC,~CR}_{W+jets,~0btag}$ ratio used for estimation of one lepton background yields in the SRs.

Except of these uncertainties following uncertainties had to be evaluated:
\begin{itemize} 
\item Uncertainty on the W+b cross section: For the background estimation, the simulation is used to determine a factor which transfers the data yield from the control to the signal region as can be seen in Eq.~\ref{eq:bkgEstWJ}. Therefore the background estimate is sensitive on the modeling of the W+b fraction in the simulation.
\item Uncertainty on the contamination of control regions by non-$W+jets \to 1\ell$.
\end{itemize}

In this case the dominant uncertainties arise from the limited statistics in data and simulation and also from the contamination of control regions by non-$W+jets \to 1\ell$  processes. These uncertainties can reach up to 90\%.

The uncertainty on $t\bar{t} \to 1\ell$ estimate was evaluated to be 100\% from the mentioned studies of the \MET resolution. All the other sources of the uncertainty are negligible.
%\subsection{Systematic uncertainties~\label{sec:systematics}}

%There are several uncertainties entering into the background estimations, one is the uncertainty due to limited statistics in simulated and control data samples and others arise for example from the differences in modeling of the physics objects in data and simulation. Many uncertainties are common to all background estimations and therefore here again, due to my personal involvement, the systematic uncertainties for the $Z \to \nu \bar{\nu}$ background are described first. Then the differences for other backgrounds are discussed.

\subsection{Uncertainties on the signal yields}

The signal uncertainties are evaluated similarly as the background ones.  As previously, the largest source of uncertainty is the limited statistics in the simulated samples which varies from 5 to 25\%. The signal uncertainties with their typical values are summarized in Table~\ref{tab:systAll}.

\begin{table}[htb]
\centering
\begin{tabular}{lc}%c}
\hline\hline
Source & Typical range of values [\%] \\%& shape effect \\
\hline
MC statistics & 5--25 \\
Scales & 2--4 \\
Integrated luminosity & 2.5 \\
Pileup & 1--4 \\
Trigger & 2--4 \\
b-tagging SF & 1--7 \\
Jet energy scale & 1--20 \\
Lepton SF & 1--4 \\
ISR & 2--15 \\
\MET modeling & 1--10 \\
\hline
Total  & 7--38 \\
\hline\hline
\end{tabular}
\caption{\label{tab:systAll} The typical order of magnitude of systematic uncertainties on the signal yields~\cite{Sirunyan:2017xse}.}
\end{table}


\subsubsection{Pileup uncertainty}

For the 2015 version of the analysis~\cite{Sirunyan:2016jpr} I performed studies of the dependency of the $M_{T}$ and \MET distributions in signal samples on the pileup. The results of this study were used to assess one of the systematic uncertainties on the signal yields. In the full 2016 analysis the pileup uncertainty was derived from standard CMS SUSY group recommendations~\cite{website:SUSYrec}.

The difference in the $M_{T}$ and \MET distributions as a function of pileup was evaluated starting from the baseline search region, removing the requirement on the $M_{T}$ and \MET variables, respectively. The total number of events $N$ was divided into bins with the number of primary vertices  $N_{vtx}$ in the event. In each  bin $i$ of the $N_{vtx}$ distribution, the number of events passing the criterion $M_{T}$>150~GeV~($N_{i,M_{T}>150}$) was divided by the total number of events in bin $i$~($N_{i}$) and scaled by $N$. The same computation was performed for the number of events in bin $i$ passing \MET>250~GeV~($N_{i,E_{T}^{miss}>250}$). The evolution of the variables $N \cdot N_{i,M_{T}>150}/N_{i}$ and $N \cdot N_{i,E_{T}^{miss}>250}/N_{i} \times N$ as a function of $N_{vtx}$ for the T2tt signal points with $m_{\tilde{t}_{1}}$=225~GeV and $M_{\tilde{\chi}_{1}^{0}}$ between 25 and 150~GeV, are shown in Fig.~\ref{fig:figures/pileupUnc}. In the plots it can be noticed that  the distributions are not flat and varying $N_{vtx}$ by five units leads to uncertainty up to around 7\%. This value was used as an uncertainty for the low $\Delta m$ signals. The variations are smaller for high $\Delta m$ signals, leading to the uncertainty up to 2\%.

    \insertTwoFigures{figures/pileupUnc}
                 {figures/tailnrMT22525to150nNEW} % Filename = label
                 {figures/tailnrMET22525to150nNEW} % Filename = label
                 {0.49}       % Width, in fraction of the whole page width
                 { The evolution  of the fraction of events in the high $M_{T}>150$~GeV (left) and \MET>250~GeV (right) part of distribution to the total number of events as a function of the reconstructed primary vertex multiplicity. }

\newpage

\section{Results and interpretation~\label{sec:results}}


The results on the background estimates together with the observed data in the signal regions are shown in Table~\ref{tab:resultsAll}. These results are graphically presented in Fig.~\ref{fig:figures/resultPlot}. In this figure the yields of three different signal models for a chosen signal point are also shown. 

In the signal regions no significant excess of data from the SM expectation was observed and therefore the $\mathrm{CL_{s}}$ method described in Section~\ref{sec:stats} was used to interpret the results as limits on the considered SMS signal models. These limits on model production cross-sections are given as a function of the SUSY particles masses, in this case the stop and neutralino masses. They are obtained by combining all signal regions for a given signal point. A contour line is drawn when the upper limit on the cross section is equal to the theoretical cross section given in~\cite{Borschensky:2014cia}. The signal points with an upper limit on the cross section lower than the theoretical cross section are excluded. The signal yields entering the statistical procedure are corrected for a possible contamination in the control regions by SUSY signal, these corrections are around 5\% to 10\%.

\begin{table}[htb]
\footnotesize
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{$N_{J}$} & \multirow{2}{*}{$t_{mod}$} & $M_{\ell b}$ & \MET & Lost  & \multirow{2}{*}{1$\ell$ (top)} & 1$\ell$ (not & \multirow{2}{*}{$Z\rightarrow\nu\bar{\nu}$} & Total & \multirow{2}{*}{Data} \\
  &  &  [GeV] &  [GeV] &  lepton &  &  top) &  & background &  \\
\hline
$\leq3$ &    $>10$ & $\leq175$ & $250-350$ & 53.9$\pm$6.2 & $<0.1$ & 7.2$\pm$2.5 & 4.7$\pm$1.2 & 65.8$\pm$6.8 & 72 \\
$\leq3$ &    $>10$ & $\leq175$ & $350-450$ & 14.2$\pm$2.4 & 0.2$\pm$0.2 & 4.1$\pm$1.4 & 2.1$\pm$0.8 & 20.5$\pm$2.9 & 24 \\
$\leq3$ &    $>10$ & $\leq175$ & $450-600$ & 2.9$\pm$0.9 & 0.1$\pm$0.1 & 1.7$\pm$0.7 & 1.6$\pm$0.5 & 6.4$\pm$1.3 & 6 \\
$\leq3$ &    $>10$ & $\leq175$ &    $>600$ & 0.6$\pm$0.5 & 0.3$\pm$0.3 & 0.8$\pm$0.3 & 0.7$\pm$0.4 & 2.4$\pm$0.8 & 2 \\
\hline
$\leq3$ &    $>10$ &     $>175$ & $250-450$ & 1.7$\pm$0.8 & $<0.1$ & 5.6$\pm$2.2 & 1.5$\pm$0.5 & 8.9$\pm$2.4 & 6 \\
$\leq3$ &    $>10$ &     $>175$ & $450-600$ & 0.02$\pm$0.01 & $<0.1$ & 1.6$\pm$0.6 & 0.4$\pm$0.3 & 1.9$\pm$0.7 & 3 \\
$\leq3$ &    $>10$ &     $>175$ &    $>600$ & 0.01$\pm$0.01 & $<0.1$ & 0.9$\pm$0.4 & 0.1$\pm$0.3 & 1.0$\pm$0.5 & 2 \\
\hline
$\geq4$ & $\leq0$ & $\leq175$ & $250-350$ & 346$\pm$30 & 13.2$\pm$13.2 & 9.7$\pm$8.6 & 14.4$\pm$3.9 & 383$\pm$34 & 343 \\
$\geq4$ & $\leq0$ & $\leq175$ & $350-450$ & 66.3$\pm$7.9 & 2.3$\pm$2.3 & 2.5$\pm$1.7 & 4.4$\pm$1.2 & 75.5$\pm$8.5 & 68 \\
$\geq4$ & $\leq0$ & $\leq175$ & $450-550$ & 12.1$\pm$2.8 & 0.6$\pm$0.6 & 0.5$\pm$0.5 & 1.8$\pm$0.5 & 15.0$\pm$2.9 & 13 \\
$\geq4$ & $\leq0$ & $\leq175$ & $550-650$ & 3.4$\pm$1.5 & 0.1$\pm$0.1 & 0.3$\pm$0.2 & 0.4$\pm$0.1 & 4.1$\pm$1.5 & 6 \\
$\geq4$ & $\leq0$ & $\leq175$ &    $>650$ & 5.9$\pm$2.8 & $<0.1$ & 0.4$\pm$0.4 & 0.2$\pm$0.1 & 6.6$\pm$2.9 & 2 \\
\hline
$\geq4$ & $\leq0$ &     $>175$ & $250-350$ & 26.0$\pm$4.3 & 3.1$\pm$3.1 & 7.5$\pm$3.0 & 3.0$\pm$0.9 & 39.7$\pm$6.2 & 38 \\
$\geq4$ & $\leq0$ &     $>175$ & $350-450$ & 10.4$\pm$2.6 & 0.6$\pm$0.6 & 1.6$\pm$0.7 & 1.2$\pm$0.4 & 13.7$\pm$2.8 & 8 \\
$\geq4$ & $\leq0$ &     $>175$ & $450-550$ & 1.7$\pm$0.9 & 0.4$\pm$0.4 & 0.6$\pm$0.3 & 0.5$\pm$0.2 & 3.1$\pm$1.1 & 2 \\
$\geq4$ & $\leq0$ &     $>175$ &    $>550$ & 1.1$\pm$0.8 & $<0.1$ & 1.0$\pm$0.6 & 0.09$\pm$0.03 & 2.2$\pm$1.0 & 1 \\
\hline
$\geq4$ &   $0-10$ & $\leq175$ & $250-350$ & 43.0$\pm$5.9 & 1.7$\pm$1.7 & 5.7$\pm$3.0 & 8.3$\pm$2.2 & 58.7$\pm$7.2 & 65 \\
$\geq4$ &   $0-10$ & $\leq175$ & $350-550$ & 9.1$\pm$2.0 & 0.5$\pm$0.5 & 1.2$\pm$0.5 & 3.9$\pm$1.1 & 14.7$\pm$2.4 & 23 \\
$\geq4$ &   $0-10$ & $\leq175$ &    $>550$ & 0.6$\pm$0.3 & 0.3$\pm$0.3 & 0.3$\pm$0.2 & 0.3$\pm$0.3 & 1.5$\pm$0.6 & 1 \\
\hline
$\geq4$ &   $0-10$ &     $>175$ & $250-450$ & 4.4$\pm$1.4 & 0.3$\pm$0.3 & 3.1$\pm$1.3 & 1.1$\pm$0.3 & 8.9$\pm$1.9 & 9 \\
$\geq4$ &   $0-10$ &     $>175$ &    $>450$ & 0.10$\pm$0.17 & $<0.1$ & 0.2$\pm$0.3 & 0.2$\pm$0.1 & 0.6$\pm$0.2 & 0 \\
\hline
$\geq4$ &    $>10$ & $\leq175$ & $250-350$ & 9.5$\pm$2.3 & 0.8$\pm$0.8 & 1.1$\pm$0.9 & 3.0$\pm$0.8 & 14.3$\pm$2.7 & 12 \\
$\geq4$ &    $>10$ & $\leq175$ & $350-450$ & 5.9$\pm$1.8 & 0.7$\pm$0.7 & 0.7$\pm$0.5 & 2.7$\pm$0.8 & 10.0$\pm$2.1 & 9 \\
$\geq4$ &    $>10$ & $\leq175$ & $450-600$ & 3.8$\pm$1.3 & 0.1$\pm$0.1 & 0.4$\pm$0.3 & 2.0$\pm$0.5 & 6.3$\pm$1.5 & 3 \\
$\geq4$ &    $>10$ & $\leq175$ &    $>600$ & 0.8$\pm$0.6 & 0.7$\pm$0.7 & 0.3$\pm$0.4 & 0.7$\pm$0.3 & 2.4$\pm$1.0 & 0 \\
\hline
$\geq4$ &    $>10$ &     $>175$ & $250-450$ & 0.5$\pm$0.3 & $<0.1$ & 1.0$\pm$0.6 & 0.4$\pm$0.1 & 1.9$\pm$0.7 & 0 \\
$\geq4$ &    $>10$ &     $>175$ &    $>450$ & 0.2$\pm$0.2 & 0.1$\pm$0.1 & 0.5$\pm$0.3 & 0.5$\pm$0.2 & 1.3$\pm$0.4 & 2 \\
\hline
\hline
\multicolumn{3}{|l|}{Compressed region} & $250-350$ & 67.5$\pm$8.9 & 5.3$\pm$5.3 & 5.0$\pm$1.8 & 4.3$\pm$1.2 & 82$\pm$11 & 72 \\
\multicolumn{3}{|l|}{Compressed region} & $350-450$ & 15.1$\pm$3.5 & 1.0$\pm$1.0 & 0.8$\pm$0.3 & 1.9$\pm$0.6 & 18.9$\pm$3.7 & 30 \\
\multicolumn{3}{|l|}{Compressed region} & $450-550$ & 2.4$\pm$1.3 & 0.1$\pm$0.1 & 0.4$\pm$0.2 & 0.8$\pm$0.3 & 3.7$\pm$1.4 & 2 \\
\multicolumn{3}{|l|}{Compressed region} &    $>550$ & 3.9$\pm$2.0 & 0.1$\pm$0.1 & 0.2$\pm$0.2 & 0.6$\pm$0.2 & 4.8$\pm$2.0 & 2 \\
\hline
\end{tabular}
\caption{\label{tab:resultsAll} Observed data and the background estimations results with the systematic uncertainties in the 31 signal regions for the integrated luminosity of 35.9~fb$^{-1}$. The uncertainties include both the statistical and systematic part~\cite{Sirunyan:2017xse}. }
\end{table}


    \insertFigure{figures/resultPlot} % Filename = label
                 {0.75}       % Width, in fraction of the whole page width
                 { The graphical presentation of the observed data (black dots), background estimates and expected signal for three different signal models in the 31 signal regions. The lost lepton background is shown in green, the $1 \ell$ background not from top in yellow, $Z \to \nu \bar{\nu}$ background in violet and $t\bar{t} \to 1\ell$ in red. The T2tt model ($\tilde{t}_{1} \to t \tilde{\chi}^{0}_{1}$ ) yields for the stop mass of 900~GeV and LSP mass of 300~GeV are shown in blue dashed line. The T2tb ($\tilde{t}_{1} \to t \tilde{\chi}^{0}_{1} /\tilde{t}_{1} \to b \tilde{\chi}^{\pm}_{1}$) and T2bW ($\tilde{t}_{1} \to b \tilde{\chi}^{\pm}_{1}$) yields for the stop mass of 600~GeV and neutralino mass of 300~GeV are shown in pink and green dashed lines, respectively. The red dashed line separates the signal regions of nominal and optimized corridor analyses~\cite{Sirunyan:2017xse}. }

The limit plots on T2tt in Fig.~\ref{fig:figures/limitsT2tt} and T2tb~Fig.\ref{fig:figures/limitsT2tb} model are shown for the 2015 analysis of the 13~TeV data~\cite{Sirunyan:2016jpr} and the full 2016 analysis discussed by default above~\cite{Sirunyan:2017xse}. The 95\% confidence level~(CL) upper limit on the T2tt model ($ pp \to \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t \bar{t} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}$ ) and T2tb model (($ pp \to \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t b \tilde{\chi}^{0}_{1} \tilde{\chi}^{+}_{1}$ )) assuming unpolarized top quarks originating from stops is shown in left plot of Fig.~\ref{fig:figures/limitsT2tt} and Fig.~\ref{fig:figures/limitsT2tb}, respectively, for data corresponding to the integrated luminosity of 2.3~fb$^{-1}$. The right plots of Fig.~\ref{fig:figures/limitsT2tt} and ~\ref{fig:figures/limitsT2tb} show 95\% confidence level~(CL) upper limit on the T2tt and T2tb model, respectively, for data corresponding to the integrated luminosity of 35.9~fb$^{-1}$.  The analysis of 2015 data~\cite{Sirunyan:2016jpr} combines searches for the stop quark pair production in all-hadronic final states and in the $1\ell$ final state. Therefore in the left plot of Fig.~\ref{fig:figures/limitsT2tt} and also Fig.~\ref{fig:figures/limitsT2tb}, the expected limit from the $0\ell$~(magenta dashed line) and $1\ell$~(blue dashed line) are depicted. In the 2015 analysis, both observed~(black line) and expected~(red dashed line) limits are computed from the combination of signal regions of both analyses, which are mutually exclusive. 

In the case of T2tt model in Fig.~\ref{fig:figures/limitsT2tt}, the  $0\ell$ analysis outperforms the $1\ell$ analysis in the part of the mass plane where the stop is heavy and the LSP light, because of the larger signal acceptance. On the other hand the $1\ell$ analysis is more sensitive in the region of heavier LSP and intermediate stop mass. The combined analysis of data corresponding to an integrated luminosity of 2.3~fb$^{-1}$ excludes the stop masses between 280 and 830~GeV when the LSP is massless and LSP masses up to 260~GeV for a stop mass of 675~GeV. 

The full 2016 analysis~\cite{Sirunyan:2017xse} of data corresponding to an integrated luminosity of 35.9~fb$^{-1}$ combines four signal regions of the corridor analysis in the region 100 $\leq \Delta m \leq $ 225~GeV and the 27 signal regions of the nominal analysis everywhere else. The exclusion limits purely based on the $1\ell$ search exclude  stop masses up to 1120~GeV for a massless LSP and LSP masses up to 515~GeV for a stop mass of 950~GeV. 

In both analyses in case of the T2tt model, the white region where the LSP is light and $\Delta m$ is around the mass of the top quark is not interpreted because of the problematic modeling of quickly changing kinematics in this region. The analyses have small sensitivity when $\Delta m$ is around mass of the top quark. In this region the neutralinos are produced almost at rest, leading to the small \MET and therefore the signal final state is very similar to these of the SM backgrounds. The full 2016 analysis improved sensitivity in this region by requiring a boost of the stop pair against ISR which is enhancing the \MET. Similar situation happens in the region where $\Delta m$ is around mass of W boson, but moreover this region suffers from a  large contribution from $W+jets \to 1\ell$ background. The sensitivity in this region is also improved in the full 2016 analysis by requiring a boost against the ISR. Different signal regions bring different sensitivity in the $\Delta m$ plane. For example, the signal regions with high \MET are sensitive mainly to the signals with high $\Delta m$ and push the limits for high stop and low LSP masses. The signal regions are also differently sensitive to different signal decay modes, the signal regions with low modified topness are more sensitive to the T2tt signals,  while the high modified topness regions target signal with intermediate charginos. In the full 2016 analysis, the observed limit is stronger than the expected one. This is caused by upward fluctuations of the predicted SM backgrounds in the signal regions. In case of downward fluctuations, the expected exclusion limit is stronger than the observed one.

    \insertTwoFigures{figures/limitsT2tt}
                 {figures/limitT2ttOLD} % Filename = label
                 {figures/limitT2tt} % Filename = label
                 {0.49}       % Width, in fraction of the whole page width
                 { The exclusion limits at 95\% CL on the T2tt model corresponding to the integrated luminosity of 2.3~fb$^{-1}$~\cite{Sirunyan:2016jpr} (left) and   35.9~fb$^{-1}$ ~\cite{Sirunyan:2017xse} (right). The interpretation is provided in the context of SMS, in plane of the stop mass vs. the LSP mass. The color code shows the 95\% CL upper limit on cross section assuming a branching ratio of 100\% in $ \tilde{t}_{1} \to t  \tilde{\chi}^{0}_{1} $. The red and black contours represent the expected and observed exclusion limits, respectively. The blue and magenta lines in the left plot show the expected limits of $1 \ell$ and $0 \ell$ analyses, respectively.  The area below the black thick curve indicates the excluded region of the signal points.  }

The exclusion limits of the 2015 analysis \cite{Sirunyan:2016jpr} and full 2106 analysis  \cite{Sirunyan:2017xse} for the T2tb signal, where the branching ratio of $ \tilde{t}_{1} \to t  \tilde{\chi}^{0}_{1} $ and $ \tilde{t}_{1} \to b  \tilde{\chi}^{\pm}_{1} $  is 50\% each and $m_{\tilde{\chi}_{1}^{\pm}} = m_{\tilde{\chi}_{1}^{0}} + 5~\mathrm{GeV}$ , is shown in Fig.~\ref{fig:figures/limitsT2tb}. The combined analysis of data corresponding to an integrated luminosity of 2.3~fb$^{-1}$ excludes the stop masses up to 725~GeV for intermediate LSP masses and the LSP masses up to 210~GeV for a stop mass of around 500~GeV. This exclusion is mainly driven by the fully hadronic analysis due to its larger signal acceptance. The full 2016 analysis~\cite{Sirunyan:2017xse} of data corresponding to integrated luminosity of 35.9~fb$^{-1}$ excludes stop masses up to 980~GeV for a massless LSP and LSP masses up to 400~GeV for a stop mass of 825~GeV. Here again, there is a loss of sensitivity for regions of low $\Delta m$  where the neutralinos are produced at rest and on top of it might suffer by large SM background.

    \insertTwoFigures{figures/limitsT2tb}
                 {figures/limitT2tbOLD} % Filename = label
                 {figures/limitT2tb} % Filename = label
                 {0.49}       % Width, in fraction of the whole page width
                 { The exclusion limits at 95\% CL on the T2tb model corresponding to the integrated luminosity of 2.3~fb$^{-1}$~\cite{Sirunyan:2016jpr} (left) and   35.9~fb$^{-1}$ ~\cite{Sirunyan:2017xse} (right). The interpretation is provided in the context of SMS, in plane of the stop mass vs. the LSP mass. The color code shows the 95\% CL upper limit on cross section assuming a branching ratio of 50\% in $ \tilde{t}_{1} \to t  \tilde{\chi}^{0}_{1} $ and 50\% in $ \tilde{t}_{1} \to b  \tilde{\chi}^{\pm}_{1} $. The red and black contours represent the expected and observed exclusion limits, respectively. The blue and magenta lines in the left plot show the expected limits of $1 \ell$ and $0 \ell$ analyses, respectively.  The area below the black thick curve indicates the excluded region of the signal points.  }

The last considered model is T2bW where $ pp \to \tilde{t}_{1} \bar{\tilde{t}}_{1} \to b \bar{b} \tilde{\chi}^{+}_{1} \tilde{\chi}^{-}_{1} \to b \bar{b} W^{+} W^{-} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}$ in 100\% of the cases and the chargino mass is fixed by the condition $m_{\tilde{\chi}_{1}^{\pm}} = ( m_{\tilde{t}} +  m_{\tilde{\chi}_{1}^{0}} )/2$. The 95\% CL upper limit on this model obtained by the analysis~\cite{Sirunyan:2017xse} is shown in Fig.~\ref{fig:figures/limitT2bW}. In this model stop masses are excluded up to 1000~GeV for a massless LSP and LSP masses up to 450~GeV for a stop mass of 800~GeV.

    \insertFigure{figures/limitT2bW} % Filename = label
                 {0.5}       % Width, in fraction of the whole page width
                 { The exclusion limits at 95\% CL on the T2bW model corresponding to an integrated luminosity of 35.9~fb$^{-1}$~\cite{Sirunyan:2017xse}. The interpretation is provided in the context of the SMS, in the plane of the stop mass vs. LSP mass. The color code shows the 95\% CL upper limit on cross section. The red and black contours represent the expected and observed exclusion limits, respectively. The area below the black thick curve indicates the excluded region of the signal points.  }

\section{Conclusion}

The excluded stop mass in the T2tt model was increased by around of 50~GeV in early Run~2 compared to the strongest exclusion results of Run~1 presented in Fig.~\ref{fig:figures/T2tt2015} despite the lower integrated luminosity. This increase in the sensitivity is caused by the change of the center-of-mass-energy from 8~TeV to 13~TeV as well as by an improvement in the optimization of the analyses. The analysis of data corresponding to the integrated luminosity of 35.9~fb$^{-1}$ outperforms the combined analysis of data corresponding to the integrated luminosity of 2.3~fb$^{-1}$ by almost 300~GeV in the stop mass exclusion of the T2tt model and around 250~GeV in the stop mass exclusion of the T2tb model. The exclusion limit on the stop mass reaches the TeV range which is considered as the naturalness boundary. But the exclusion of the stop masses around 1~TeV happens for the low LSP masses and there are still configurations in the $\Delta m$ plane, which are uncovered and where the stops can have relatively low masses. Moreover, this limit is interpreted in terms of simplified model spectra and when considering more complex models the limits can become considerably weaker. For example the Ref.~\cite{Baer:2012uy} is suggesting that natural SUSY models might lead to final states with very soft visible particle spectra and therefore they are very difficult to be extracted from the SM backgrounds making the search for them at the LHC very challenging.%TODO remarks of Eric


%tables and plots
%Up to 2~TeV stops at HL-LHC
\newpage

\section{Perspectives}

As the exclusion limits on the stop mass are reaching the naturalness bound lot of effort is being recently made to revisit this bound and find natural SUSY scenarios which are not excluded by the experiments. The Ref.~\cite{Baer:2016bwh} shows that the mass of the lighter stop $(\tilde{t}_{1})$ of up to the 3~TeV can lead to the natural SUSY. This reference discusses that if an observable $O$ can be written as

\eq{finetuning}
{
O = a + b + f(b) + c,
}
the $O$ is claimed to be natural in the case that all terms on the right hand side are of comparable values or smaller than $O$. If they would be larger than the observable $O$, some degree of fine-tuning would be needed to cancel the large terms between each other in order to end up with small value of $O$. But in the Eq.~\ref{eq:finetuning} two terms which are $b$ and $f(b)$ are correlated and they can cancel each other without any fine-tuning, e.g. if $f(b) = -b$, the $b+f(b)$ is always zero independently on the value of $b$. Therefore when evaluating the fine-tuning, only independent terms should be compared between each other. The naturalness measure of the observable $O$ can be defined as 

\eq{ftm}
{
\Delta = \frac{\abs{\mathrm{largest~term~on~right~hand~side}}}{\abs{O}}.
}

The two terms on the right hand side of  Eq.~\ref{eq:HiggsMass} giving the Higgs boson mass, which are $m_{H,0}^{2}$ and $\Delta m_{H}^{2}$ are independent and to avoid the fine-tuning, each of them should be around the measured mass of the Higgs boson. Within the MSSM, the SM Higgs boson mass denoted as $m_{h}$ is given by relation

\eq{sustHiggs}
{
m_{h}^{2} \approx -2 \mu^{2} (\Lambda)+ m_{H_{u}}^{2} (\Lambda)+ \delta m_{H_{u}}^{2}(\Lambda),
}
where $\Lambda$ is the energy cut-off, $\mu$ is the Higgs mass parameter, $m_{H_{u}}((\Lambda))$ is the up-Higgs mass and $\delta m_{H_{u}}^{2}((\Lambda))$ is the radiative correction to the up-Higgs mass. In the Eq.~\ref{eq:sustHiggs} terms $m_{H_{u}}^{2} ((\Lambda))$ and  $\delta m_{H_{u}}^{2} ((\Lambda))$ are dependent, larger the $m_{H_{u}}^{2} ((\Lambda))$, larger is the canceling correction  $\delta m_{H_{u}}^{2} ((\Lambda))$. Therefore in evaluating the naturalness as in Eq.~\ref{eq:ftm}, only the combination of the dependent terms should appear in the numerator.

In the context of the MSSM the mass of the Z boson, when assuming $\beta \gtrsim 5 $, can be expressed as 

\eq{Zmass}
{
\frac{m_{Z}^{2}}{2} \simeq -m_{H_{u}}^{2} - \sum_{u}^{u} - \mu^{2},
}
where $\sum_{u}^{u}$ denotes 1-loop corrections and the three terms on the right hand side are mutually independent. The $\Delta_{EW}$ defined as

\eq{deltaEW}
{
\Delta_{EW} = \frac{\abs{\mathrm{largest~term~on~right~hand~side~of~Eq.~\ref{eq:Zmass}}}}{m_{Z}^{2}/2}.
}
gives a measure for fine-tuning of SUSY at the electroweak scale. Requiring the $\Delta_{EW}$<30, i.e. 3\% fine-tuning or better, the upper bound on the gluino mass is up to 4~TeV and the lighter stop mass up to 3~TeV. Two  lightest neutralinos and the lightest chargino are higgsino-like and their mass can go up to 300~GeV. These two neutralinos and the chargino are expected to be almost mass degenerate. The masses of the other charginos and neutralinos are expected to be at the TeV scale.

To compute the spectrum of the possible lighter stop vs. LSP masses, the two-extra-parameter non-universal Higgs model~(NUH2M)~\cite{Matalliotakis:1994ft, Nath:1997qm, Ellis:2002iu, Baer:2005bu} was used. The theoretical parameters in this model were varied within given boundary: $\Delta_{EW} <30$, the lightest chargino is heavier than 103.5~GeV, the masses of gluino and all other squarks,  except of $\tilde{t}_{1}$, are higher than excluded masses by the LHC during the Run~1, and the Higgs boson mass is of $125 \pm 2$~GeV. The possible configurations of the $\tilde{t}_{1}$ and LSP masses obtained by the described model are shown in Fig.~\ref{fig:figures/mz1mt1delew}. In this figure the possible scenarios with $\Delta_{EW} <15$ and $\Delta_{EW} <30$ are shown in red and blue crosses, respectively, together with the existing exclusion limits existing in 2016 which were derived by the CMS~\cite{CMS:2016hxa} (red line) and the ATLAS~\cite{ATLAS:2016jaa, ATLAS:2016ljb, ATLAS:2016xcm, Aaboud:2016tnv} (black line) collaborations. The predicted exclusion limit and the discovery potential to be reached by the ATLAS collaboration in the fully hadronic search assuming the integrated luminosity of $3000$fb$^{-1}$, which is expected at the HL-LHC, is shown in an orange curve. The black dotted line corresponds to the situation when the lighter stop and LSP have the same masses. In this figure it can be seen, that many signal scenarios were already excluded, but as well many scenarios will remain uncovered even analyzing the HL-LHC data. The predicted reach of the HL-LHC is up to around 1.4~TeV in the stop masses, however the natural SUSY scenarios predict stop masses up to 3~TeV. Also in can be noticed that this model does not predict many scenarios in the compressed region.  

    \insertFigure{figures/mz1mt1delew} % Filename = label
                 {0.7}       % Width, in fraction of the whole page width
                 { The plane of the $\tilde{t}_{1}$ vs LSP masses. The natural SUSY scenarios with $\Delta_{EW} <15$ and $\Delta_{EW} <30$ are shown in the red and blue crosses, respectively. In red, black and orange curves, the existing and predicted exclusion limits are depicted. The green curves shows discovery potential for the HL-LHC. The dotted line is drawn for the situation when the stop and the LSP have same masses~\cite{Baer:2016bwh}.   }

The Ref.~\cite{Baer:2016bwh} also examines prospects for the stop discovery and presents the exclusion potential for current and future colliders as depicted in Fig.~\ref{fig:figures/collidersstop}. The figure reveals, that as shown previously, the stop mass of 3~TeV will not be probed at current center-of-mass energies of 13~TeV. To probe such high stop masses, a machine delivering collisions at center-of-mass energy of 33~TeV would be needed. 

    \insertFigure{figures/collidersstop} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { Potential for discovery (5$\sigma$) and exclusion (95\%) of the current LHC with center-of-mass energy of 13~TeV, HL-LHC with the center-of-mass energy of 13~TeV, HE-LHC with the center-of-mass energy of 33~TeV,  and a pp collider with the center-of-mass energy of 100~TeV (FCC) of the stop $\tilde{t}_{1}$ masses~\cite{Baer:2016bwh}.  }


One may also examine what are the expectations on the SUSY mass spectrum from the other measurements than the direct searches for the SUSY particles. One of the options is to study the branching ratio of the b-quark to the s-quark and the photon BR($b \to s\gamma$).  This decay is possible in the SM via loops with W boson and top quark, within the SUSY other loops are possible as well. These are mainly loops with the  stop and the chargino, and loops with the b-quark and the charged Higgs boson. The measured BR$(b \to s\gamma) = (3.01 \pm 0.22)$~\cite{Belle:2016ufb} can be compared with the prediction of the BR for each scenario in the $\tilde{t}_{1}$ vs LSP plane. The compatibility of the measured and predicted branching ratios excludes stop masses lower than 500~GeV. In the majority of scenarios with the stop mass higher than 1.5~TeV, the predicted and measured BR are in agreement within $2\sigma$~\cite{Baer:2016bwh}.

One can also obtain constraints on the masses of SUSY particles from the observed average cold dark matter density $\rho_{DM} \approx 1.2 \times 10^{-6} \mathrm{GeV/cm^{3}}$~\cite{Ade:2015xua, Patrignani:2016xqp}.  In the early universe there was a thermal equilibrium between the SM and SUSY particles. As the universe cooled down, heavier sparticles could not be produced anymore, they only annihilated into the SM particles or decayed to the LSP which is in this text considered to be a neutralino. Eventually LSPs also annihilated into the SM particles. With continuing expansion of the universe, the annihilation rate became small and the LSPs experienced a ``freeze-out''. If the LSP is mostly higgsino or wino, in order to reproduce the observed dark matter density, the LSP needs to be very heavy, of order of 1~TeV and more. This model is not yet excluded by the LHC, but on the other hand it is not natural. Another option is a bino-like LSP, but the mass of such LSP is in the majority of parameter space, given the constraint on the cold dark matter density,  already excluded by the LHC. The non-excluded bino-like LSP lead to too large cold dark matter density, which can be reduced for example by assuming that one of the sleptons is not too heavy.  But in this case the slepton is too light and large part of such light slepton models is already excluded at the LHC. The cold dark matter density for the bino-like LSP scenario can be also reduced in the compressed scenario, when the LSP mass is around at least mass of the top quark or is maximum 100~GeV heavier. The $\tilde{t}_{1}$ mass is required to be around 150~GeV larger then the mass of the LSP. In order to make this  model natural, the gluino and wino mass parameters ratio must be smaller than the prediction giving the unification of interactions~\cite{Martin:2008aw}. Although as described, many possible scenarios are already ruled out by the requirements on the cold dark matter density, there are still parts of the parameter space which are uncovered. Moreover, the described considerations assume only simple thermal mechanism for production of the LSP. If, for example, there is other production mechanisms or the R-parity is broken, the conclusions do not have to hold.

There is also a possibility that the MSSM is not the realization of the SUSY chosen by nature and we should examine the theories beyond the MSSM. For example in the MSSM the R-parity is conserved, which protects the proton to decay. But there is no fundamental reason that R-parity should be conserved and can be replaced by another symmetry, which also protects proton to decay. For example, such symmetry conserves baryon number but not the lepton one. In the case that the R-parity is violated, the LSP can decay and therefore the signal production and final states differ from the ones assuming the R-parity conservation. Both pair and single SUSY particle production is expected. Depending on the R-parity violation scenario, the LSP can decay to leptons or/and jets or not decay in the detector, resulting in the final states with large number of charged leptons, jets or \MET, or combination of these.

As discussed in the previous paragraphs, there are many models of SUSY which are already excluded. But on the other hand there are still many possibilities where to search for the SUSY. In was shown, that the lighter stop can have mass up to 3~TeV and be beyond the reach of the LHC. Or it might be needed to go beyond MSSM and focus our searches on more complicated models.

