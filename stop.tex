\chapter{Search for top squark pair production in pp collisions at sqrt(s)=13~TeV using single lepton events}

%\section{Introduction and motivation}

The supersymmetry was found to be the most popular extention of the Standard Model, due to its capabality to adress many shortcomings of the SM. Among others it provides a natural solution to the hierarchy probelm and provieds dark matter candidate. Supersymmetry introduces a supersymmetric partner to each SM particle, which have the same quantum numbers except of spin differing by 1/2. This chapter is focused on the production of the suppersymetric partners of the stop quarks, reffered as ``stops''~($\tilde{t}$). There are two scalar stops $\tilde{t}_{R}$ and  $\tilde{t}_{L}$ as there is left and right component of the SM fermion field. These two stops mix into mass eigenstates $\tilde{t}_{1}$ and $\tilde{t}_{2}$,  $\tilde{t}_{1}$ being the lighter one. Due to the naturalnes constraint the ligter stop should have mass in TeV range. Morover the Higss mass measurements give condition that $\sqrt{m_{\tilde{t}_{1}} m_{\tilde{t}_{2}}}$ should be around 600~GeV. In this chapter a SUSY model, in which the R-parity is conserved and the LSP is the lightest neutralino~($\tilde{\chi}^{0}_{1}$), is considered. Furthermore only direct stop pair production is assumed.

Depending on the mass difference between stop and neutralino, $\Delta m =  \tilde{\chi}^{0}_{1}$, several decay modes of the stop quark are possible. The stops can decay via two, three or four body decays to final states with b or c quarks. This chapter discribes search for top squark pair production in pp collisions at sqrt(s)=13~TeV using single lepton events~\cite{Sirunyan:2017xse}. As I was also involved in previous versions of this analysis~\cite{Sirunyan:2016jpr, CMS:2016vew}, several differences between the three analyses are briefly discussed, but in general they are largely similar. The analyses were performed in the working group of around twenty to fourty people and therefore in following sections I will especially highlight some chosen contributions from my side.

%-delta M around top mass challenging kinematics -> looks like SM tt -> this region is called stealthy region
%-for t2bW when W becomes on-shell also difficult kinematics

\section{Signal topologies}

The preseneted analysis focuses on the kinematic region where $\Delta m > m_W+m_b$ and on three different decay modes of the stop pair.  All modes lead to states with two b-jets, two W-bosons and two neutralinos, but the kinematics differ depending on the decay mode and $\Delta m$. The $\Delta m$ plane is shown in Fig.~\ref{fig:figures/dmplane}. In the part of the plane where $\Delta m < m_t$, often reffered as ``compressed spectra'' region, the decay producs are soft and often not reconstructed. On the other hand when the $\Delta m$ is large, boosted topologies can be expected. 

    \insertFigure{figures/dmplane} % Filename = label
                 {0.99}       % Width, in fraction of the whole page width
                 { dm plane ~\cite{Aad:2014kra}. }

In this analysis thei targeted final states are with one leptonically and one haronically decaying W-boson, resulting in one charged lepton in final state. The advange of this one lepton channel is its relatively high branching ratio, around 32\% of directly produced stop pairs decay to final states with one lepton, and low occurance of the Standard model backgrounds. 

The first of the three considered decay chains of the stop pair shown in the left part of Fig.~\ref{figures/stopdecays} is reffered as ``T2tt'', where both stops decay to top quark and neutralino, followed by decay of each top quark to W-boson and b-quark:

\eq{t2tt}
{
    \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t \bar{t} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1} \to b \bar{b} W^{+} W^{-} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}.
}

The second possibility depicted in the right part of Fig.~\ref{figures/stopdecays} is reffered as ``T2bW'' and in this case both stop quarks decay via intermediary chargino:

\eq{t2bW}
{
    \tilde{t}_{1} \bar{\tilde{t}}_{1} \to b \bar{b} \tilde{\chi}^{+}_{1} \tilde{\chi}^{-}_{1} \to b \bar{b} W^{+} W^{-} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}.
}

    \insertTwoFigures{figures/stopdecays}
                 {figures/T2tt} % Filename = label
                 {figures/T6bbWW} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{website:SUSYdiagrams}. }

In the T2bW the chargino mass is fixed to halfway between the mass of stop and neutralino. The third decay shown in Fig.~\ref{figures/T4tbW} combines the previous two with 50\% branching ratio of $\tilde{t} \to t \tilde{\chi}_{1}^{0})$. In this case, reffered as ``T2tb'' one of the stops decay to top quark and neutralino and the second to bottom quark and chargino.

\eq{t2tb}
{
    \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t b \tilde{\chi}^{0}_{1} \tilde{\chi}^{+}_{1} \to b \bar{b} W^{+} W^{-} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}.
}

    \insertFigure{figures/T4tbW} % Filename = label
                 {0.5}       % Width, in fraction of the whole page width
                 { mixed decay diagram ~\cite{website:SUSYdiagrams}. }

In T2tb the chargino and neutralino are almost mass degenerate, the chargino mass is fixed by relation $m_{\tilde{\chi}_{1}^{\pm}} = m_{\tilde{\chi}_{1}^{0}} + 5~\mathrm{GeV}$. Because of this small difference between chargino and neutralino masses, the W-boson is produced ofshell and its decay producst have low transverse momenta and are probably not reconstructed. If the jets from hadronically decaying W-boson are not reconstructed, only the two b-jets are present in this signal topology, what is being one of the motivation to search for final states with less jets than the four expected.

The analysis strategy introduced in the next section is general for all types of signal topologies. Only for corridior region of T2tt the separately optimized ``corridor'' analysis was designed to target better very challenging signal final state, which is very similar to standard model processes. To be able to combine results of the nominal and corridor analysis, the sensitivity of both analysis for different $\Delta m$ was evaluated and it was decided to use compressed analysis for T2tt signals in $\Delta m$ range between 100 and 225~GeV and the nominal one everywhere else.


\section{Analysis strategy}

Because of the two neutralinos, the signal processes (T2tt, T2bW and T2tb)  can have final state signature largery differing from the background processes coming from Standard Model. This knowledge of the signal and background signature is used to define the baseline search region, in which the SM background is supressed. This baseline search region is then divided to smaller signal regions with help of discriminating variables. These variables are useful to define signal regions where some of the signals are enriched or some fo the backgrounds are more reduced. The remaining backround is estimated from data-driven techniques or simulations for each signal region. In teh baseline search region there are three groups of backgrounds present. 

The leading background is ``Lost lepton'' background mainly coming mainly from $\mathrm{t\bar{t}} \to 2 \ell$ processes, where both W-bosons decay into leptons and one of the charged leptons is lost because of acceptance or other critearia. This background is reduced by applying veto on second lepton, which was misreconstructed, but even after this it remains the largest backround. The subleding background is ``One lepton'' background, which mainly composed of W+jets and $\mathrm{t\bar{t}} \to 1 \ell$ processes in which the W-boson decays leptonically. The last relevant background is denoted ``$Z \to \nu \bar{\nu}$'' and comes from processes such as $t\bar{t}Z$ and $WZ$ in which the Z-boson decays to two neutrinos and one of the W-bosons leptonically.  

The signal and estimated background yields are compared with data in the signal regions and in case that no excess from the SM is observed, the exclusion limits can be put on given signal models. For the purposes of the optimization of analysis and limits setting, the procedure of the hypothesis testing in the LHC searches is briefly described in the section~\ref{sec:stats}. 

In the following subsection~\ref{sec:variables}, the variables designed for definition of baseline search region and its division to signal regions will be introduced and justified on the exaples of signal topologies.  The backgrounds behaviour with regard to the choice of the variables is as aslo discussed. In the end the final definition of the baseline and signal regions for the presented analysis is revealed.

Once the most discriminative variables are identified the object selection, triggers and final design of baseline selection and signal regions are in troduced in subsections~\ref{sec:objects},~\ref{sec:trigger},~\ref{sec:baseline} and ~\ref{sec:sr}. Then in subsection~\ref{sec:estimations} the background estimations in the signal region are discussed, starting from the $Z \to \nu \bar{\nu}$ which was provided by myself. After estimating all relevant backgrounds in subsection~\ref{sec:systematics} the systematic uncertainties on the background estimates as well as on signal yields are identified. Finally the section~\ref{sec:results} summarizes the results of this analysis and provides interpreatation of them. 

%binned approach in variables which tend to reduce signal
%single top?


\subsection{Statistical methods in LHC searches~\label{sec:stats} }

The hypothesis testing and limit setting procedure chosen by the ATLAS and CMS collaborations is modified frequentis method reffered as ``$\mathrm{CL_{s}}$'' method~\cite{Read:2002hq, Junk:1999kv, Cowan:2010js, CMS-NOTE-2011-005}. There are two hypothesis to be tested, the background only hypothesis $H_{0}$ also reffered as ``null hypothesis'' and the signal plus background hypothesis $H_{1}$ usually called ``alernative hypothesis''. These hypothesis are functions of $b+\mu s$, where $b$ and $s$ are the expected background and signal yields and $\mu$ is the signal strength modifier. In case of background only hypothesis $H_{0}$ the signal strenght $\mu$ is zero. The signal and bacground estimates enetering into the hypothesis testing are burdened by meany systematic uncertainties, which are in the $\mathrm{CL_{s}}$ methods therated as nuisance parameters and denoted colectively as $\theta$. Then the expected signal and backgroun yields are depending on these nuisance parameters: $s \to s(\theta)$ and $b \to b(\theta)$.

To compute an \textbf{observed limits} a likelihood function $\mathcal{L}(data|\mu, \theta)$ can be built followingly

\eq{likelihood}
{
\mathcal{L}(data|\mu, \theta) = Poisson(data| \mu s(\theta) +b(\theta)) p(\tilde{\theta}|\theta),
}

where $p(\tilde{\theta}\theta)$ are the pdfs for a nuisicance parameters $\theta$ with $\tilde{\theta}$ being the default value of the nuisance parameter and data for this purpose reffer to the information about observed data and expected signal and background yields. In case of binned Likelihood the term $Poisson(data| \mu s(\theta) +b(\theta))$ can be expressed as


\eq{binnedlikelihood}
{
Poisson(data| \mu s(\theta) +b(\theta)) p(\tilde{\theta}|\theta) = \prod_{i} \frac{\mu s_{i}+b_{i}}{n_{i}!} e^{-\mu s_{i}-b_{i}},
}

which is a product  of Poisson probabilities to observe $n_{i}$ events in bin $i$. Having the Likelihood function, the compatibility of data with $H_{0}$ or $H_{1}$ hypothesis can be tested with help of test statistics $\tilde{q}_{\mu}$ which is in form of the profile likelihood ratio

\eq{pLR}
{
\tilde{q}_{\mu} = -2 \mathrm{ln } \frac{ \mathcal{L}(data| \mu , \hat{\theta}_{\mu}) } {\mathcal{L}(data| \hat{\mu} , \hat{\theta})}, 0 \leq \hat{\mu} \leq \mu ,
}

where $\mu$ is a free parameter, $\hat{\theta}_{\mu}$ are conditional maximum likelihood estimators of parameters $\theta$ given the parameter $\mu$ and measured data. The $\hat{\mu}$ and $\hat{\theta}$ are the parameter estimators at the global mximum of likelihood. The condition $\hat{\mu} \geq 0 $ express that the rate of signal cannot be negative and $\hat{\mu} \leq \mu $ is constraint to consider only one-sided confidence interval 
	  
Once having the masured data, the observed value of profile likelihood ratio~\ref{pLR} $\tilde{q}_{\mu}^{obs}$ can be computed and the maximum lakelihood estimators of $\theta$ best describing the observed data can be evaluated for both $H_{0}$ and $H_{1}$ hypotheses. The $\mathrm{CL_{s}}$  method defines two p-values, the first one is $p_{\mu}$ testing the compatibility of observation with $H_{1}$ and the second testing the compatibility of observation with background only hypothesis~$H_{0}$. The $CL_{s}$ for given signal strenght $\mu$ is then defined as

\eq{cls}
{
CL_{s}(\mu) = \frac{p_{\mu}}{1-p_{b}},
}

where $p_{\mu}$ can be writes as

\eq{pmu}
{
P(\tilde{q}_{\mu} \geq \tilde{q}_{\mu}^{obs}|s+b)
}

and $1-p_{b}$ as

\eq{pb}
{
P(\tilde{q}_{\mu} \geq \tilde{q}_{\mu}^{obs}|b).
}

When $\mu$ being set to one, in case that $\mathrm{CL_{s}} \leq \alpha$ where for exclusion limits $\alpha = 0.05$ the $H_{1}$ is excluded at $1-\alpha$ (=95\%) $\mathrm{CL_{s}}$ confidence level. In the case ofthe  SMS the $\mu$ is not fixed to one and therefore the exclusion limits are set on the cross section given a fixed branching ratio of given SMS model. In such procedure the signal strength $\mu$ starts at zero and it is increased up to the point where $\mathrm{CL_{s}}$ = 0.05. If for a given signal point the cross section is limited to a lower value than the theoertical cross section, the signal point is excluded. The contour line is drown at the place where limit on cross section is equal to the theoretical cross section.

The \textbf{expected limits} can be determined by similar method using generated background only pseudo-data instead of observed data. 

Assuming that the test statistics has Gaussian tail  the p-value can be converted to the significance $Z$ with help of convention

\eq{significance}
{
 p = \int_Z^{\infty} \frac{1}{\sqrt{2\pi}} e^{-x^{2}/2} dx,
}

where if considering backgroun only hypothesis, the significance of 5 standard deviations corresponds to $p_{b} = 2.8 \times 10^{-7} $ suggest that the $H_{0}$ hypothesis is excluded at $5 \sigma$ in favour of $H_{1}$ hypothesis. %TODO check that

\subsection{Variables~\label{sec:variables}}

\subsubsection{Number of leptons~($N_{\ell}$)}

The presented search focuses on final states with one charged lepton and therefore requirement on number of leptons to be equal to one must be imposed. The lepton is either electron or muon and the events with tau leptons are rejected.

\subsubsection{Number of jets~($N_{J}$)}

As shown on example in blue on the left of Fig.~\ref{fig:figures/T2ttlep1}, four jets are expected in the final state. But as already discussed, because of the low mass diffference between chargino and neutralino in case of T2tb, the two jets from hadronically decaying W-boson have low \pt and thus they do not have to be reconstructed and only two jets are expected. In case of high $\Delta m$ the decay products of stops can be boosted and thus two or more jets can be merged into one, resulting to reduced number of reconstructed jets in final state. The baseline requirement is to have at least two jets, but this variable is also used for the definition of signal regions targeting different topologies.

%TODO jets PT, eta, jet wp

    \insertTwoFigures{figures/T2ttlep1}
                 {figures/T2ttlepJET} % Filename = label
                 {figures/T2ttlepMET} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{CMS:2016vew}. }

\subsubsection{Number of b-jets~($N_{b}$)}

In the left diagram of Fig.~\ref{fig:figures/T2ttlep1} it can be also noticed that two of the jets in green are originating from the b-quark. For this reason number of b-tagged jets selected by medium working point of CSVv2 are one of the disriminating variables. In the signal regions where W+jets background is dominant, the tighti b-tagging working point is used to further suppress it, as teh jets are expected to be mainly composed of light flavours.

\subsubsection{Missing transverse energy~(MET or $E_{T}^{miss}$)}

Right diagram of Fig.~\ref{figures/T2ttlep1} shows that in the final staes of the signal processes there are two neutralinos and neutrino originating from the leptonicallyd ecaying W-boson. These particles escape detector and cause missing energy in transverse plane, reffered as ``Missing transverse energy''. This variable is very powerful in rejection of SM backgrounds bacouse they tend to have small values of \MET as the sources of the \MET are more limited than in case of signal. Therefore the basline selection requires \MET to be larger than 250~GeV. The distribution of the \MET in the baseline search region for all relevant backrounds and selected signal points is shown in the left plot of Fig.~\ref{fig:figures/METMT}. In this Figure the one lepton background is decomposed to ``1l from top'' composed of $t \bar{t} \to 1\ell$ and ``1l not from top'' populated by W+jets processes. The \MET was also found to be good variable for definiton of signal regions.

Because of the two neutrinos and misreconstrcuted lepton in lost lepton background, the \MET of this background can be very large, which also applies for $Z \to \nu \bar{\nu}$ background, where the source of \MET are three neutrinos.

\subsubsection{Transverse mass of lepton-\MET system~($M_{T}$)}

The \MET in combination with the $M_{T}$ defined as

\eq{MT}
{
 M_{T} = \sqrt{2 p_{T}^{\ell} E_{T}^{miss} (1 - \mathrm{cos}(\phi)) } ,
}

where $p_{T}^{ell}$ is the transverse momentum of lepton and $\phi$ is the angle between momentum of lepton and \MET, ensure dramatic reduction of the SM backround. The $M_{T}$ is  designed to suppress backgrounds where both lepton and \MET (neutrino) come from one W-boson. In such case the $M_{T}$ has an endpoint at W-boson mass. As shown in the left diagram of Fig.~\ref{fig:figures/T2ttlep2} for the signal the leptonically decaying W-boson is not the only source of \MET and therefore it has no endpoint.

    \insertTwoFigures{figures/T2ttlep2}
                 {figures/T2ttlepMT} % Filename = label
                 {figures/T2ttlepDPHI} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{CMS:2016vew}. }

The baseline selection on  $M_{T}$ variable was chosen to be  larger than 150~GeV, which supresses hugely mainly one lepton bacgrounds. The leton and \MET in both  W+jets and $t \bar{t} \to 1\ell$ originate from one W-boson and thus their $M_{T}$ should have endpoint at W-boson mass~($\sim$80~GeV). It can be seen in the right plot of Fig.~\ref{fig:figures/METMT}, that indeeed the bulk of the one lepton events have low $M_{T}$, but there are tails with high $M_{T}$. In case of $t \bar{t} \to 1\ell$ the top quark constraints the kinematics of W-boson and therefore the tail in $M_{T}$  is mainly caused by \MET resolution. The situation is different for W+jets, where the kinematics permits offshel production of W-boson and consequesntly $M_{W^{*}}> 80$~GeV. For the lost lepton and $Z \to \nu \bar{\nu}$ backgrounds there is no endpoint as the \MET does not originate from one W-boson.

    \insertTwoFigures{figures/METMT}
                 {figures/LogMET2j} % Filename = label
                 {figures/LogMT2j} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{website:stopSupp}. }

\subsubsection{Minimal azimuthal angle between direction and  one of the two leading jets and \MET~(min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ )}

In the backgrounds where the neutrino is only source of the \MET it is probable that the neutrino is close to the b-quark  originating from the same top decay. This mainly appears for $t\bar{t} \to 1\ell$ process as shown in the left plot of Fig.~\ref{fig:figures/DPHIMLB}. In case of the signal, as depicted in the right diagram of Fig.~\ref{fig:figures/T2ttlep2}, there are more sources of \MET and therefore there is no constraint on this variable. The chosen baseline requirement on min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ to be larger than 0.8 is considerably reducing the SM backgrounds but leads to the relatively small reduction of the signal.

    \insertTwoFigures{figures/DPHIMLB}
                 {figures/LogMDPhi2j} % Filename = label
                 {figures/LogMlb2j} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{website:stopSupp}. }

\subsubsection{Invariant mass of the reconstructed lepton and the closest b-quark~($M_{\ell b}$)}

In case that lepton and b-jet come from one top as displayed in Fig.~\ref{fig:figures/T2ttlepMlb} , there is a bound on the $M_{\ell b}$ variable wich is

\eq{Mlb}
{
 M_{t} \sqrt{1 - \frac{M_{W}^{2}}{M_{t}^{2}} } \approx 153~\mathrm{GeV} ,
}

where $M_{W}$ is the mass of the top quark and $M_{W}$ the W-boson mass. This limit is true for backgrounds coming from $t\bar{t}$ decay, but also for T2tt signal. On the other hand there is no limit for W+jets backgrounds and T2bW signal. Therefore this variable is not suitable for baseline region definition, as puting a constraint on it would considerably reduce the signal but it can be used to define signal regions with enriched one kind of signal and supressed one type of the background. The distribution of  $M_{\ell b}$ for all relevant signals and different backgrounds in the baseline search region is shown in Fig.~\ref{fig:figures/DPHIMLB}, where it can be clearly noticed that low $M_{\ell b}$ part of distribution si more lost lepton and T2tt-like, while high $M_{\ell b}$ is mainly populated by W+jets and T2bW-like signal.

    \insertFigure{figures/T2ttlepMlb}
                 {0.5}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{CMS:2016vew}. }

\subsubsection{Modified topness~($t_{mod}$)}

The topness is $\chi^{2}$-like type of variable developed to supress background coming from $t \bar{t} \to 2\ell$ processes, where one lepton in misreconstructed. It aim is to discriminate how well an even agrees with $t \bar{t} \to 2\ell$ hypothesis. It was discovered that removing several terms from topness variable, improves the signal disrimination in this analysis. The new variable called ``modified topness''~($t_{mod}$) is defined as


\eq{tmod}
{
 t_{mod} = \mathrm{ln(\mathrm{min}S)},~where~S(\vec{p}_{W}, p_{\nu, z} ) = \frac{(m_{W}^{2}- (p_{\nu}+p_{\ell})^2 )^2 }{a_{W}^{4}} + \frac{(m_{t}^{2}- (p_{b_{2}}+p_{W})^2 )^2 }{a_{t}^{4}},
}

with $m_{W}$ and  $m_{t}$ is the mass of W-boson and top quark, ${p}_{W}$, ${p}_{\nu}$, ${p}_{\ell}$, ${p}_{b_{2}}$ are momenta of W-boson, neutrino, lepton and b-jet. The parameters $a_{W} =5$~GeV and $a_{t}=15$~GeV are the resolution parameters. The first term in Eq.~\ref{eq:tmod} aims to reconstruct mass of the W-boson which lepton was reconstructed and the purpose of second term is reconstruct the top mass of the leg, where lepton was lost. There are more options how to chose  b-jet. Several options were studied for case of this analysis following procedure was chosen. The modified topness is computed for three jets with highest CSVv2 discriminator. Then the jet which lead to the smallest modified topness is chosen.

Different backgrounds and signal models with different $\Delta m$ populates $t_{mod}$ distribution shown in Fig.~\ref{fig:figures/Logtmod2j} diferently, therefore this variable is valuable for definition of signal regiions.

    \insertFigure{figures/Logtmod2j}
                 {0.5}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{website:stopSupp}. }

\subsubsection{$M_{T2}^{W}$}

The purpose of $M_{T2}^{W}$ is similar as of modified topness, to reduce the lost lepton background. This variably simillarly tries to reconstruct event under $t \bar{t} \to \ell \ell$ with one lost lepton hypothesis. The $M_{T2}^{W}$ has an endpoint at top mass for $t\bar{t}$ events with lost lepton. It would be possible to use this variable for baseline selection and requiring high values of $M_{T2}^{W}$, but it was found out that signals with low $\Delta m$ also lead to small $M_{T2}^{W}$. In the past versions of analysis the $M_{T2}^{W}$ was used for definition of signal regions. The $M_{T2}^{W}$ has very similar behavior as $t_{mod}$ and with more statistics collected it was found out that $t_{mod}$ is more discrimininat in full analysis phase-space than $M_{T2}^{W}$ and therefore in current analysis $M_{T2}^{W}$ is not used anymore.

\subsubsection{Number of W-tags~($N_{J}$)}

In case of high  $\Delta m$ signals the top quarks are expected to be significantly boosted. As in the SM the top quark is the heaviest particle, similar source of boost of top quarks is not present within SM and therefore tagging of boosted objects could help the signal discrimination. As show in Fig.~\ref{fig:figures/boostedTopologies} when the momentum of hadronic top is low, three resolved ak4 jets are observed. With growing boost of the top guark, the jets originating from W-boson or all three jets of the top quark can be merged into one fat jet. The merged jets can be taged by special W- or top-tagging techniques.

    \insertFigure{figures/boostedTopologies}
                 {0.99}       % Width, in fraction of the whole page width
                 { W, top tag }


Part of my contribution to the presented stop searches was study of W-tagging techniques and evaluation of its benefits for the stop search. The study was performed based on results of single lepton stop analysis~\cite{Sirunyan:2016jpr} which is using data collected at center-of-mass energy of 13~TeV in 2015 which correspond to integrated luminosity of 2.3~fb$^-1$. This analysis defines four kinds of signal regions targeting signals with different kinematics. In case of this study we focused only on region groups with high $\Delta m $ reffered as ``High $\Delta m$'' and ``Boosted High $\Delta m$'' defined in Table~\ref{tab:SRnoW}. This study redifines the signal regions with use of number of W-tagged jets~($N_{W}$) instead of \MET, but keeps the number of the signal regions the same. The proposed signal regions are shown in Table~\ref{tab:SRW}.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Name            & $N_{J}$  & $M_{T2}^{W}$~[GeV] & $E_{T}^{miss}$~[GeV]  \\
\hline
\hline
                & $\geq$4  & >200                & 250<$E_{T}^{miss}$<350   \\
High $\Delta m$ & $\geq$4  & >200                & 350<$E_{T}^{miss}$<450   \\
                & $\geq$4  & >200                & $E_{T}^{miss}$>450   \\
\hline
Boosted High $\Delta m$ & 3  & >200                & 250<$E_{T}^{miss}$<350   \\
                        & 3  & >200                & $E_{T}^{miss}$>350   \\
\hline
\end{tabular}
\caption[Table caption text]{2015 default sr definitions. }
\label{tab:SRnoW}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Name            & $N_{J}$  & $M_{T2}^{W}$~[GeV]            & $E_{T}^{miss}$~[GeV]    & $N_{W}$ \\
\hline
\hline
                          & $\geq$4  & >200                & 250<$E_{T}^{miss}$<350   & --   \\
High $\Delta m$ W-tagging & $\geq$4  & >200                & $E_{T}^{miss}$>350       & 0    \\
                          & $\geq$4  & >200                & $E_{T}^{miss}$>350       & 1    \\
\hline
Boosted High $\Delta m$ W-tagging & 3  & >200              & $E_{T}^{miss}$>250 & 0  \\
                                  & 3  & >200              & $E_{T}^{miss}$>250 & 1  \\
\hline
\end{tabular}
\caption[Table caption text]{2015 default sr definitions. }
\label{tab:SRW}
\end{center}
\end{table}


The standard jets used in analysis are clusterized by ak4 algorithm with cone size of 0.4. The boosted W-jets are expected to be ``fat'' and therefore they need to be reclustered with different cone size. In the study two possibilities were exploited, ak8 jets with cone size of 0.8 and ak10 jets with cone size of 1.0. The merged W-jet should have large transverse momentum and mass around the mass of W-boson. To discriminate if the jet is merged jet originating from a W-boson, a requirement on its mass can be posed. The raw mass  of the jet is contaminated by the initial state radiation, underlying events and pile-up and tehrefore it must be cleand from these contributions. Several ``grooming techniques'' of the jet mass were developed by CMS, but in this study only pruned mass is used, as it was recomended technique for the merged W-jets. The pruning procedure~\cite{Ellis:2009su} rejects large angle and soft constituents during reclustering iterations. Another variable used in W-tagging is called N-subjettiness~($\tau_{N}$) which aim is to disriminate how the jet is consistent with N-jet hypothesis. The N-subjettiness ratio $\tau_{21} = \tau_{2}/\tau{1}$ has high disriminating power for merged W-jets and therefore in this study the requirement on jet $\tau_{21}$ is used to tag merged W-jets. Taking into acount the discriminating variables, two definitions of W-tagged jets reffered as ``ak8-W'' and ``ak10-W'' defined in Table~\ref{tab:Wtags} are established.



\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Name            & Clustering algorithm &      pruned mass $m_{W,p}$ [GeV]  &        $\tau_{21}$  & \pt [GeV]  \\
\hline
\hline
ak8-W  &        ak8                     &   60<$m_{W,p}$<100               & <0.5   & >200  \\
\hline
ak10-W  &        ak10                     &   60<$m_{W,p}$<100               & <0.5   & >200  \\
\hline
\end{tabular}
\caption[Table caption text]{ W-tag definition. }
\label{tab:Wtags}
\end{center}
\end{table}

To evaluate the benefits of the W-tagging  the expected significance in the default signal regions in Table.~\ref{tab:SRnoW} was compared to expected significance in proposedi W-tagging signal regions in Table.~\ref{tab:SRnoW}. The significance described in section~\ref{sec:stats} is expected significance when using generated pseudatata with signal strengt of one instead of observed data. For this comparison all relevant backgrounds and three different T2tt signal points with different stop and neutralino mass $(m_{\tilde{t}_{1}}, m_{\tilde{\chi}^{0}_{1}})$ were used. Thsese signal points are (900,1), (800,300) and (650,450). In this study the significances for two different luminosity options of 2.3~fb$^{-1}$ and 10~fb$^{-1}$, two different options of W-tagging defined in Table~\ref{tab:Wtags} and different combinations of default and W-tagged signal regions were computed. The best performing combination of signal regions was found to be combination of ``High $\Delta m$ W-tagging'' with ``High $\Delta m$'' regions as shown in Table~\ref{tab:taggingResults}. In this table the significances of the best performing combination of signal regions for both ak8-W and ak10-W is shown together with significances of default signal regions for three signal points and integrated luminosity of 2.3~$fb^{-1}$. It can be noticed, that using ak10 jets leads to larger significance than ak8 jets. Although there is an increase in the significance up to around 20~\% in some cases, the increase is not that large to support usage of complicated and time consuming W-taggers which have gain only for high $\Delta m$ signals. Also in this study no systematic uncertainties were evaluated. But when studying the significances for different luminosities it was found that with growing luminosity the gain in significance when using W-tagging is inreasing and therefore the W-tagging is interesting option for analyses with larger integrated luminosity. 

%Table results
\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Signal point             & --      & ak8-W & ak10-W \\
\hline
                       & High $\Delta m$ +           &  High $\Delta m$ W-tagging +  &   High $\Delta m$ W-tagging +  \\
                       &  Boosted High $\Delta m$    & Boosted High $\Delta m$       &   Boosted High $\Delta m$  \\
\hline
\hline
(900,1) &     2.68 & 2.63 & 3.43  \\
\hline
(800,300) &   3.58 & 3.76 & 4.61  \\
\hline
(650,450) &   0.38 & 0.39 & 0.44  \\
\hline
\end{tabular}
\caption[Table caption text]{tagging results for 2.3fb. }
\label{tab:taggingResults}
\end{center}
\end{table}


%-expected significance, reject backfround hypothesis

%leptonic diagram~\cite{CMS:2016vew}
%MT2W - formula; explanation; supress lost lepton (already suppressed by requiring no additional lepton, but not enough); tries to reconstruct the event under tt2l and one undetected lepton assumption; for signal large delta M leads to alrge MT2W, while small delta M has lage MT2W; endpoint at top mass
%tmod - formula; chi2 like variable how well the event agrees with tt2l hypothesis, similar behavior to Mt2W; removing some of the terms from oifficial topness helps the discrimination; works better at low jet multiplicities than MT2W

\subsection{Triggers, data and simulated samples~\label{sec:trigger}}

In this analysis several kinds of triggers are used for different purposes. The single lepton data are triggered with single lepton and \MET triggers, where the \MET triggers help to select data with lower \pt leptons which would not pass the single lepton trigger. In the analysis OR of these triggers is used. The data triggered by the double lepton trigger serve for check of kinematics of lost lepton background. Similarly the data triggered by single photon trigger are used for cross-check of \MET resolution in one lepton background.

The SM background samples are generated by leading-order or next-to-leading-order generators and processed by full simulation of CMS. The signal samples are produced by leading-order generators and injected into fast simulation to simulate the resaponse of CMS detecotr to signal events.

\subsection{Objects and event selection~\label{sec:objects}}

The presented analysis selects events with single lepton, jets, b-jets and \MET in final state. On the other hand the events which in final state have tau leptons or second ``veto'' lepton passing looser lepton criterium are vetoed. To select or reject such events, the obejcts definition and criteria on its selection are introduced.

\textbf{Primary vertex}

The primary interaction vertex in the event must passed the good quality criteria and it is identified as a vertex which leads to the largest sum \pt$^{2}$s of physics objects belonging to this vertex.

\textbf{Lepton}

The ``selected'' lepton for this analysis is a large \pt electron or muon isolated from other activity in detector. The selected lepton must originate from primary vertex and have \pt>20~GeV and $|\eta|<1.4442$ for electron and $|\eta|<2.4$ for muon. This analysis also defines second ``veto'' lepton which fails isolation criteria, \pt thershold or which pass looser identification than the selected one. 

\textbf{Jets and b-jets}

The jets are reconstructed by the anti-kt algorithm with cone radius parameter of 0.4. The jet \pt is required to be higher than 30~Gev and itn must be in range $|\eta|<2.4$. The medium b-tagging working point is used for signal regions where the $M_{\ell b}$ is small and tight working point in those with large $M_{\ell b}$.  

\textbf{Veto on isolated tracks}

Majority of the tau leptons decay into one charged track which is predominantly charged hadron. Therefore the events with taus can be supressed by vetoing those, which ahve one isolated charged track.

\textbf{Veto on hadronic tau}

This additional veto helps to veto the taus which pass isolated track sleection. It mainly helps to reduce the lost lepton background.

\subsection{Baseline search region selection~\label{sec:baseline}}

After evaluating which kinematic variables help to supress SM background while keeping high signal yields and identifing criteria on selected and vetoed objects the baseline search region is defined followingly:

\begin{itemize}

\item Good primary vertex
\item One selected electron or muon
\item No veto lepton, isolated track or hadronic tau
\item At least two jets
\item At least on b-jet passing medium or tight, if $M_{\ell b} \leq$175~GeV or $M_{\ell b}>$175~GeV, respectively
\item \MET>250~GeV
\item $M_{T}$>150~GeV
\item $M_{T}$>150~GeV
\item min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ > 0.8

\end{itemize}

In the compressed T2tt region the top quarks are prodiced off-shell, limiting the available momentum,  and therefore their decay products have very low \pt consequently leading to low \MET. Obviously, such kind of the signal has different kinematics and the baseline selection must be adjusted to account for it. To increase the \MET in the event the stop pair must be boosted against and initial state radiation~(ISR), leading to the requirement of additional jet in the event. The ISR jet should have the largest \pt, but such requirement does not help overall signal discrimination as \MET>250~GeV is already required. The jet with the highest \pt must fail medimum b-tagging working point in order to increase the probability that it comes from ISR. The stop system should be boosted in opposite direcion to the ISR and tus the direction of objects originating from stop pair decay, like \MET and lepton should have similar direction. As the \pt of objects in compressed region is not large puting a limit on lepton \pt was found to be useful in signal discrimination. Taking these considerations into account in the corridor region the following additional criteria on the baseline search region  are imposed

\begin{itemize}
\item Five jets
\item Lepton \pt<150~GeV
\item Leading jet fails b-tagging medium working point
\item $\Delta \phi(E_{T}^{miss}, \ell)$<2.0 
\item min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ is loosened to be larger than  0.5

\end{itemize}

\subsection{Signal regions}

In order to increase the sensitivity of the analysis to the signal, the baseline search region can be divided into signal regions with help of discriminating variables defined in secton~\ref{sec:variables}. For the nominal search there are 8 signal region groups denoted A-H, leading to 27 exclusive signal regions. Additional group I of four signal regions is created for the T2tt compressed spectrum with 100<$\Delta m$<225~GeV. The definition of all signal regions is shown in Table~\ref{tab:SR}.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|lll|l|}
\hline
Group  &  $N_{J}$  & $t_{mod}$    &  $M_{\ell b}$ [GeV]     & \MET [GeV]                       \\
\hline
A      &  2-3      &   >10        &  $\leq$175              &  250-350, ~350-450, ~450-600, ~>600  \\
B      &  2-3      &   >10        &       >175              &  250-450, ~450-600, ~>600  \\
C      &  $\geq$4  &   $\leq$0    &  $\leq$175              &  250-350, ~350-450, ~450-550, ~550-650, ~>650  \\
D      &  $\geq$4  &   $\leq$0    &       >175              &  250-350, ~350-450, ~450-550, ~>550  \\
E      &  $\geq$4  &   0-10       &  $\leq$175              &  250-350, ~350-550, ~>550  \\
F      &  $\geq$4  &   0-10       &       >175              &  250-450, ~>450  \\
G      &  $\geq$4  &   >10        &  $\leq$175              &  250-350, ~350-450, ~450-600, ~>600  \\
H      &  $\geq$4  &   >10        &       >175              &  250-450, ~>450  \\
\hline
I      &  $\geq$5  &   --         &   --                    &  250-350, ~350-450, ~450-550, ~>550  \\
\hline
\end{tabular}
\caption[Table caption text]{SRs A-I. }
\label{tab:SR}
\end{center}
\end{table}

In the previous versions of analysis there is lower number of the signal regions, due to smaller available statistics of data. Also as discussed before, the $M_{T2}^{W}$ variable was used instead of $t_{mod}$ for definition of signal regions with three and more jets. In presented analysis a new disriminating avriable $M_{\ell b}$ was added and the signal regions were re-optimized with respect to $N_{J}$ and \MET .



%\subsection{Undefined}
%-SF, rewighting

%\subsection{compressed spectra}
%	ISR jets
%	recoil, enhance MET

\section{Background estimations}

After the baseline search region selection, tehre are three categories of relevant background from  the SM processes. The lost lepton background coming from $t\bar{t} \to 2 \ell$ and single-top processes where one of the leptons in misreconstructed makes the largest contribution in the signal regions with low $M_{\ell b}$ and at least four jets. This background is estimated with help of dilepton control samples. The one lepton background is represented by W+jets and $t\bar{t} \to 1\ell$ processes. These backgrounds are largely suppressed by \MET and $M_{T}$ cuts. After these cuts the yield of $t\bar{t} \to 1\ell$ process is the signal region is small and can be estimated from simulation. The W+jets lead to larger contribution especially in bins with high $M_{\ell b}$ and are estimated from zero b-tag control samples. $Z \to \nu \bar{\nu}$ is the last relevant background and it arises from $t\bar{t}Z$ and $WZ$ processes, where the z-bososn decays to neutrinos and one W-boson decays leptonically. The yields of $Z \to \nu \bar{\nu}$ in the signal region are estimated from simulation and are normalized by overall normalization factor obtaind from three lepton control region.  

In the described version of analysis my responsibility was to provide estimation of the $Z \to \nu \bar{\nu}$ background. In this secton this background is described first and than building on knowledge $Z \to \nu \bar{\nu}$ background estimation method, the other background estimates are intorduced.


\subsection{$Z \to \nu \bar{\nu}$ background estimation}

The $Z \to \nu \bar{\nu}$ backround, which arises from $t\bar{t}Z$ and $WZ$ processes with one elptonically decaying W-boson and Z-boson decaying to neutrinos is overall the smallest relevant group of backgrounds, but it is largely enhanced in high \MET and $t_{mod}$ signal regions. The main contribution to the $Z \to \nu \bar{\nu}$ background arises from  the $t\bar{t}Z$ process, however at low jet multiplicities, the $WZ$ process ebcomes an important part of this background.  The previous versions of this analysis estimated this background purely from simualtion, because of the small yield in the three lepton control region. Now with larger statistics, the normalziation of both processes is derived from data-driven method, but the shape is taken from simulation. 

  
To explain the data-driven beackground estimation technique, first only one signal~(SR) and control~(CR) region is considered. The control region for this kind of background is three lepton control region, in which th Z-boson decay to two leptons instead of two neutrinos. The relationship between data~($N^{Data}$) and MC~($N^{MC}$) yields in the signal (SR) and control (CR) region can be written followingly

\eq{bkgEst}
{
\frac{N^{Data,~SR}}{N^{Data,~CR}}  = \frac{N^{MC,~SR}}{N^{MC,~CR}},
}

therefore the estimated background yield in the signal region~($N^{Data~estimate,~SR}$) can be obtained from the simulated yield in signal region~($N^{MC,~SR}$) and the scale factor~($SF^{CR}$) determined from control region by rewriting Eq.~\ref{eq:bkgEst} as

\eq{bkgEst2}
{
N^{Data~estimate,~SR}  = N^{MC,~SR} \times  \frac{N^{Data,~CR}}{ N^{MC,~CR}} = N^{MC,~SR} \times  SF^{CR},
}

in the analysis there is not only one signal region and thus the background yield estimation in Eq.~\ref{eq:bkgEst2} needs to be performed for each signal region separately. Ideally there is same number of signal regions as control regions, which should be defined the same as the control regions, just with change of one requirement, in case of $Z \to \nu \bar{\nu}$ the lepton multiplicity. Consequently there should be same number of scale factors as signal regions.  But there is not enough of statistics to define as many three lepton signal regions asi the signal ones and therefore for the $Z \to \nu \bar{\nu}$ only one control region is used, leading to one scale factor. However the scale facor is determined for each procces ($t\bar{t}Z$, $WZ$) separately. For the $Z \to \nu \bar{\nu}$ background the Eq.~\ref{eq:bkgEst2} can be rewritten as

\eq{ZnunuEst}
{
N_{proc}^{Data~estimate, SR}  = N_{proc}^{MC, SR} \times SF_{proc}^{CR3l},
}

where the scale factor $SF_{proc}^{CR3l}$ is determined for each process from a template fit of $N_{b}$ distribution in the three lepton control region as described in the following subsection.

\subsubsection{Scale factors from the three lepton control region}


The scale factors for the $t\bar{t}Z$ and  $WZ$ were determined in paper~\cite{Sirunyan:2017uyt} which searches for BSM physics in final states with two leptons of same sign, \MET, and jets, and here the procedure of their estimation is briefly summarized. These normalization scale factors are determined from the distribution of number of b-jets in three lepton control region defined by requirements:

\begin{itemize}
\item Two leptons passing nominal isolation and identification, one of \pt>25~GeV and second of \pt>20~GeV
\item Third lepton of \pt>10~GeV passes tight identification and must have opposite sign and same flavour as one of the leptons above and the lepton flavour must have mass within $\pm$15~GeV window around Z-boson mass
\item $H_{T}$ = $\sum_{jets} p_{T}$>80~GeV
\item $N_{J} \geq 2$ with \pt>40~GeV
\item No cut on b-jets
\item \MET>30~GeV.

\end{itemize}

The $t\bar{t}Z$ and  $WZ$ are expected to have different number of b-jets and therefore the number of b-jets is discriminateing avriable between these processes. The zero b-jet bin of the the control region is composed around 70\% by the $WZ$ process, while the $t\bar{t}Z$ process represent around 70\% to the rest of the b-jet distribution. Except of these processes there is contribution to the control region from processes collectively denoted as ``rare SM'' and ``$X+\gamma$'' processes. The rare SM arises from diboson (ZZ), triboson (WWW, WWZ, WZZ, ZZZ), Higgs (HZZ, VH), same-sign WW from double-parton scattering (DPS WW), and rare top (tZq and $\mathrm{t\bar{t}t\bar{t}}$) processes decaying to three leptons. The second group of contribution is ``$X+\gamma$'', to which belong processes where one of the selected leptons is and electron from unidentified photon conversion. Such processes are $W\gamma$, $Z\gamma$, $t\bar{t}\gamma$ and $t\gamma$.

The distribution of number of b-jets in data is then simultaneously fitted by these four components. This fit procedure results in scale factors for $t\bar{t}Z$ of $1.14 \pm 0.30$ and for $WZ$ of $1.26 \pm 0.09$, which are defined as a ratio of data to simulation ratio in the three lepton control region. The uncertainty on the scale factors comprise both statistical uncertainty and uncertainties affecting the shape of the b-jet distribution in simulation. The post-fit distribution of number of b-jets in the three lepton control region is shown in Fig.~\ref{fig:figures/nbtagspostfit}.

    \insertFigure{figures/nbtagspostfit} % Filename = label
                 {0.5}       % Width, in fraction of the whole page width
                 { WZ, ttZ norm ~\cite{Sirunyan:2017uyt}. }


The final estimates of the $t\bar{t}Z$ and $WZ$ yields computed according to Eq.~\ref{eq:ZnunuEst} are shown in Table~\ref{tab:YZnunu}. The simulated events are corrected for different lepton and b-tagging efficiencies in data and simulation which is standard procedure for all background estimations. On top of this the events in $t\bar{t}Z$ and $t\bar{t}$ samples are reweighted in order to improve modelling of jets coming from ISR. The uncertainties on the estimates includes both statistical and systematic uncertainties which are described in section~\ref{sec:systematics}.


\begin{table}[h]
\begin{center}
\begin{tabular}{|l|ccc|}
\hline
&
\textbf{ttZ}    &
\textbf{WZ}     &
\textbf{Total}  \\
\hline
\textbf{ A 250$<E_T^{miss}<$350}         & 3.33 $\pm$ 0.96       & 1.38 $\pm$ 0.58       & 4.71 $\pm$ 1.21       \\
\textbf{ A 350$<E_T^{miss}<$450}         & 1.85 $\pm$ 0.53       & 0.21 $\pm$ 0.53       & 2.05 $\pm$ 0.75       \\
\textbf{ A 450$<E_T^{miss}<$600}         & 1.15 $\pm$ 0.33       & 0.47 $\pm$ 0.39       & 1.62 $\pm$ 0.53       \\
\textbf{ A $E_T^{miss}>$600}     & 0.36 $\pm$ 0.11       & 0.36 $\pm$ 0.41       & 0.71 $\pm$ 0.40       \\
\textbf{ B 250$<E_T^{miss}<$450}         & 0.61 $\pm$ 0.18       & 0.93 $\pm$ 0.47       & 1.54 $\pm$ 0.52       \\
\textbf{ B 450$<E_T^{miss}<$600}         & 0.15 $\pm$ 0.05       & 0.20 $\pm$ 0.34       & 0.35 $\pm$ 0.33       \\
\textbf{ B $E_T^{miss}>$600}     & 0.09 $\pm$ 0.03       & 0.02 $\pm$ 0.26       & 0.11 $\pm$ 0.26       \\
\textbf{ C 250$<E_T^{miss}<$350}         & 14.28 $\pm$ 3.84      & 0.10 $\pm$ 0.62       & 14.38 $\pm$ 3.92      \\
\textbf{ C 350$<E_T^{miss}<$450}         & 3.83 $\pm$ 1.06       & 0.60 $\pm$ 0.48       & 4.43 $\pm$ 1.23       \\
\textbf{ C 450$<E_T^{miss}<$550}         & 1.06 $\pm$ 0.31       & 0.73 $\pm$ 0.39       & 1.79 $\pm$ 0.52       \\
\textbf{ C 550$<E_T^{miss}<$650}         & 0.40 $\pm$ 0.12       & 0.00 $\pm$ 0.00       & 0.40 $\pm$ 0.12       \\
\textbf{ C $E_T^{miss}>$650}     & 0.20 $\pm$ 0.06       & 0.00 $\pm$ 0.00       & 0.20 $\pm$ 0.06       \\
\textbf{ D 250$<E_T^{miss}<$350}         & 2.06 $\pm$ 0.56       & 0.95 $\pm$ 0.63       & 3.01 $\pm$ 0.91       \\
\textbf{ D 350$<E_T^{miss}<$450}         & 0.62 $\pm$ 0.18       & 0.55 $\pm$ 0.40       & 1.18 $\pm$ 0.41       \\
\textbf{ D 450$<E_T^{miss}<$550}         & 0.24 $\pm$ 0.07       & 0.21 $\pm$ 0.22       & 0.45 $\pm$ 0.24       \\
\textbf{ D $E_T^{miss}>$550}     & 0.09 $\pm$ 0.03       & 0.00 $\pm$ 0.00       & 0.09 $\pm$ 0.03       \\
\textbf{ E 250$<E_T^{miss}<$350}         & 7.88 $\pm$ 2.14       & 0.40 $\pm$ 0.44       & 8.27 $\pm$ 2.21       \\
\textbf{ E 350$<E_T^{miss}<$550}         & 3.34 $\pm$ 0.94       & 0.52 $\pm$ 0.39       & 3.87 $\pm$ 1.07       \\
\textbf{ E $E_T^{miss}>$550}     & 0.26 $\pm$ 0.08       & 0.03 $\pm$ 0.25       & 0.29 $\pm$ 0.26       \\
\textbf{ F 250$<E_T^{miss}<$450}         & 0.98 $\pm$ 0.27       & 0.13 $\pm$ 0.17       & 1.11 $\pm$ 0.33       \\
\textbf{ F $E_T^{miss}>$450}     & 0.21 $\pm$ 0.06       & 0.00 $\pm$ 0.00       & 0.21 $\pm$ 0.06       \\
\textbf{ G 250$<E_T^{miss}<$350}         & 2.89 $\pm$ 0.78       & 0.14 $\pm$ 0.15       & 3.03 $\pm$ 0.81       \\
\textbf{ G 350$<E_T^{miss}<$450}         & 2.51 $\pm$ 0.71       & 0.16 $\pm$ 0.18       & 2.67 $\pm$ 0.77       \\
\textbf{ G 450$<E_T^{miss}<$600}         & 1.80 $\pm$ 0.50       & 0.16 $\pm$ 0.16       & 1.96 $\pm$ 0.54       \\
\textbf{ G $E_T^{miss}>$600}     & 0.70 $\pm$ 0.20       & 0.00 $\pm$ 0.00       & 0.70 $\pm$ 0.23       \\
\textbf{ H 250$<E_T^{miss}<$450}         & 0.37 $\pm$ 0.11       & 0.00 $\pm$ 0.00       & 0.37 $\pm$ 0.11       \\
\textbf{ H $E_T^{miss}>$450}     & 0.33 $\pm$ 0.10       & 0.16 $\pm$ 0.17       & 0.49 $\pm$ 0.20       \\
\hline
\textbf{ I 250$<E_T^{miss}<$350}         & 3.66 $\pm$ 1.01       & 0.66 $\pm$ 0.43       & 4.33 $\pm$ 1.16       \\
\textbf{ I 350$<E_T^{miss}<$450}         & 1.57 $\pm$ 0.45       & 0.35 $\pm$ 0.34       & 1.93 $\pm$ 0.59       \\
\textbf{ I 450$<E_T^{miss}<$550}         & 0.58 $\pm$ 0.17       & 0.19 $\pm$ 0.20       & 0.77 $\pm$ 0.27       \\
\textbf{ I $E_T^{miss}>$550}     & 0.41 $\pm$ 0.13       & 0.17 $\pm$ 0.17       & 0.58 $\pm$ 0.22       \\
\hline
\end{tabular}
\caption[Table caption text]{znunu yields SRs A-I. }
\label{tab:YZnunu}
\end{center}
\end{table}


\subsection{Lost lepton background estimation}

The lost lepton background composed of $t\bar{t} \to 2\ell$, single top, $t\bar{t}V$ and diboson processes where one lepton is misreconstructed, is dominant background because of the large tails in \MET and $M_{T}$ distribution caused by two neutrinos and not reconstructed lepton. The majority of lost lepton background arises from the $t\bar{t} \to 2\ell$ process. The background yields in the signal regions are  estimated according to Eq.\ref{eq:bkgEst2} from the dilepton control regions. These control regions have purity above 95\% in studied processes and therefore no correction for contamination from other processes is not needed. 

The control regions pass the same selection as the signal regions, except of the veto on the second lepton, which is reversed. Because of small yields in high \MET control regions, several control regions differing only by \MET range has to be merged. Due to this combination of the control regions, the background estimate in the signal regions has to be corrected by corresponding intrapolation factor. The lost lepton background estimate in signal region can be then 

\eq{bkgEstLL}
{
N^{Data~estimate,~SR}_{lost~\ell}  = N^{MC,~SR}_{lost~\ell} \times  \frac{N^{Data,~CR}_{\ell}}{ N^{MC,~CR}_{\ell}} \times \frac{N^{MC,~SR,~E_{T}^{miss}~bin}_{lost~\ell}}{ N^{MC,~SR}_{lost~\ell}},
}

where the first term on right side denotes the simulated yield in the signal region, second term corresponds to data to simulation scale factor in the control region and third factor is simulation based intrapolation factor, which is equal to one in case that the given control region was not merge with another one.

The described extrapolation of control regions in \MET could lead to unprecise estimates in case that the \MET shape in data and MC differ. For this reason the shape of the \MET is checked in enriched top quark region with electron muon pair in the final state. Then the data to simulation ratio in \MET bins is used as a correction of the simulated \MET spectra. 

The final estimates of the lost lepton background in the signal regions are shown in table of Fig.~\ref{fig:figures/Table005}, together with uncertainties including both statistical and systematic uncertainties which are described in section~\ref{sec:systematics}

%Additional correction factor obtained from studies of $\gamma$+jets processes is applied on background estimates to take into account differeneces in \MET resolution in data and simulation causing different migration between 
%An additional correction factor is used to take into account  ??!!
%gamma plus jets -> MET resolution > bin migrations - gamma pt spectrum reveighted to match neutrino pt spectrum, from reweighted events METmodified = reconstructed-MET + pTgamma -check that
%->ask about that -> not really clear from the papaer

\subsection{One lepton background estimation}


The  one lepton backround is the subleading background compsed of W+jets and $t\bar{t} \to 1\ell$ processes. The W+jets is the largest aprt of the one lepton background and is estimated from conrol regions with zero b-jets. The control regions are defined in a same way as signal regions, only the b-jet requirement is reversed. The   $t\bar{t} \to 1\ell$ represents only small contribution to the one elpton background and therefore is taken directly from the simulation.

In the zero b-jet control regions, the $M_{\ell b}$ variable needs to be redefined as there is no b-jet. Instead of the closest b-jet, the jet with highest CSVv2 discriminator is used to define $M_{\ell b}$ in this case. Contrary to the previous background estimations there is enough statistics to define corresponding zero b-tag control region to each signal region. On the other hand the control regions are not pure in W+jets process and the background estimates have to account for the contamination by non-W+jets processes in the control regions. The W+jets background estimation in the signal regions can be then written as

\eq{bkgEstWJ}
{
N^{Data~estimate,~SR}_{W+jets,~\geq~1btag}  = N^{MC,~SR}_{W+jets,~\geq~1btag} \times  \frac{N^{Data,~CR}_{W+jets,~0btag}}{ N^{MC,~CR}_{W+jets,~0btag}},
}

where the first term on right side denotes the simulated yield in the signal region and second term corresponds to data to simulation scale factor in the control region for W+jets process. Because of the contamination of conrol regions by other processes the $N^{Data,~CR}_{W+jets,~0btag}$ is defined followingly


\eq{bkgEstWJT2}
{
N^{Data,~CR}_{W+jets,~0btag} = N^{Data,~CR}_{all,~0btag} \times \frac{N^{MC,~CR}_{W+jets,~0btag}}{ N^{MC,~CR}_{all,~0btag}} ,
}

where the second term on the right side is simulation based factor which corrects data yields in control region for non-W+jets processes.

Because of the $M_{\ell b}$ redefinition, in this case it hase to be investigated if the shape of this variable agree in the control and signal regions. The study based on simulations confirmed, that the two shapes are in agreement. 


From the simulation the contribution to the total background from $t\bar{t} \to 1\ell$ process is lower than 10\% and therefore it is resonable to estimate the background yield directly from simualtion. The $t\bar{t} \to 1\ell$ process appears in signal regions mainly due to the \MET resolution. As this background is purely estimated from simulation the mismodelling of the \MET resolution could case imprecise estimation. The \MET resolution is checked with help of $\gamma$+jets data and simulated samples. The $\gamma$ \pt spectrum in data and simulation is reweighted to match the neutrino \pt spectrum in the $t\bar{t} \to 1\ell$. Then the modified \MET is computed as a sum of reweighted $\gamma$ \pt and the reconstructed \MET. The data to simulation ratio in the modified \MET distribution then can reveal the \MET resolution mismodelling. It was found out that the difference between data and simulation in modified \MET are up to 40\%, but this mismodelling is only taken into account as a systematic uncertainty. %really true? 
 

The final estimates of the one lepton background in the signal regions are shown in table of Fig.~\ref{fig:figures/Table005}, together with uncertainties including both statistical and systematic uncertainties which are described in section~\ref{sec:systematics}.

\subsection{Systematic uncertainties~\label{sec:systematics}}

There are several uncertainties entering into the background estimations, one is the uncertainty due to limited statistics in simulated and control data samples and others arise for example from the differences in modelling of objects in data and simulation. Many uncertainties are common to all background estimates and therefore here again, due to my personal involvment, first the systematic uncertainties for the $Z \to \nu bar{\nu}$ uncertainties are described.  Then the differences for other background estimates are discussed.

\subsubsection{Systematic uncertainties on the $Z \to \nu \bar{\nu}$ background estimation}

There are two kinds of systematic uncertainties affecting the background estimates. The first group is reffered as ``theoretical uncertainties'' and consists of the uncertainties arising from the choice of factorization and renormalization scale, PDFs and $\alpha_{S}$. As for the $Z \nu \bar{\nu}$ processes the background yields in signal regions are taken from simulation and only normalized by overall factor determined by the data-driven method, these estimates are sesitive to the migrations of events between signal regions due to imprecise modelling of the tehoretical parameters in the simulation. Because of the limited statistics in the signal regions large fluctuations driven by the statisctics were observed when varying the theoretical parameters. To avoid this for evaluation of systematic uncertainties, the binning of signal regions in $M_{\ell b}$ and $t_{mod}$ was removed and only binning in \MET and $N_{J}$ was kept. Effectively we grouped signal regions accros grops A--B and C--H, the corridor region is treated separately. To be sure that the uncertainty is not underestimated due to grouping in $M_{\ell b}$ and $t_{mod}$ the distribution of these variables for varied theoretical parameters were studied. Only small variations in shape of $M_{\ell b}$ and $t_{mod}$ were observed when varying the theoretical parameters, compared to the shapes of \MET and $N_{J}$. In these grouped regions, the relative uncertainty on the background estimated isevaluated and then applied on to the corresponding search signal regions background estimates obtain the absoule uncertainties. In the following paragraphs the tehoretical uncertainties are discussed one by one.


\textbf{Factorization and renormalization scale}

The cross section of a given process is dependeds on the choice factorization and renormalziation scales. The theoretical $t\bar{t}Z$ cross section at 13~TeV obtained in~\ref{deFlorian:2016spz} is 839.3~fb. the quothed scale uncertainty is around 10\%. As the normalization of the background estimate is derived from data, the scale uncertainty should only influence the shapes and consequently the migration of events between signal regions. In this analysis both renormalziation and factorization scales are varied simultaneously, while keeping cross section constant. The uncertainty is then evaluated from the change of background estimates when varying the scales. The scale uncertainty on the $Z \to \nu \bar{\nu}$ estimate in  each signal region is shown in Table~\ref{tab:SysZnunu}, overall it is lower than 10\%.

%The central value for renormalization and factorization scales is set to 0=Mt+MV/2.
%For the NLO QCD part of the calculation scale uncertainties are estimated by independent variations of
%renormalization and factorization scales in the range /2R,F2, with 1/2R/F2,while for PDF and s uncertainties the PDF4LHC15 prescription is used. The resulting uncertainties are applied also to NLO EW correction effects
%str 156 handbook

\textbf{PDFs}

%The uncertainty on the $t\bar{t}Z$ cross section quoted in~\ref{deFlorian:2016spz} is 2.8\%

The NNPDF3.0~\cite{Ball:2014uwa} set of parton distribution functions~(PDFs) is used for production of simulated samples. To evaluate the uncertainty arising from PDFs, 100 variations were computed and each was used to calculate an event weight $y_{i,n}$, where $i$ denotes i-th event and $n$ the n-th variation. Then the stadard deviation of the PDF variations for an event $i$ computed as

\eq{pdfVar}
{
\sigma_{i} = \sqrt{\frac{1}{99} \sum_{n=1}^{100} (y_{i,n} - \bar{y}_{i})^{2}}, ~with~ \bar{y}_{i} = \frac{1}{100} \sum_{n=1}^{100} y_{i,n},
}

is used to evaluate the PDF uncertainties. Here again only shape uncertainties are taken into account by keeping the normalization constatnt. The uncertainties due to PDFs go up to 10\% as shown in Table~\ref{tab:SysZnunu} for $Z \to \nu \bar{\nu}$ estimates in all signal regions.

\textbf{$\alpha_{S}$}

The unceratinties on the background estimates coming from $\alpha_{S}$ are as well evaluated with help of the event weights. The defalut value of $\alpha_{S}$ which is 0.118 is varied to 0.117 and 0.119.  The intereset is in the shape uncertainties and therefore again the normalization is forced to be constant when varying $\alpha_{S}$. The final uncertainties are of order of 3\% and are show in Table~\ref{tab:SysZnunu} for all $Z \to \nu \bar{\nu}$ background estimates.


The Fig.~\ref{fig:figures/ttZUnc} shows the $t\bar{t}Z$ \MET distribution in grouped regions of A--B~(left) and C--H(right) with a band corresponding to combined scales, PDFs and $\alpha_{S}$ uncertainties. It can be noticed taht the uncertainties are very small for low \MET, but grow with \MET up to around 10\%.

    \insertTwoFigures{figures/ttZUnc}
                 {figures/AB250lessMETlessInfCanFrank} % Filename = label
                 {figures/CDEFGH250lessMETlessInfCanFrank} % Filename = label
                 {0.49}       % Width, in fraction of the whole page width
                 { ttZ (theoretical?) uncertainty. }


The second group of systematic uncertainties is reffered as ``experimental uncertainties''. Part of the uncertainties originate from uncertainties on scale factors, which are light and heavy flavour b-tagging and lepton scale factors. The uncertainties evlauated variations of scale factors as persribed by corresponding POGs~(Physics objects Groups) are present in Table.~\ref{tab:SysZnunu} for $Z \to \nu \bar{\nu}$ background estimates. The uncertainty on the b-tagging and lepton sclae factors is typically of order of few percent, except of few outliers caused by limited statistics in simulated samples.

Another source of uncertainty arises from the Jet Energy Scale~(JES). The Jet Energy Corrections~(JEC) must be applied on the reconstructed jets to have correct energy scale of the jets. The uncertaintes from variations of JES are of order of few percent, alhough in few signal regions with small statistics they can reach up to three or four tens of percent. The uncertainty arising from differences in pile-up profile in data and simulation was found to be extremely sensitive to the limited statistics and therefore for $Z \nu bar{\nu}$ processes only one gloabl uncertainty was evaluated by varying the PU in the baseline search region. The systematic uncertainty on PU was determined to be lower than one percent, but the conservative estimate of 3\% is used. As for $t\bar{t}Z$ process the ISR reweighting was applied, the uncertainty arising from this procedure must be evaluated by varuing the ISR weight. The uncertainty introduced by ISR reweighting was evaluated to be lower than 10\%.

The dominant ucertainty comes from limited statistics in the simulated samples. The statistical uncertainty varies from around 5\% to 90\% and in one signal region reaches up to 228\%.

The breakedown of the systematic uncertainties on the $Z \nu \bar{\nu}$  background estimates in all signal regions is shown in Table~\ref{tab:SysZnunu}. The summarized typical order of magnitude of these uncertainties is presented in table~\ref{tab:SysZnunuSum}.

%Like all experimentally-reconstructed objects, jets need to be calibrated in order to have the correct energy scale: this is the aim of the jet energy corrections (JEC). 

\begin{table}[h]
\begin{center}
\noindent\hrulefill
\smallskip\noindent
\resizebox{\linewidth}{!}{%
\begin{tabular}{|l|cccccccccccc|}
\hline
&
\textbf{MC stat.}  &
\textbf{$SF_{l}$}        &
\textbf{$SF_{b}$(light)}    &
\textbf{$SF_{b}$(heavy)}    &
\textbf{PU}         &
\textbf{PDF}  &
\textbf{$\alpha_{S}$}       &
\textbf{Scales}   &
\textbf{$SF_{ISR(n_{jets})}$}   &
\textbf{JES}  &
\textbf{Norm.}        &
\textbf{Total}  \\
\hline
\textbf{ A 250$<E_T^{miss}<$350}         & 11.88         & 0.31          & 1.58          & 1.23          & 3.00          & 0.23          & 0.86          & 3.86          & 7.12          & 2.55          & 20.70         & 25.60         \\
\textbf{ A 350$<E_T^{miss}<$450}         & 25.04         & 0.35          & 3.63          & 0.77          & 3.00          & 3.33          & 0.61          & 1.31          & 9.09          & 0.37          & 24.38         & 36.61         \\
\textbf{ A 450$<E_T^{miss}<$600}         & 23.87         & 0.37          & 1.34          & 1.34          & 3.00          & 0.45          & 0.39          & 2.47          & 7.02          & 0.46          & 20.77         & 32.71         \\
\textbf{ A $<E_T^{miss}>$600}    & 51.43         & 0.41          & 2.87          & 1.74          & 3.00          & 8.04          & 0.36          & 2.37          & 5.13          & 10.54         & 16.77         & 56.17         \\
\textbf{ B 250$<E_T^{miss}<$450}         & 29.98         & 0.50          & 1.67          & 1.23          & 3.00          & 1.17          & 0.76          & 2.86          & 3.97          & 1.01          & 14.74         & 34.01         \\
\textbf{ B 450$<E_T^{miss}<$600}         & 86.04         & 0.77          & 5.28          & 1.26          & 3.00          & 0.45          & 0.40          & 2.47          & 4.00          & 32.62         & 15.46         & 93.63         \\
\textbf{ B $<E_T^{miss}>$600}    & 228.31        & 2.65          & 19.40         & 1.64          & 3.00          & 8.03          & 0.36          & 2.37          & 8.12          & 44.04         & 22.72         & 234.76        \\
\textbf{ C 250$<E_T^{miss}<$350}         & 4.56          & 0.45          & 0.06          & 0.68          & 3.00          & 0.81          & 0.60          & 1.77          & 0.40          & 4.78          & 26.18         & 27.26         \\
\textbf{ C 350$<E_T^{miss}<$450}         & 10.85         & 0.35          & 0.48          & 0.66          & 3.00          & 2.01          & 1.23          & 5.00          & 0.99          & 6.85          & 23.73         & 27.73         \\
\textbf{ C 450$<E_T^{miss}<$550}         & 20.70         & 0.31          & 4.53          & 0.26          & 3.00          & 1.89          & 1.34          & 4.86          & 2.03          & 4.52          & 18.54         & 29.25         \\
\textbf{ C 550$<E_T^{miss}<$650}         & 9.20          & 0.38          & 0.77          & 0.38          & 3.00          & 4.90          & 1.86          & 3.67          & 3.67          & 2.14          & 26.32         & 29.09         \\
\textbf{ C $<E_T^{miss}>$650}    & 12.73         & 0.45          & 0.20          & 0.40          & 3.00          & 8.11          & 3.08          & 4.23          & 4.92          & 5.74          & 26.32         & 31.85         \\
\textbf{ D 250$<E_T^{miss}<$350}         & 19.82         & 0.31          & 0.72          & 0.41          & 3.00          & 0.81          & 0.60          & 1.76          & 0.82          & 9.76          & 20.24         & 30.20         \\
\textbf{ D 350$<E_T^{miss}<$450}         & 27.58         & 0.32          & 1.82          & 1.08          & 3.00          & 2.02          & 1.23          & 4.99          & 1.63          & 10.79         & 17.33         & 34.98         \\
\textbf{ D 450$<E_T^{miss}<$550}         & 47.40         & 0.33          & 4.96          & 0.22          & 3.00          & 1.88          & 1.33          & 4.85          & 2.24          & 17.87         & 17.31         & 54.17         \\
\textbf{ D $<E_T^{miss}>$550}    & 18.94         & 0.44          & 0.06          & 0.83          & 3.00          & 7.53          & 2.44          & 2.55          & 7.31          & 7.15          & 26.32         & 35.14         \\
\textbf{ E 250$<E_T^{miss}<$350}         & 5.55          & 0.42          & 0.65          & 0.74          & 3.00          & 0.81          & 0.60          & 1.77          & 2.52          & 3.80          & 25.40         & 26.66         \\
\textbf{ E 350$<E_T^{miss}<$550}         & 10.40         & 0.40          & 0.01          & 0.83          & 3.00          & 1.97          & 1.26          & 4.96          & 0.66          & 7.23          & 23.72         & 27.63         \\
\textbf{ E $<E_T^{miss}>$550}    & 87.72         & 0.77          & 7.05          & 0.02          & 3.00          & 7.55          & 2.47          & 2.51          & 2.26          & 1.66          & 24.61         & 91.86         \\
\textbf{ F 250$<E_T^{miss}<$450}         & 16.02         & 0.42          & 0.06          & 0.31          & 3.00          & 1.16          & 0.78          & 2.70          & 2.10          & 4.41          & 24.07         & 29.63         \\
\textbf{ F $<E_T^{miss}>$450}    & 12.82         & 0.47          & 0.40          & 0.57          & 3.00          & 3.72          & 1.70          & 3.91          & 2.22          & 3.37          & 26.33         & 30.26         \\
\textbf{ G 250$<E_T^{miss}<$350}         & 5.75          & 0.39          & 0.01          & 1.71          & 3.00          & 0.81          & 0.60          & 1.77          & 2.09          & 3.44          & 25.42         & 26.68         \\
\textbf{ G 350$<E_T^{miss}<$450}         & 7.06          & 0.43          & 0.05          & 1.35          & 3.00          & 2.01          & 1.23          & 5.00          & 1.79          & 9.62          & 25.14         & 28.63         \\
\textbf{ G 450$<E_T^{miss}<$600}         & 9.01          & 0.37          & 0.19          & 1.27          & 3.00          & 2.63          & 1.54          & 4.72          & 0.51          & 5.15          & 24.77         & 27.64         \\
\textbf{ G $<E_T^{miss}>$600}    & 7.00          & 0.47          & 0.19          & 1.01          & 3.00          & 8.01          & 2.34          & 1.74          & 0.68          & 15.62         & 26.32         & 32.70         \\
\textbf{ H 250$<E_T^{miss}<$450}         & 9.94          & 0.49          & 0.05          & 0.07          & 3.00          & 1.17          & 0.79          & 2.72          & 2.06          & 4.60          & 26.32         & 28.91         \\
\textbf{ H $<E_T^{miss}>$450}    & 34.01         & 0.43          & 0.04          & 0.88          & 3.00          & 3.71          & 1.70          & 3.91          & 0.93          & 2.41          & 19.94         & 40.03         \\
\textbf{ I 250$<E_T^{miss}<$350}         & 9.76          & 0.38          & 0.78          & 0.47          & 3.00          & 0.21          & 0.32          & 3.91          & 4.30          & 5.32          & 23.38         & 26.72         \\
\textbf{ I 350$<E_T^{miss}<$450}         & 17.91         & 0.35          & 0.76          & 0.60          & 3.00          & 1.99          & 1.41          & 5.88          & 4.56          & 6.05          & 22.78         & 30.79         \\
\textbf{ I 450$<E_T^{miss}<$550}         & 25.04         & 0.51          & 4.83          & 0.35          & 3.00          & 1.89          & 1.62          & 4.90          & 5.38          & 4.89          & 21.64         & 34.80         \\
\textbf{ I $<E_T^{miss}>$550}    & 29.62         & 0.40          & 0.22          & 0.68          & 3.00          & 1.82          & 1.55          & 7.48          & 4.76          & 6.34          & 20.76         & 37.98         \\
\hline
\end{tabular}}
\caption[Table caption text]{znunu syst SRs A-I. }
\label{tab:SysZnunu}
\end{center}
\end{table}


\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Source}       & \textbf{Typical range of values} [\%]                       \\
\hline
\textbf{MC statistics}      &  5-88   \\
\hline
\textbf{Lepton efficiencies}              &  <3  \\
\textbf{b-tagging SF (light flavor)}      &  1-19  \\
\textbf{b-tagging SF (heavy flavor)}      &  <2  \\
\textbf{Jet energy scale}                 &  1-44  \\
\textbf{Pile-up}                          &  <3  \\
\hline
\textbf{PDF}                 &  1-8  \\
\textbf{$\alpha_{S}$}                 &  1-3  \\
\textbf{Scales}                 &  1-8  \\
\textbf{$SF_{ISR(n_{jets})}$}                 &  1-9  \\
\textbf{Normalization}                 &  15-26  \\
\hline
\textbf{Total}                 &  26-94  \\
\hline
\end{tabular}
\caption[Table caption text]{summmary znunu syst }
\label{tab:SysZnunuSum}
\end{center}
\end{table}

\subsubsection{Systematic uncertainties on the lost lepton background estimation}

As for $Z \nu \bar{nu}$ the uncertainties on the lost lepton background estimates contain the uncertainties due to limited statistics in simulation, ISR reweighting, PU, JES, PDFs, scales, $\alpha_{S}$, b-tagging and lepton scale factors  applied as well, although many of them largely cancel in $N^{MC,~SR}_{lost \ell}\ N^{MC,~CR}_{\ell\ell}$ ratio used for estimation of lost lepton background yields in SRs.

Except of these uncertainties following uncertaintes have to be evaluated:

\begin{itemize} 
\item Uncertainty from the limited statistic of data in control regions
\item Uncertainty arising from the efficiency of dilepton trigger used for CR samples
\item Uncertainty on the \MET resolution which can cause migration of events in between signal regions
\item Uncertainty arising from different \MET shape in data and simualtion; the shape difference can again cause migration of events in between signal regions
\item Uncertainty on efficiency of hadronic tau and isolated track vetos %why we do not have this?
\end{itemize}


Here again the dominat uncertainty arise from the limited statistics data and simulation which can go up to around 60\%.


\subsubsection{Systematic uncertainties on the one lepton background estimation}

The uncertainties on the W+jets background estimates are very similar to the previous cases as they contain uncertainties due to limited statistics in simulation, PU, JES, PDFs, scales, $\alpha_{S}$, b-tagging and lepton scale factors  applied as well, although many of them largely cancel in $N^{MC,~SR}_{lost \ell}\ N^{MC,~CR}_{\ell\ell}$ ratio used for estimation of lost lepton background yields in SRs.

Except of these uncertainties following uncertaintes have to be evaluated:

\begin{itemize} 
\item Uncertainty from the limited statistic of data in control regions
%\item Uncertainty on the \MET resolution which can cause migration of events in between signal regions
\item Uncertainty on W+b cross section
\item Uncertainty on the contamination of control regions
\end{itemize}

In tjis case the dominat uncertainties arise from the limited statistics in data and simulation and also from the contamination of control regions by non-W+jets processes. These uncertainties can reach up to 90\%.

The uncertainty on $t\bar{t} \to 1\ell$ estimates was evaluated to be 100\% from the studies of the \MET resolution.

\subsubsection{Uncertainties on the signal yields}

The signal uncertainties are evaluated similarly as the background ones.  As previously the largest source of uncertainty is the limited statistics in the simulated samples which varies from 5 to 25\%. The subleading uncertainties come from JES, ISR and PU modelling. Th signal uncertainties with their typicall values are summarized in table of Fig.~\ref{fig:figures/Table006}.

    \insertFigure{figures/Table006} % Filename = label
                 {0.7}       % Width, in fraction of the whole page width
                 { signal uncertainties ~\cite{Sirunyan:2017xse}. }

\textbf{Pile-up uncertainty}

For the 2015 version of analysis~\cite{Sirunyan:2016jpr} I performed studies of the dependency of the $M_{T}$ and \MET distributions in signal on the pile-up. The results of these study were used to assess one of the systematic uncertainties on the signal yields.

To difference in the $M_{T}$ and \MET distrubutions as a function of pile-up was evaluated staring from the baseline search region, removing requirement on $M_{T}$ and \MET, respectively. The total number of events $N$ was divided into bins in the number of primary vertices in the events $N_{vtx}$. In each $N_{vtx}$ bin $i$ the number of events passing the criterum $M_{T}$>150~GeV~($N_{i,M_{T}>150}$) (\MET>250~GeV~($N_{i,E_{T}^{miss}>250}$)) were divided by the total number of events in bin $i$~($N_{i}$) and scaled by $N$. The evolutions of variable $N_{i,M_{T}>150}/N_{i} \times N$ and $N_{i,E_{T}^{miss}>250}/N_{i} \times N$ as a function of $N_{vtx}$ for T2tt signal points (225,25--150) are shown in Fig.~\ref{fig:figures/pileupUnc}. In the figures it can be noticed that  the distrunution are not flat and varying $N_{vtx}$ by five units leads to uncertainty up to around 7\%. This value was used as an uncertainty for low $\Delta m$ signals. The variations are smaller for high $\Delta m$ signals, leading to uncertainty up to 2\%.

    \insertTwoFigures{figures/pileupUnc}
                 {figures/tailnrMT22525to150nNEW} % Filename = label
                 {figures/tailnrMET22525to150nNEW} % Filename = label
                 {0.49}       % Width, in fraction of the whole page width
                 { nvtx in MT and MET. }
\newpage
\section{Results and interpretation}

The results of the background estimations with their systematical uncvertainties together with the observed data in the signal regions are shown in table of Fig.~\ref{fig:figures/Table005}. These results are graphically presented in Fig.~\ref{fig:figures/Table005}. In this figure also yields of the three different signal models for a chosen signal point are shown. 

In the singal regions no significant excess of data from the SM expectation is observed and therefore the $CL_{s}$ method described in section~\ref{sec:stats} was used to interpret the results as limits on considered SMS signal models. These limits on model production cross-section are produced as a function of SUSY particles masses, in this case the stop and LSP (neutralino) masses and they are obtained by combining all signal regions for a given signal point. The contour line is drawn where the teroetical cross section given in~\cite{Borschensky:2014cia} agrees with the predicted one. The signal points with production cross ection lower than tehoretical one are excluded. The signal yields enetering the statistical procedure are corrected for possible contamination of control regions by SUSY signali, these corrections are around 5\% to 10\%.

    \insertFigure{figures/Table005} % Filename = label
                 {0.99}       % Width, in fraction of the whole page width
                 { result table ~\cite{Sirunyan:2017xse}. }

    \insertFigure{figures/resultPlot} % Filename = label
                 {0.75}       % Width, in fraction of the whole page width
                 { result plot ~\cite{Sirunyan:2017xse}. }

The limit plots on T2tt and T2tb model are shown for the first analysis of the 13~TeV data~\cite{Sirunyan:2016jpr} and the last presented analysis discussed by default above~\cite{Sirunyan:2017xse}. The 95\% confidence level~(CL) upper limit on the T2tt model ($ pp \to \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t \bar{t} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}$ ) assuming unpolarized top quarks originating from stops is shown in left parto of Fig.~\ref{fig:figures/limitsT2ttT2tb} for data corresponding to integrated luminosity of 2.3~fb$^-1$ (top) and 35.9~fb$^-1$ (bottom). The analysis of 2015 data~\cite{Sirunyan:2016jpr} combines searches for the stop quark pair production in all-hadronic final states and in $1\ell$ final state. Therefore on the top left plot of Fig.~\ref{fig:figures/limitsT2ttT2tb} the expected limit from the $0\ell$~(magenta dashed line) and $1\ell$~(blue dashed line) are depicted. Both observed~(black line) and expected~(red dashedi line) limits are computed from combination of signal regions of both analysis, which are mutually exclusive. The $0\ell$ analysis outperforms the $1\ell$ analysis in the part of the mass plane where the stop is heavy and LSP light, becuase of the larger signal acceptance. On the other hand the $1\ell$ analysis is more sensitive in the region of heavier LSP and intermediate stop mass. The combined analysis of data corresponding to integrated luminosity of 2.3~fb$^-1$ excludes the stop masses between 280 and 830~GeV when LSP is masless and LSP masses up to 260~GeV for stop mass of 675~GeV. The analysis~\cite{Sirunyan:2017xse} of data corresponding to integrated luminosity of 35.9~fb$^-1$ combines 4 singal regions of the corridor analysis in region 100 $\leq \Delta m \leq $ 225~GeV and the 27 signal regions of the nominal analysis everywhere else. The exclusion limits purely based on $1\ell$ search exclude  the stop masses up to 1120~GeV for a masless LSP and LSP masses up to 515~GeV for the stop mass of 950~GeV. In both analysis, in case of the T2tt model the white region where LSP is light and $\Delta m$ is around mass of the top quark is not interpreted becuase of the problematic modelling of the quickly changing kinematics in this region.

The exclusion limits of analysis \cite{Sirunyan:2016jpr} and  \cite{Sirunyan:2017xse} for the T2tb signal, where the branching ratio of $ \tilde{t}_{1} \to t  \tilde{\chi}^{0}_{1} $ and $ \tilde{t}_{1} \to b  \tilde{\chi}^{\pm}_{1} $  is 50\% each and $m_{\tilde{\chi}_{1}^{\pm}} = m_{\tilde{\chi}_{1}^{0}} + 5~\mathrm{GeV}$ , is show in the right part of Fig.~\ref{fig:figures/limitsT2ttT2tb}. The combined analysis of data corresponding to integrated luminosity of 2.3~fb$^-1$ excludes the stop masses up to 725~GeV for the intermediate LSP masses and the LSP masses up to 210~GeVi for the stop mass of around 500~GeV. This exclusion is mainly driven by the fully hadronic analysis due to its larger signal acceptance. The analysis~\cite{Sirunyan:2017xse} of data corresponding to integrated luminosity of 35.9~fb$^-1$ excludes the stop masses up to 980~Gev for a masless LSP and LSP masses up to 400~GeV for a stop mass of 825~GeV. 

    \insertFourFigures{figures/limitsT2ttT2tb}
                 {figures/limitT2tt} % Filename = label
                 {figures/limitT2tb} % Filename = label
                 {figures/limitT2ttOLD} % Filename = label
                 {figures/limitT2tbOLD} % Filename = label
                 {0.49}       % Width, in fraction of the whole page width
                 { limit plots t2tt/t2tb OLD ~\cite{Sirunyan:2016jpr} and t2tt/t2tb new  ~\cite{Sirunyan:2017xse}}

The last considered model is T2bW where $ pp \to \tilde{t}_{1} \bar{\tilde{t}}_{1} \to b \bar{b} \tilde{\chi}^{+}_{1} \tilde{\chi}^{-}_{1} \to b \bar{b} W^{+} W^{-} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}$ and the chargino mass is fixed by condition $m_{\tilde{\chi}_{1}^{\pm}} = ( m_{\tilde{t}} +  m_{\tilde{\chi}_{1}^{0}} )/2$. The 95\% CL upper limit on this model obtained by analysis~\cite{Sirunyan:2017xse} is shown  in Fig.~\ref{fig:figures/limitT2bW}. In this model the stop masses are excluded up to 1000~GeV for a masless LSP and LSP masses up to 450~GeV for stop mass of 800~GeV.

    \insertFigure{figures/limitT2bW} % Filename = label
                 {0.5}       % Width, in fraction of the whole page width
                 { limit plot t2bW ~\cite{Sirunyan:2017xse}. }

The excluded stop mass in T2tt model was increased by around of 50~GeV in early Run~II compared to the strongest exclusion results of Run~II presented in Fig.~\ref{fig:figures/T2tt2015} despite the lower integrated luminosity. This increase in the sensitivity is caused by the increased center-of-mass-energy from 8~TeV to 13~TeV and improvement in optimization of the analyses. The analysis of data corresponding to integrated luminosity of 35.9~fb$^-1$ outperforms the  combined analysis of data corresponding to integrated luminosity of 2.3~fb$^-1$ by almost 300~GeV in the stop mass exclusion of the T2tt model and around 250~GeV in the stop mass exclusion of the T2tb model. The exclusion limit on the stop mass reaches the TeV range which is the naturalness boundary, although this is not true for all models and depending on the model limits can become weaker.

\section{Conclusion}

%tables and plots

\subsection{Perspectives: W/top-tagging ?}

%\textbf{Motivation}

%	high delta M regim - boost -> jets merge

%\textbf{Techniques}
%	larger radius jets
%	tau ratios - N subjetiness
%	different masses-> grooming techniques
	
%\textbf{Results}
%	end of 2015 - slide 8,9 - results on different ak8 W-tagging categories (18/11); lumi=2 1/fb
%	mid of 2016 -update of previous study - lumi=2.26 fb; ak8+ak10 tagging; tables slide 7, 11 -> mo improvement with ak8, but slight improvement with ak10 


%\textbf{Perspectives}
%	-results from Sicheng with top tagging
%	-resolved top tagger - 3ak4 jets
%	-second presentation (31/01 same as 12 feb?), slide 6 - interesting plot, slide 10,11 significance table
%        -merged top tagger - boosted objects
%		slide 4, 5, 6

%\subsection{Depndence of discriminating variables on pileup}
%-not much to say
	
