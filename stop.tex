\chapter{Search for top squark pair production in pp collisions at sqrt(s)=13~TeV using single lepton events}

%\section{Introduction and motivation}

The supersymmetry was found to be the most popular extention of the Standard Model, due to its capabality to adress many shortcomings of the SM. Among others it provides a natural solution to the hierarchy probelm and provieds dark matter candidate. Supersymmetry introduces a supersymmetric partner to each SM particle, which have the same quantum numbers except of spin differing by 1/2. This chapter is focused on the production of the suppersymetric partners of the stop quarks, reffered as ``stops''~($\tilde{t}$). There are two scalar stops $\tilde{t}_{R}$ and  $\tilde{t}_{L}$ as there is left and right component of the SM fermion field. These two stops mix into mass eigenstates $\tilde{t}_{1}$ and $\tilde{t}_{2}$,  $\tilde{t}_{1}$ being the lighter one. Due to the naturalnes constraint the ligter stop should have mass in TeV range. Morover the Higss mass measurements give condition that $\sqrt{m_{\tilde{t}_{1}} m_{\tilde{t}_{2}}}$ should be around 600~GeV. In this chapter a SUSY model, in which the R-parity is conserved and the LSP is the lightest neutralino~($\tilde{\chi}^{0}_{1}$), is considered. Furthermore only direct stop pair production is assumed.

Depending on the mass difference between stop and neutralino, $\Delta m =  \tilde{\chi}^{0}_{1}$, several decay modes of the stop quark are possible. The stops can decay via two, three or four body decays to final states with b or c quarks. This chapter discribes search for top squark pair production in pp collisions at sqrt(s)=13~TeV using single lepton events~\cite{Sirunyan:2017xse}. As I was also involved in previous versions of this analysis~\cite{Sirunyan:2016jpr, CMS:2016vew}, several differences between the three analyses are briefly discussed, but in general they are largely similar. The analyses were performed in the working group of around twenty to fourty people and therefore in following sections I will especially highlight some chosen contributions from my side.

%-delta M around top mass challenging kinematics -> looks like SM tt -> this region is called stealthy region
%-for t2bW when W becomes on-shell also difficult kinematics

\section{Signal topologies}

The preseneted analysis focuses on the kinematic region where $\Delta m > m_W+m_b$ and on three different decay modes of the stop pair.  All modes lead to states with two b-jets, two W-bosons and two neutralinos, but the kinematics differ depending on the decay mode and $\Delta m$. The $\Delta m$ plane is shown in Fig.~\ref{fig:figures/dmplane}. In the part of the plane where $\Delta m < m_t$, often reffered as ``compressed spectra'' region, the decay producs are soft and often not reconstructed. On the other hand when the $\Delta m$ is large, boosted topologies can be expected. 

    \insertFigure{figures/dmplane} % Filename = label
                 {0.99}       % Width, in fraction of the whole page width
                 { dm plane ~\cite{Aad:2014kra}. }

In this analysis thei targeted final states are with one leptonically and one haronically decaying W-boson, resulting in one charged lepton in final state. The advange of this one lepton channel is its relatively high branching ratio, around 32\% of directly produced stop pairs decay to final states with one lepton, and low occurance of the Standard model backgrounds. 

The first of the three considered decay chains of the stop pair shown in the left part of Fig.~\ref{figures/stopdecays} is reffered as ``T2tt'', where both stops decay to top quark and neutralino, followed by decay of each top quark to W-boson and b-quark:

\eq{t2tt}
{
    \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t \bar{t} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1} \to b \bar{b} W^{+} W^{-} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}.
}

The second possibility depicted in the right part of Fig.~\ref{figures/stopdecays} is reffered as ``T2bW'' and in this case both stop quarks decay via intermediary chargino:

\eq{t2bW}
{
    \tilde{t}_{1} \bar{\tilde{t}}_{1} \to b \bar{b} \tilde{\chi}^{+}_{1} \tilde{\chi}^{-}_{1} \to b \bar{b} W^{+} W^{-} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}.
}

    \insertTwoFigures{figures/stopdecays}
                 {figures/T2tt} % Filename = label
                 {figures/T6bbWW} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{website:SUSYdiagrams}. }

In the T2bW the chargino mass is fixed to halfway between the mass of stop and neutralino. The third decay shown in Fig.~\ref{figures/T4tbW} combines the previous two. In this case, reffered as ``T2tb'' one of the stops decay to top quark and neutralino and the second to bottom quark and chargino.

\eq{t2tb}
{
    \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t b \tilde{\chi}^{0}_{1} \tilde{\chi}^{+}_{1} \to b \bar{b} W^{+} W^{-} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}.
}

    \insertFigure{figures/T4tbW} % Filename = label
                 {0.5}       % Width, in fraction of the whole page width
                 { mixed decay diagram ~\cite{website:SUSYdiagrams}. }

In T2tb the chargino and neutralino are almost mass degenerate, the chargino mass is fixed by relation $m_{\tilde{\chi}_{1}^{\pm}} = m_{\tilde{\chi}_{1}^{0}} + 5~\mathrm{GeV}$. Because of this small difference between chargino and neutralino masses, the W-boson is produced ofshell and its decay producst have low transverse momenta and are probably not reconstructed. If the jets from hadronically decaying W-boson are not reconstructed, only the two b-jets are present in this signal topology, what is being one of the motivation to search for final states with less jets than the four expected.

The analysis strategy introduced in the next section is general for all types of signal topologies. Only for corridior region of T2tt the separately optimized ``corridor'' analysis was designed to target better very challenging signal final state, which is very similar to standard model processes. To be able to combine results of the nominal and corridor analysis, the sensitivity of both analysis for different $\Delta m$ was evaluated and it was decided to use compressed analysis for T2tt signals in $\Delta m$ range between 100 and 225~GeV and the nominal one everywhere else.


\section{Analysis strategy}

Because of the two neutralinos, the signal processes (T2tt, T2bW and T2tb)  can have final state signature largery differing from the background processes coming from Standard Model. This knowledge of the signal and background signature is used to define the baseline search region, in which the SM background is supressed. This baseline search region is then divided to smaller signal regions with help of discriminating variables. These variables are useful to define signal regions where some of the signals are enriched or some fo the backgrounds are more reduced. The remaining backround is estimated from data-driven techniques or simulations for each signal region. In teh baseline search region there are three groups of backgrounds present. 

The leading background is ``Lost lepton'' background mainly coming mainly from $\mathrm{t\bar{t}} \to 2 \ell$ processes, where both W-bosons decay into leptons and one of the charged leptons is lost because of acceptance or other critearia. This background is reduced by applying veto on second lepton, which was misreconstructed, but even after this it remains the largest backround. The subleding background is ``One lepton'' background, which mainly composed of W+jets and $\mathrm{t\bar{t}} \to 1 \ell$ processes in which the W-boson decays leptonically. The last relevant background is denoted ``$Z \to \nu \bar{\nu}$'' and comes from processes such as $t\bar{t}Z$ and $WZ$ in which the Z-boson decays to two neutrinos and one of the W-bosons leptonically.  

The signal and estimated background yields are compared with data in the signal regions and in case that no excess from the SM is observed, the exclusion limits can be put on given signal models. For the purposes of the optimization of analysis and limits setting, the procedure of the hypothesis testing in the LHC searches is briefly described in the section~\ref{sec:stats}. 

In the following subsection~\ref{sec:variables}, the variables designed for definition of baseline search region and its division to signal regions will be introduced and justified on the exaples of signal topologies.  The backgrounds behaviour with regard to the choice of the variables is as aslo discussed. In the end the final definition of the baseline and signal regions for the presented analysis is revealed.

Once the most discriminative variables are identified the object selection, triggers and final design of baseline selection and signal regions are in troduced in subsections~\ref{sec:objects},~\ref{sec:trigger},~\ref{sec:baseline} and ~\ref{sec:sr}. Then in subsection~\ref{sec:estimations} the background estimations in the signal region are discussed, starting from the $Z \to \nu \bar{\nu}$ which was provided by myself. After estimating all relevant backgrounds in subsection~\ref{sec:systematics} the systematic uncertainties on the background estimates as well as on signal yields are identified. Finally the section~\ref{sec:results} summarizes the results of this analysis and provides interpreatation of them. 

%binned approach in variables which tend to reduce signal
%single top?


\subsection{Statistical methods in LHC searches~\label{sec:stats} }

The hypothesis testing and limit setting procedure chosen by the ATLAS and CMS collaborations is modified frequentis method reffered as ``$\mathrm{CL_{s}}$'' method~\cite{Read:2002hq, Junk:1999kv, Cowan:2010js, CMS-NOTE-2011-005}. There are two hypothesis to be tested, the background only hypothesis $H_{0}$ also reffered as ``null hypothesis'' and the signal plus background hypothesis $H_{1}$ usually called ``alernative hypothesis''. These hypothesis are functions of $b+\mu s$, where $b$ and $s$ are the expected background and signal yields and $\mu$ is the signal strength modifier. In case of background only hypothesis $H_{0}$ the signal strenght $\mu$ is zero. The signal and bacground estimates enetering into the hypothesis testing are burdened by meany systematic uncertainties, which are in the $\mathrm{CL_{s}}$ methods therated as nuisance parameters and denoted colectively as $\theta$. Then the expected signal and backgroun yields are depending on these nuisance parameters: $s \to s(\theta)$ and $b \to b(\theta)$.

To compute an \textbf{observed limits} a likelihood function $\mathcal{L}(data|\mu, \theta)$ can be built followingly

\eq{likelihood}
{
\mathcal{L}(data|\mu, \theta) = Poisson(data| \mu s(\theta) +b(\theta)) p(\tilde{\theta}|\theta),
}

where $p(\tilde{\theta}\theta)$ are the pdfs for a nuisicance parameters $\theta$ with $\tilde{\theta}$ being the default value of the nuisance parameter and data for this purpose reffer to the information about observed data and expected signal and background yields. In case of binned Likelihood the term $Poisson(data| \mu s(\theta) +b(\theta))$ can be expressed as


\eq{binnedlikelihood}
{
Poisson(data| \mu s(\theta) +b(\theta)) p(\tilde{\theta}|\theta) = \prod_{i} \frac{\mu s_{i}+b_{i}}{n_{i}!} e^{-\mu s_{i}-b_{i}},
}

which is a product  of Poisson probabilities to observe $n_{i}$ events in bin $i$. Having the Likelihood function, the compatibility of data with $H_{0}$ or $H_{1}$ hypothesis can be tested with help of test statistics $\tilde{q}_{\mu}$ which is in form of the profile likelihood ratio

\eq{pLR}
{
\tilde{q}_{\mu} = -2 \mathrm{ln } \frac{ \mathcal{L}(data| \mu , \hat{\theta}_{\mu}) } {\mathcal{L}(data| \hat{\mu} , \hat{\theta})}, 0 \leq \hat{\mu} \leq \mu ,
}

where $\mu$ is a free parameter, $\hat{\theta}_{\mu}$ are conditional maximum likelihood estimators of parameters $\theta$ given the parameter $\mu$ and measured data. The $\hat{\mu}$ and $\hat{\theta}$ are the parameter estimators at the global mximum of likelihood. The condition $\hat{\mu} \geq 0 $ express that the rate of signal cannot be negative and $\hat{\mu} \leq \mu $ is constraint to consider only one-sided confidence interval 
	  
Once having the masured data, the observed value of profile likelihood ratio~\ref{pLR} $\tilde{q}_{\mu}^{obs}$ can be computed and the maximum lakelihood estimators of $\theta$ best describing the observed data can be evaluated for both $H_{0}$ and $H_{1}$ hypotheses. The $\mathrm{CL_{s}}$  method defines two p-values, the first one is $p_{\mu}$ testing the compatibility of observation with $H_{1}$ and the second testing the compatibility of observation with background only hypothesis~$H_{0}$. The $CL_{s}$ for given signal strenght $\mu$ is then defined as

\eq{cls}
{
CL_{s}(\mu) = \frac{p_{\mu}}{1-p_{b}},
}

where $p_{\mu}$ can be writes as

\eq{pmu}
{
P(\tilde{q}_{\mu} \geq \tilde{q}_{\mu}^{obs}|s+b)
}

and $1-p_{b}$ as

\eq{pb}
{
P(\tilde{q}_{\mu} \geq \tilde{q}_{\mu}^{obs}|b).
}

When $\mu$ being set to one, in case that $\mathrm{CL_{s}} \leq \alpha$ where for exclusion limits $\alpha = 0.05$ the $H_{1}$ is excluded at $1-\alpha$ (=95\%) $\mathrm{CL_{s}}$ confidence level. In the case ofthe  SMS the $\mu$ is not fixed to one and therefore the exclusion limits are set on the cross section times branching ratio of given SMS model. In such procedure the signal strength $\mu$ starts at zero and it is increased up to the point where $\mathrm{CL_{s}}$ = 0.05. If for a given signal point the cross section is limited to a lower value than the theoertical cross section, the signal point is excluded. The contour line is drown at the place where limit on cross section is equal to the theoretical cross section.

The \textbf{expected limits} can be determined by similar method using generated background only pseudo-data instead of observed data. 

Assuming that the test statistics has Gaussian tail  the p-value can be converted to the significance $Z$ with help of convention

\eq{significance}
{
 p = \int_Z^{\infty} \frac{1}{\sqrt{2\pi}} e^{-x^{2}/2} dx,
}

where if considering backgroun only hypothesis, the significance of 5 standard deviations corresponds to $p_{b} = 2.8 \times 10^{-7} $ suggest that the $H_{0}$ hypothesis is excluded at $5 \sigma$ in favour of $H_{1}$ hypothesis. %TODO check that

\subsection{Variables~\label{sec:variables}}

\subsubsection{Number of leptons~($N_{\ell}$)}

The presented search focuses on final states with one charged lepton and therefore requirement on number of leptons to be equal to one must be imposed. The lepton is either electron or muon and the events with tau leptons are rejected.

\subsubsection{Number of jets~($N_{J}$)}

As shown on example in blue on the left of Fig.~\ref{fig:figures/T2ttlep1}, four jets are expected in the final state. But as already discussed, because of the low mass diffference between chargino and neutralino in case of T2tb, the two jets from hadronically decaying W-boson have low \pt and thus they do not have to be reconstructed and only two jets are expected. In case of high $\Delta m$ the decay products of stops can be boosted and thus two or more jets can be merged into one, resulting to reduced number of reconstructed jets in final state. The baseline requirement is to have at least two jets, but this variable is also used for the definition of signal regions targeting different topologies.

%TODO jets PT, eta, jet wp

    \insertTwoFigures{figures/T2ttlep1}
                 {figures/T2ttlepJET} % Filename = label
                 {figures/T2ttlepMET} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{CMS:2016vew}. }

\subsubsection{Number of b-jets~($N_{b}$)}

In the left diagram of Fig.~\ref{fig:figures/T2ttlep1} it can be also noticed that two of the jets in green are originating from the b-quark. For this reason number of b-tagged jets selected by medium working point of CSVv2 are one of the disriminating variables. In the signal regions where W+jets background is dominant, the tighti b-tagging working point is used to further suppress it, as teh jets are expected to be mainly composed of light flavours.

\subsubsection{Missing transverse energy~(MET or $E_{T}^{miss}$)}

Right diagram of Fig.~\ref{figures/T2ttlep1} shows that in the final staes of the signal processes there are two neutralinos and neutrino originating from the leptonicallyd ecaying W-boson. These particles escape detector and cause missing energy in transverse plane, reffered as ``Missing transverse energy''. This variable is very powerful in rejection of SM backgrounds bacouse they tend to have small values of \MET as the sources of the \MET are more limited than in case of signal. Therefore the basline selection requires \MET to be larger than 250~GeV. The distribution of the \MET in the baseline search region for all relevant backrounds and selected signal points is shown in the left plot of Fig.~\ref{fig:figures/METMT}. In this Figure the one lepton background is decomposed to ``1l from top'' composed of $t \bar{t} \to 1\ell$ and ``1l not from top'' populated by W+jets processes. The \MET was also found to be good variable for definiton of signal regions.

Because of the two neutrinos and misreconstrcuted lepton in lost lepton background, the \MET of this background can be very large, which also applies for $Z \to \nu \bar{\nu}$ background, where the source of \MET are three neutrinos.

\subsubsection{Transverse mass of lepton-\MET system~($M_{T}$)}

The \MET in combination with the $M_{T}$ defined as

\eq{MT}
{
 M_{T} = \sqrt{2 p_{T}^{\ell} E_{T}^{miss} (1 - \mathrm{cos}(\phi)) } ,
}

where $p_{T}^{ell}$ is the transverse momentum of lepton and $\phi$ is the angle between momentum of lepton and \MET, ensure dramatic reduction of the SM backround. The $M_{T}$ is  designed to suppress backgrounds where both lepton and \MET (neutrino) come from one W-boson. In such case the $M_{T}$ has an endpoint at W-boson mass. As shown in the left diagram of Fig.~\ref{fig:figures/T2ttlep2} for the signal the leptonically decaying W-boson is not the only source of \MET and therefore it has no endpoint.

    \insertTwoFigures{figures/T2ttlep2}
                 {figures/T2ttlepMT} % Filename = label
                 {figures/T2ttlepDPHI} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{CMS:2016vew}. }

The baseline selection on  $M_{T}$ variable was chosen to be  larger than 150~GeV, which supresses hugely mainly one lepton bacgrounds. The leton and \MET in both  W+jets and $t \bar{t} \to 1\ell$ originate from one W-boson and thus their $M_{T}$ should have endpoint at W-boson mass~($\sim$80~GeV). It can be seen in the right plot of Fig.~\ref{fig:figures/METMT}, that indeeed the bulk of the one lepton events have low $M_{T}$, but there are tails with high $M_{T}$. In case of $t \bar{t} \to 1\ell$ the top quark constraints the kinematics of W-boson and therefore the tail in $M_{T}$  is mainly caused by \MET resolution. The situation is different for W+jets, where the kinematics permits offshel production of W-boson and consequesntly $M_{W^{*}}> 80$~GeV. For the lost lepton and $Z \to \nu \bar{\nu}$ backgrounds there is no endpoint as the \MET does not originate from one W-boson.

    \insertTwoFigures{figures/METMT}
                 {figures/LogMET2j} % Filename = label
                 {figures/LogMT2j} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{website:stopSupp}. }

\subsubsection{Minimal azimuthal angle between direction and  one of the two leading jets and \MET~(min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ )}

In the backgrounds where the neutrino is only source of the \MET it is probable that the neutrino is close to the b-quark  originating from the same top decay. This mainly appears for $t\bar{t} \to 1\ell$ process as shown in the left plot of Fig.~\ref{fig:figures/DPHIMLB}. In case of the signal, as depicted in the right diagram of Fig.~\ref{fig:figures/T2ttlep2}, there are more sources of \MET and therefore there is no constraint on this variable. The chosen baseline requirement on min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ to be larger than 0.8 is considerably reducing the SM backgrounds but leads to the relatively small reduction of the signal.

    \insertTwoFigures{figures/DPHIMLB}
                 {figures/LogMDPhi2j} % Filename = label
                 {figures/LogMlb2j} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{website:stopSupp}. }

\subsubsection{Invariant mass of the reconstructed lepton and the closest b-quark~($M_{\ell b}$)}

In case that lepton and b-jet come from one top as displayed in Fig.~\ref{fig:figures/T2ttlepMlb} , there is a bound on the $M_{\ell b}$ variable wich is

\eq{Mlb}
{
 M_{t} \sqrt{1 - \frac{M_{W}^{2}}{M_{t}^{2}} } \approx 153~\mathrm{GeV} ,
}

where $M_{W}$ is the mass of the top quark and $M_{W}$ the W-boson mass. This limit is true for backgrounds coming from $t\bar{t}$ decay, but also for T2tt signal. On the other hand there is no limit for W+jets backgrounds and T2bW signal. Therefore this variable is not suitable for baseline region definition, as puting a constraint on it would considerably reduce the signal but it can be used to define signal regions with enriched one kind of signal and supressed one type of the background. The distribution of  $M_{\ell b}$ for all relevant signals and different backgrounds in the baseline search region is shown in Fig.~\ref{fig:figures/DPHIMLB}, where it can be clearly noticed that low $M_{\ell b}$ part of distribution si more lost lepton and T2tt-like, while high $M_{\ell b}$ is mainly populated by W+jets and T2bW-like signal.

    \insertFigure{figures/T2ttlepMlb}
                 {0.5}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{CMS:2016vew}. }

\subsubsection{Modified topness~($t_{mod}$)}

The topness is $\chi^{2}$-like type of variable developed to supress background coming from $t \bar{t} \to 2\ell$ processes, where one lepton in misreconstructed. It aim is to discriminate how well an even agrees with $t \bar{t} \to 2\ell$ hypothesis. It was discovered that removing several terms from topness variable, improves the signal disrimination in this analysis. The new variable called ``modified topness''~($t_{mod}$) is defined as


\eq{tmod}
{
 t_{mod} = \mathrm{ln(\mathrm{min}S)},~where~S(\vec{p}_{W}, p_{\nu, z} ) = \frac{(m_{W}^{2}- (p_{\nu}+p_{\ell})^2 )^2 }{a_{W}^{4}} + \frac{(m_{t}^{2}- (p_{b_{2}}+p_{W})^2 )^2 }{a_{t}^{4}},
}

with $m_{W}$ and  $m_{t}$ is the mass of W-boson and top quark, ${p}_{W}$, ${p}_{\nu}$, ${p}_{\ell}$, ${p}_{b_{2}}$ are momenta of W-boson, neutrino, lepton and b-jet. The parameters $a_{W} =5$~GeV and $a_{t}=15$~GeV are the resolution parameters. The first term in Eq.~\ref{eq:tmod} aims to reconstruct mass of the W-boson which lepton was reconstructed and the purpose of second term is reconstruct the top mass of the leg, where lepton was lost. There are more options how to chose  b-jet. Several options were studied for case of this analysis following procedure was chosen. The modified topness is computed for three jets with highest CSVv2 discriminator. Then the jet which lead to the smallest modified topness is chosen.

Different backgrounds and signal models with different $\Delta m$ populates $t_{mod}$ distribution shown in Fig.~\ref{fig:figures/Logtmod2j} diferently, therefore this variable is valuable for definition of signal regiions.

    \insertFigure{figures/Logtmod2j}
                 {0.5}       % Width, in fraction of the whole page width
                 { decay diagrams ~\cite{website:stopSupp}. }

\subsubsection{$M_{T2}^{W}$}

The purpose of $M_{T2}^{W}$ is similar as of modified topness, to reduce the lost lepton background. This variably simillarly tries to reconstruct event under $t \bar{t} \to \ell \ell$ with one lost lepton hypothesis. The $M_{T2}^{W}$ has an endpoint at top mass for $t\bar{t}$ events with lost lepton. It would be possible to use this variable for baseline selection and requiring high values of $M_{T2}^{W}$, but it was found out that signals with low $\Delta m$ also lead to small $M_{T2}^{W}$. In the past versions of analysis the $M_{T2}^{W}$ was used for definition of signal regions. The $M_{T2}^{W}$ has very similar behavior as $t_{mod}$ and with more statistics collected it was found out that $t_{mod}$ is more discrimininat in full analysis phase-space than $M_{T2}^{W}$ and therefore in current analysis $M_{T2}^{W}$ is not used anymore.

\subsubsection{Number of W-tags~($N_{J}$)}

In case of high  $\Delta m$ signals the top quarks are expected to be significantly boosted. As in the SM the top quark is the heaviest particle, similar source of boost of top quarks is not present within SM and therefore tagging of boosted objects could help the signal discrimination. As show in Fig.~\ref{fig:figures/boostedTopologies} when the momentum of hadronic top is low, three resolved ak4 jets are observed. With growing boost of the top guark, the jets originating from W-boson or all three jets of the top quark can be merged into one fat jet. The merged jets can be taged by special W- or top-tagging techniques.

    \insertFigure{figures/boostedTopologies}
                 {0.99}       % Width, in fraction of the whole page width
                 { W, top tag }


Part of my contribution to the presented stop searches was study of W-tagging techniques and evaluation of its benefits for the stop search. The study was performed based on results of single lepton stop analysis~\cite{Sirunyan:2016jpr} which is using data collected at center-of-mass energy of 13~TeV in 2015 which correspond to integrated luminosity of 2.3~fb$^-1$. This analysis defines four kinds of signal regions targeting signals with different kinematics. In case of this study we focused only on region groups with high $\Delta m $ reffered as ``High $\Delta m$'' and ``Boosted High $\Delta m$'' defined in Table~\ref{tab:SRnoW}. This study redifines the signal regions with use of number of W-tagged jets~($N_{W}$) instead of \MET, but keeps the number of the signal regions the same. The proposed signal regions are shown in Table~\ref{tab:SRW}.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Name            & $N_{J}$  & $M_{T2}^{W}$~[GeV] & $E_{T}^{miss}$~[GeV]  \\
\hline
\hline
                & $\geq$4  & >200                & 250<$E_{T}^{miss}$<350   \\
High $\Delta m$ & $\geq$4  & >200                & 350<$E_{T}^{miss}$<450   \\
                & $\geq$4  & >200                & $E_{T}^{miss}$>450   \\
\hline
Boosted High $\Delta m$ & 3  & >200                & 250<$E_{T}^{miss}$<350   \\
                        & 3  & >200                & $E_{T}^{miss}$>350   \\
\hline
\end{tabular}
\caption[Table caption text]{2015 default sr definitions. }
\label{tab:SRnoW}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Name            & $N_{J}$  & $M_{T2}^{W}$~[GeV]            & $E_{T}^{miss}$~[GeV]    & $N_{W}$ \\
\hline
\hline
                          & $\geq$4  & >200                & 250<$E_{T}^{miss}$<350   & --   \\
High $\Delta m$ W-tagging & $\geq$4  & >200                & $E_{T}^{miss}$>350       & 0    \\
                          & $\geq$4  & >200                & $E_{T}^{miss}$>350       & 1    \\
\hline
Boosted High $\Delta m$ W-tagging & 3  & >200              & $E_{T}^{miss}$>250 & 0  \\
                                  & 3  & >200              & $E_{T}^{miss}$>250 & 1  \\
\hline
\end{tabular}
\caption[Table caption text]{2015 default sr definitions. }
\label{tab:SRW}
\end{center}
\end{table}


The standard jets used in analysis are clusterized by ak4 algorithm with cone size of 0.4. The boosted W-jets are expected to be ``fat'' and therefore they need to be reclustered with different cone size. In the study two possibilities were exploited, ak8 jets with cone size of 0.8 and ak10 jets with cone size of 1.0. The merged W-jet should have large transverse momentum and mass around the mass of W-boson. To discriminate if the jet is merged jet originating from a W-boson, a requirement on its mass can be posed. The raw mass  of the jet is contaminated by the initial state radiation, underlying events and pile-up and tehrefore it must be cleand from these contributions. Several ``grooming techniques'' of the jet mass were developed by CMS, but in this study only pruned mass is used, as it was recomended technique for the merged W-jets. The pruning procedure~\cite{Ellis:2009su} rejects large angle and soft constituents during reclustering iterations. Another variable used in W-tagging is called N-subjettiness~($\tau_{N}$) which aim is to disriminate how the jet is consistent with N-jet hypothesis. The N-subjettiness ratio $\tau_{21} = \tau_{2}/\tau{1}$ has high disriminating power for merged W-jets and therefore in this study the requirement on jet $\tau_{21}$ is used to tag merged W-jets. Taking into acount the discriminating variables, two definitions of W-tagged jets reffered as ``ak8-W'' and ``ak10-W'' defined in Table~\ref{tab:Wtags} are established.



\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Name            & Clustering algorithm &      pruned mass $m_{W,p}$ [GeV]  &        $\tau_{21}$  & \pt [GeV]  \\
\hline
\hline
ak8-W  &        ak8                     &   60<$m_{W,p}$<100               & <0.5   & >200  \\
\hline
ak10-W  &        ak10                     &   60<$m_{W,p}$<100               & <0.5   & >200  \\
\hline
\end{tabular}
\caption[Table caption text]{ W-tag definition. }
\label{tab:Wtags}
\end{center}
\end{table}

To evaluate the benefits of the W-tagging  the expected significance in the default signal regions in Table.~\ref{tab:SRnoW} was compared to expected significance in proposedi W-tagging signal regions in Table.~\ref{tab:SRnoW}. The significance described in section~\ref{sec:stats} is expected significance when using generated pseudatata with signal strengt of one instead of observed data. For this comparison all relevant backgrounds and three different T2tt signal points with different stop and neutralino mass $(m_{\tilde{t}_{1}}, m_{\tilde{\chi}^{0}_{1}})$ were used. Thsese signal points are (900,1), (800,300) and (650,450). In this study the significances for two different luminosity options of 2.3~fb$^{-1}$ and 10~fb$^{-1}$, two different options of W-tagging defined in Table~\ref{tab:Wtags} and different combinations of default and W-tagged signal regions were computed. The best performing combination of signal regions was found to be combination of ``High $\Delta m$ W-tagging'' with ``High $\Delta m$'' regions as shown in Table~\ref{tab:taggingResults}. In this table the significances of the best performing combination of signal regions for both ak8-W and ak10-W is shown together with significances of default signal regions for three signal points and integrated luminosity of 2.3~$fb^{-1}$. It can be noticed, that using ak10 jets leads to larger significance than ak8 jets. Although there is an increase in the significance up to around 20~\% in some cases, the increase is not that large to support usage of complicated and time consuming W-taggers which have gain only for high $\Delta m$ signals. Also in this study no systematic uncertainties were evaluated. But when studying the significances for different luminosities it was found that with growing luminosity the gain in significance when using W-tagging is inreasing and therefore the W-tagging is interesting option for analyses with larger integrated luminosity. 

%Table results
\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Signal point             & --      & ak8-W & ak10-W \\
\hline
                       & High $\Delta m$ +           &  High $\Delta m$ W-tagging +  &   High $\Delta m$ W-tagging +  \\
                       &  Boosted High $\Delta m$    & Boosted High $\Delta m$       &   Boosted High $\Delta m$  \\
\hline
\hline
(900,1) &     2.68 & 2.63 & 3.43  \\
\hline
(800,300) &   3.58 & 3.76 & 4.61  \\
\hline
(650,450) &   0.38 & 0.39 & 0.44  \\
\hline
\end{tabular}
\caption[Table caption text]{tagging results for 2.3fb. }
\label{tab:taggingResults}
\end{center}
\end{table}


%-expected significance, reject backfround hypothesis

%leptonic diagram~\cite{CMS:2016vew}
%MT2W - formula; explanation; supress lost lepton (already suppressed by requiring no additional lepton, but not enough); tries to reconstruct the event under tt2l and one undetected lepton assumption; for signal large delta M leads to alrge MT2W, while small delta M has lage MT2W; endpoint at top mass
%tmod - formula; chi2 like variable how well the event agrees with tt2l hypothesis, similar behavior to Mt2W; removing some of the terms from oifficial topness helps the discrimination; works better at low jet multiplicities than MT2W

\subsection{Triggers, data and simulated samples~\label{sec:trigger}}

In this analysis several kinds of triggers are used for different purposes. The single lepton data are triggered with single lepton and \MET triggers, where the \MET triggers help to select data with lower \pt leptons which would not pass the single lepton trigger. In the analysis OR of these triggers is used. The data triggered by the double lepton trigger serve for check of kinematics of lost lepton background. Similarly the data triggered by single photon trigger are used for cross-check of \MET resolution in one lepton background.

The SM background samples are generated by leading-order or next-to-leading-order generators and processed by full simulation of CMS. The signal samples are produced by leading-order generators and injected into fast simulation to simulate the resaponse of CMS detecotr to signal events.

\subsection{Objects and event selection~\label{sec:objects}}

The presented analysis selects events with single lepton, jets, b-jets and \MET in final state. On the other hand the events which in final state have tau leptons or second ``veto'' lepton passing looser lepton criterium are vetoed. To select or reject such events, the obejcts definition and criteria on its selection are introduced.

\textbf{Primary vertex}

The primary interaction vertex in the event must passed the good quality criteria and it is identified as a vertex which leads to the largest sum \pt$^{2}$s of physics objects belonging to this vertex.

\textbf{Lepton}

The ``selected'' lepton for this analysis is a large \pt electron or muon isolated from other activity in detector. The selected lepton must originate from primary vertex and have \pt>20~GeV and $|\eta|<1.4442$ for electron and $|\eta|<2.4$ for muon. This analysis also defines second ``veto'' lepton which fails isolation criteria, \pt thershold or which pass looser identification than the selected one. 

\textbf{Jets and b-jets}

The jets are reconstructed by the anti-kt algorithm with cone radius parameter of 0.4. The jet \pt is required to be higher than 30~Gev and itn must be in range $|\eta|<2.4$. The medium b-tagging working point is used for signal regions where the $M_{\ell b}$ is small and tight working point in those with large $M_{\ell b}$.  

\textbf{Veto on isolated tracks}

Majority of the tau leptons decay into one charged track which is predominantly charged hadron. Therefore the events with taus can be supressed by vetoing those, which ahve one isolated charged track.

\textbf{Veto on hadronic tau}

This additional veto helps to veto the taus which pass isolated track sleection. It mainly helps to reduce the lost lepton background.

\subsection{Baseline search region selection~\label{sec:baseline}}

After evaluating which kinematic variables help to supress SM background while keeping high signal yields and identifing criteria on selected and vetoed objects the baseline search region is defined followingly:

\begin{itemize}

\item Good primary vertex
\item One selected electron or muon
\item No veto lepton, isolated track or hadronic tau
\item At least two jets
\item At least on b-jet passing medium or tight, if $M_{\ell b} \leq$175~GeV or $M_{\ell b}>$175~GeV, respectively
\item \MET>250~GeV
\item $M_{T}$>150~GeV
\item $M_{T}$>150~GeV
\item min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ > 0.8

\end{itemize}

In the compressed T2tt region the top quarks are prodiced off-shell, limiting the available momentum,  and therefore their decay products have very low \pt consequently leading to low \MET. Obviously, such kind of the signal has different kinematics and the baseline selection must be adjusted to account for it. To increase the \MET in the event the stop pair must be boosted against and initial state radiation~(ISR), leading to the requirement of additional jet in the event. The ISR jet should have the largest \pt, but such requirement does not help overall signal discrimination as \MET>250~GeV is already required. The jet with the highest \pt must fail medimum b-tagging working point in order to increase the probability that it comes from ISR. The stop system should be boosted in opposite direcion to the ISR and tus the direction of objects originating from stop pair decay, like \MET and lepton should have similar direction. As the \pt of objects in compressed region is not large puting a limit on lepton \pt was found to be useful in signal discrimination. Taking these considerations into account in the corridor region the following additional criteria on the baseline search region  are imposed

\begin{itemize}
\item Five jets
\item Lepton \pt<150~GeV
\item Leading jet fails b-tagging medium working point
\item $\Delta \phi(E_{T}^{miss}, \ell)$<2.0 
\item min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ is loosened to be larger than  0.5

\end{itemize}

\subsection{Signal regions}

In order to increase the sensitivity of the analysis to the signal, the baseline search region can be divided into signal regions with help of discriminating variables defined in secton~\ref{sec:variables}. For the nominal search there are 8 signal region groups denoted A-H, leading to 27 exclusive signal regions. Additional group I of four signal regions is created for the T2tt compressed spectrum with 100<$\Delta m$<225~GeV. The definition of all signal regions is shown in Table~\ref{tab:SR}.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|lll|l|}
\hline
Group  &  $N_{J}$  & $t_{mod}$    &  $M_{\ell b}$ [GeV]     & \MET [GeV]                       \\
\hline
A      &  2-3      &   >10        &  $\leq$175              &  250-350, ~350-450, ~450-600, ~>600  \\
B      &  2-3      &   >10        &       >175              &  250-450, ~450-600, ~>600  \\
C      &  $\geq$4  &   $\leq$0    &  $\leq$175              &  250-350, ~350-450, ~450-550, ~550-650, ~>650  \\
D      &  $\geq$4  &   $\leq$0    &       >175              &  250-350, ~350-450, ~450-550, ~>550  \\
E      &  $\geq$4  &   0-10       &  $\leq$175              &  250-350, ~350-550, ~>550  \\
F      &  $\geq$4  &   0-10       &       >175              &  250-450, ~>450  \\
G      &  $\geq$4  &   >10        &  $\leq$175              &  250-350, ~350-450, ~450-600, ~>600  \\
H      &  $\geq$4  &   >10        &       >175              &  250-450, ~>450  \\
\hline
I      &  $\geq$5  &   --         &   --                    &  250-350, ~350-450, ~450-550, ~>550  \\
\hline
\end{tabular}
\caption[Table caption text]{SRs A-I. }
\label{tab:SR}
\end{center}
\end{table}

In the previous versions of analysis there is lower number of the signal regions, due to smaller available statistics of data. Also as discussed before, the $M_{T2}^{W}$ variable was used instead of $t_{mod}$ for definition of signal regions with three and more jets. In presented analysis a new disriminating avriable $M_{\ell b}$ was added and the signal regions were re-optimized with respect to $N_{J}$ and \MET .



%\subsection{Undefined}
%-SF, rewighting

%\subsection{compressed spectra}
%	ISR jets
%	recoil, enhance MET

\section{Background estimation}

-backgrounds 
	what type of
-how they look like, how they can be distingusehed from signal
	tt2l - lost lepton does not obey MT<MW
	W+jets - offshel Ws - no endpoint at W mass
	Znunu no bound Mt<MW

-data driven + simulation
-general idea
-MET extrapolation?
-transfer factors
-scale factors - take into account differences in lepton, b-tagging etc efficiency
-composition of backgrouns - percentage

\subsection{Z to nunu}

-ttZ, WZ, ZZ
-2015 dataset taken from simulation -> small stats
-large in high MET and MT2W regions

-2017
-ZZ still taken from simulation
-3l scale factor -> normalization

\subsection{Single lepton}

-sensitive to MET and MT
-W+jets, tt1l
- tt1l -> constrained kinematics of W due to top, so MT tail is just due to MET resolution
-W+jets, no kinematics constraint, MT tail due to of shell W production (W width) 
-W+jets, zero b-tag control region
-tt1l -> negligible, estimation from simulation

-1 lep neutrino from W decay

ESTIMATIN:
-data driven, similar as for the lost lepton
-CR contamination N CR,data,Wjets,0btag = N CR,data,0btag - N CR,MC,ninWjets,0btag
-MET and NJET extrapolation (2015)
-W+b modelling systematics

\subsection{Lost lepton}

-dominant background after MET and MT selections
-tt2l wit both Ws decaying leptonically and one lepton lost
-largest contribution is the tt2l, then single top and then ttV and diboson processes.
-dilepton control region
-misreconstruction of lepton - 1 lepton and large MET
-two enutrinos lare MT and MT2W

ESTIMATON:
- N data,SR (ll) / N data,CR (ll) = N MC,SR (ll) / N MC,CR(ll) -> get N data,SR
-MET and Njet extrapolation (2015) -> additional transfer factor
-good modeling of njet and MET needed -> mismodelling in ssimu leads to mismodelling in TF -> special emu CR -> additional SF due to mismodelling
-gamma plus jets -> MET resolution > bin migrations - gamma pt spectrum reveighted to match neutrino pt spectrum, from reweighted events METmodified = reconstructed-MET + pTgamma


\subsection{Systematic uncertainties}

\section{Results and interpretation}

\section{Differences in previous analyses}

\section{Selected topics}

\subsection{Perspectives: W/top-tagging ?}

%\textbf{Motivation}

%	high delta M regim - boost -> jets merge

%\textbf{Techniques}
%	larger radius jets
%	tau ratios - N subjetiness
%	different masses-> grooming techniques
	
%\textbf{Results}
%	end of 2015 - slide 8,9 - results on different ak8 W-tagging categories (18/11); lumi=2 1/fb
%	mid of 2016 -update of previous study - lumi=2.26 fb; ak8+ak10 tagging; tables slide 7, 11 -> mo improvement with ak8, but slight improvement with ak10 


%\textbf{Perspectives}
%	-results from Sicheng with top tagging
%	-resolved top tagger - 3ak4 jets
%	-second presentation (31/01 same as 12 feb?), slide 6 - interesting plot, slide 10,11 significance table
%        -merged top tagger - boosted objects
%		slide 4, 5, 6

%\subsection{Depndence of discriminating variables on pileup}
%-not much to say
	
