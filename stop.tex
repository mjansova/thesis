\clearpage

\setcounter{secnumdepth}{4}
\chapterwithnum{Search for top squark pair production in pp collisions at $\sqrt{s}$=13~TeV in Run 2 using single lepton events~\label{sec:stopch}}
\setcounter{secnumdepth}{4}

%\section{Introduction and motivation}

In the previous chapter, we have introduced  supersymmetry as the most popular extension of the standard model, due to its capability to address many shortcomings of the SM. Among others, it provides a natural solution to the hierarchy problem and a dark matter candidate. Within supersymmetry each SM particle has a SUSY partner with the same quantum numbers except the spin which is differing by one half. This chapter focuses on the direct pair production of the lightest SUSY partner of the top quark, referred to as the top squark or stop~($\tilde{t}_{1}$), this $\tilde{t}_{1}$ being in fact a mass eigenstate  from the mixing of the two scalar stops $\tilde{t}_{R}$ and  $\tilde{t}_{L}$ as previously described in Section~\ref{sec:MSSM}.

%there are a left and a right components of the SM fermion fields. These two stops mix into mass eigenstates $\tilde{t}_{1}$ and $\tilde{t}_{2}$ labeled by increasing mass. 

In this chapter, SMS scenarios in which the R-parity is conserved are considered, leading to  the stops being produced in pair. The LSP being the lightest neutralino $\tilde{\chi}^{0}_{1}$. As we will see later, directly produced stop pair decays into two b quarks, two W bosons and two neutralinos. The neutralinos interact only weakly and escape the detector unmeasured. Although undetected particles can be spotted by the presence of the missing transverse energy, which can be especially large in case of final states with more neutralino(s) and neutrino(s). Therefore SUSY signal can have an unique experimental signature compared to the standard model processes.

This chapter describes the search for direct top squark pair production in pp collisions at $\sqrt{s}$=13~TeV using single lepton events~\cite{Sirunyan:2017xse} corresponding to an integrated luminosity of 35.9~fb$^{-1}$ collected during 2016. This analysis is further referred to as the ``full 2016 analysis''.  I also worked on the previous versions of this analysis~\cite{Sirunyan:2016jpr, CMS:2016vew}, based first on data from 2015 corresponding to an integrated luminosity of 2.3~fb$^{-1}$ and referred to as ``2015 analysis'' and then on data from the beginning of 2016 with an integrated luminosity of 12.9~fb$^{-1}$ referred to as the ``ICHEP2016 analysis'' as it was presented at the ICHEP conference in 2016. Several differences between the three versions are briefly discussed, but in general the strategy of the analyses remains similar. In the following sections I especially highlight some chosen personal contributions.

In the Section~\ref{sec:stopologies} the signal topologies targeted by full 2016 analysis are presented. The following Section~\ref{sec:strategy} defines the strategy of the analysis.  Then in Section~\ref{sec:estimations} the background estimation techniques in the search region are discussed, starting from the estimation of background referred as ``$Z \to \nu \bar{\nu}$'' which I provided during my PhD thesis. Within the subsections on the relevant backgrounds, the systematic uncertainties on the background estimates are identified. Finally Section~\ref{sec:results} summarizes the results of this analysis and provides their interpretation. 



%In this chapter only a SUSY model, in which the R-parity is conserved leading to the LSP being the lightest neutralino~($\tilde{\chi}^{0}_{1}$) and the stops being produced  in pair, is considered. Depending on the mass difference between the stop and the neutralino $\Delta m = m_{\tilde{t}_{1}} - m_{\tilde{\chi}^{0}_{1}}$, several decay modes of the stops are possible. The stops can decay via two, three or four body decays to final states with b- or c-quarks. In case of three (four) body decays the top quark (top quark and W boson) in the decay chain are produced off-shell. 



%-delta M around top mass challenging kinematics -> looks like SM tt -> this region is called stealthy region
%-for t2bW when W becomes on-shell also difficult kinematics

\section{Signal topologies~\label{sec:stopologies}}

As the lighter stop $\tilde{t}_{1}$ is expected in natural SUSY to be lighter than gluino~\cite{Martin:2008aw}, the stop pair can be produced either directly as shown in Fig.~\ref{fig:figures/T2tt} or via the gluino mediated production (see Fig.~\ref{fig:figures/T5tttt}). Only the first process is considered in this thesis, the later being studied in e.g.~\cite{Sirunyan:2017pjw, Sirunyan:2017kqq}.

   \insertFigure{figures/T5tttt} % Filename = label
                {0.45}       % Width, in fraction of the whole page width
                { An example of diagram of gluino mediated stop production. In this case gluinos are produced in pairs and both decay to a top quark and a top squark. Both stops decay to a top quark and an LSP. } % Caption

There are different possible decay modes for the stop depending on the mass difference between the stop and the neutralino $\Delta m = m_{\tilde{t}} -  m_{\tilde{\chi}_{1}^{0}}$. The stops can decay via two, three or four body decays leading to final states with b or c quarks. In this thesis three decay modes are considered for the stop pair in the kinematical region $\Delta m > m_W+m_b$.

The first of the three considered decay chains of the stop pair is shown in the left part of Fig.~\ref{fig:figures/stopdecays} and referred to as ``T2tt'', in which both stops decay to a top quark and a neutralino, followed by the decay of each top quark to a W boson and a b quark:

\eq{t2tt}
{
    \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t\tilde{\chi}^{0}_{1}  \bar{t} \tilde{\chi}^{0}_{1} \to b W^{+} \tilde{\chi}^{0}_{1} \bar{b} W^{-} \tilde{\chi}^{0}_{1} .
%\to b \bar{b} \ell \nu_{\ell} q \bar{q} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}.
}
Note that if $\Delta m < m_{t}$, then the decay of the stop is a three-body decay, with the top quark being off-shell. 
 
    \insertTwoFigures{figures/stopdecays}
                 {figures/T2tt} % Filename = label
                 {figures/T6bbWW} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { The SMS diagrams for the direct stop pair production. In the left diagram each stop decays to $t  \tilde{\chi}^{0}_{1}$, while in the right one they decay to $ b \tilde{\chi}^{\pm}_{1} $~\cite{website:SUSYdiagrams}. }

The second possibility depicted in the right part of Fig.~\ref{fig:figures/stopdecays} is referred to as ``T2bW'' and in this case both stop quarks decay via an intermediate chargino which decays as $\tilde{\chi}_{1}^{\pm} \to W^{\pm} \tilde{\chi}^{0}_{1} $:

\eq{t2bW}
{
    \tilde{t}_{1} \bar{\tilde{t}}_{1} \to b \tilde{\chi}^{+}_{1} \bar{b} \tilde{\chi}^{-}_{1} \to b W^{+} \tilde{\chi}^{0}_{1} \bar{b}  W^{-} \tilde{\chi}^{0}_{1}.
%\to b \ell \nu_{\ell} \tilde{\chi}^{0}_{1} \bar{b}  q \bar{q} \tilde{\chi}^{0}_{1}.
}
The chargino mass $m_{\tilde{\chi}^{\pm}_{1}}$ is fixed by the relation 

\eq{charginomass}
{
m_{\tilde{\chi}_{1}^{\pm}} = x ~m_{\tilde{t}_{1}} + (1-x) ~m_{\tilde{\chi}^{0}_{1}},
}
where  $m_{\tilde{t}_{1}}$ is the stop mass, $m_{\tilde{\chi}^{0}_{1}}$ the neutralino mass and $x$ is a fixed fraction between 0 and 1, which is for CMS searches usually chosen to be 0.25, 0.5 or 0.75 at Run~1. At Run~2, $x$ was chosen to be equal to 0.5.


The third decay, shown in Fig.~\ref{fig:figures/T4tbW} and referred as ``T2tb'' combines the previous two decay modes of the stop: one of the stops decays to a top quark and a neutralino, while other one decays to a bottom quark and a chargino:

\eq{t2tb}
{
    \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t \tilde{\chi}^{0}_{1}  b \tilde{\chi}^{+}_{1} \to b W^{+} \tilde{\chi}^{0}_{1} W^{-} \bar{b} \tilde{\chi}^{0}_{1}.
%\to b \bar{b} \ell \nu_{\ell} q \bar{q} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1} .
}
In the T2tb model, the chargino and neutralino are taken as almost mass degenerate, the chargino mass is fixed by relation $m_{\tilde{\chi}_{1}^{\pm}} = m_{\tilde{\chi}_{1}^{0}} + 5~\mathrm{GeV}$. Such model is motivated by the low mass difference between the chargino and neutralino obtained in the case when bino and wino masses are larger than the parameter $\mu$~\cite{Gunion:1987yh}. In addition, the results of the thesis~\cite{Duarte:2017fkm} show that if $\mu = 200$~GeV, $\tan \beta =10$ and the bino mass is set to 3~TeV, the mass difference between the lightest chargino and neutralino is lower than 5~GeV, depending on the mass of the wino. In our case, because of this small difference between the chargino and neutralino masses, the W boson is produced off-shell, its decay products have low transverse momenta and are typically not selected or even reconstructed. 

    \insertFigure{figures/T4tbW} % Filename = label
                 {0.5}       % Width, in fraction of the whole page width
                 { The SMS diagram for the direct stop pair production considering the mixed decay of the stop pair $t b \tilde{\chi}^{0}_{1} \tilde{\chi}^{+}_{1}$~\cite{website:SUSYdiagrams}. }


All decay modes considered by this analysis lead to states with two b quarks, two W bosons and two neutralinos, the targeted final states having one leptonically and one hadronically decaying W boson, resulting in one charged lepton~($1\ell$) in the final state: $ \tilde{t}_{1} \bar{\tilde{t}}_{1} \to b \bar{b} \ell \nu_{\ell} q \bar{q} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}$. The advantage of this $1 \ell$ channel is the low occurrence of the standard model backgrounds compared to the full hadronic search, and its relatively high branching ratio compared to the dilepton channel. The stop pair decays in around 32\% of cases to the final states with one prompt lepton ($e,\mu$) or leptonically decaying tau lepton. 
%The analysis focuses on the kinematic region where $\Delta m > m_W+m_b$ and on three different decay modes of the stop pair.  

The final state kinematics differ depending on the decay mode and $\Delta m$. The $\Delta m$ plane is shown in Fig.~\ref{fig:figures/dmplane}. In the part of the plane where $\Delta m < m_t$, often referred to as the ``compressed spectra'' region, the decay products are softer and subsequently not selected. On the other hand when the $\Delta m$ is large, boosted topologies can be expected. 


    \insertFigure{figures/dmplane} % Filename = label
                 {0.99}       % Width, in fraction of the whole page width
                 { The plane of stop ($\tilde{t}_{1}$) versus LSP~($\tilde{\chi}^{0}_{1}$) masses~\cite{Aad:2014kra}. }


\section{Analysis strategy~\label{sec:strategy}}

Because of the two neutralinos, the signal processes (T2tt, T2bW and T2tb)  can have final state signatures largely differing from the SM background ones. The knowledge about the signal and background signatures is used to define a ``baseline region'' of the search, in which the SM backgrounds are suppressed. 

In the baseline search region there are three groups of backgrounds present. The leading background is referred to as the ``lost lepton'' background which has a final state with two W bosons, both decaying into leptons and one of the charged leptons being lost due to acceptance or a problem of reconstruction or identification. This background, mainly coming from dileptonic decay of the top pair production ($\mathrm{t\bar{t}} \to 2 \ell$) and single top quark process,  is reduced by applying a veto on the presence of a second lepton passing looser criteria, but even after this it remains the largest background. The sub-leading background is the ``one lepton'' ($1\ell$) background, which is predominantly composed of associated W boson plus jets production  ($W+jets \to 1\ell$) and top pair production ($t\bar{t} \to 1 \ell$) with one W bosons decaying leptonically. The last relevant background is denoted as ``$Z \to \nu \bar{\nu}$'' and comes from processes such as $t\bar{t}Z$ and $WZ$ in which the Z~boson decays to two neutrinos and one of the W bosons decays leptonically.  

Starting from this baseline region, tighter selections are applied  with the help of discriminating variables, to define smaller signal regions. The goal is to define regions in which the signal to background ratio is enhanced either by an enrichment in signal, or by a high suppression of the backgrounds. Different signals are expected to populate differently these signal regions, allowing to discriminate between the signal scenarios in case of discovery. The remaining backgrounds in each signal region is estimated from data-driven techniques or simulations. 

Hypothesis tests are performed on the signal, estimated background and observed data yields in all signal regions. In case of no observed excess from the SM expectation, the results are interpreted in term of exclusion limits on a given signal models. For the purpose of the analysis optimization and limit setting, the procedure of hypothesis testing in the LHC searches is briefly described in the Section~\ref{sec:stats}. Then the triggers and objects used in this analysis are introduced in Sections~\ref{sec:trigger}~and~\ref{sec:objects}.
 
In the following Section~\ref{sec:variables}, the variables designed to define the baseline search region and its categorization in signal regions are introduced and justified on the examples of signal topologies. The backgrounds behavior with regard to the choice of the variables is discussed as well. Once the most discriminating variables are identified, the final definition of the baseline and signal regions is presented in Section \ref{sec:baseline} and \ref{sec:sr}. 

The introduced analysis strategy is general for all types of signal topologies. Only for compressed region of T2tt a separately optimized ``compressed'' analysis was designed to better target a very challenging signal final state scenario. To be able to combine results of the nominal and compressed analyses, the sensitivity of both analysis for different $\Delta m$ was evaluated and it was decided to use compressed analysis for T2tt signals in $\Delta m$ range between 100 and 225~GeV and the nominal one everywhere else. In part of this range not only purely compressed (off-shell) decays are present, but also the top quark can be produced on-shell.

%binned approach in variables which tend to reduce signal
%single top?


\subsection{Hypothesis testing in LHC searches~\label{sec:stats} }

The hypothesis testing and limit setting procedure chosen by the CMS collaboration is a modified frequentist method referred to as the ``$\mathrm{CL_{s}}$'' method~\cite{Read:2002hq, CMS-NOTE-2011-005}. There are two hypotheses to be tested, the background only hypothesis $H_{0}$ also referred to as the ``null hypothesis'' and the signal plus background hypothesis $H_{1}$ usually called as the ``alternative hypothesis''. These hypotheses are functions of $b+\mu s$, where $b$ and $s$ are the expected background and signal yields and $\mu$ is the signal strength modifier. In the case of the background only hypothesis $H_{0}$, the signal strength $\mu$ is zero. The signal and background estimates entering into the hypothesis testing are burdened by many systematic uncertainties, which are  treated as nuisance parameters and collectively denoted as $\theta$. The expected signal and background yields are dependent on these nuisance parameters: $s \to s(\theta)$ and $b \to b(\theta)$.

\textbf{Observed limits}

To compute the observed limits, a likelihood function $\mathcal{L}(data|\mu, \theta)$ can be built as follows~\cite{CMS-NOTE-2011-005}

\eq{likelihood}
{
\mathcal{L}(data|\mu, \theta) = \operatorname{Poisson}(data| \mu s(\theta) +b(\theta))~p(\tilde{\theta}|\theta),
}
where $p(\tilde{\theta}|\theta)$ are the probability distribution functions for the nuisance parameters $\theta$ with $\tilde{\theta}$ being the default value of the nuisance parameters, $data$ for this purpose refers to the information about the observed data yield or generated pseudo-data. In case of a binned likelihood, the term $\operatorname{Poisson}(data| \mu s(\theta) +b(\theta))$ can be expressed as


\eq{binnedlikelihood}
{
\operatorname{Poisson}(data| \mu s(\theta) +b(\theta))  = \prod_{i} \frac{\mu s_{i}+b_{i}}{n_{i}!} e^{-\mu s_{i}-b_{i}},
}
which is a product  of Poisson probabilities to observe $n_{i}$ data events in the bin $i$. Given the likelihood function, the compatibility of $data$ with the $H_{0}$ or $H_{1}$ hypothesis can be tested with the help of the test statistics $\tilde{q}_{\mu}$ which has the form of a profile likelihood ratio

\eq{pLR}
{
\tilde{q}_{\mu} = -2 \mathrm{ln } \frac{ \mathcal{L}(data| \mu , \hat{\theta}_{\mu}) } {\mathcal{L}(data| \hat{\mu} ,~\hat{\theta})},
}
where the signal strength $\mu$ is a free parameter, $\hat{\theta}_{\mu}$ are the conditional maximum likelihood estimators of the parameters $\theta$ given the parameter $\mu$ and the measured data. The $\hat{\mu}$ and $\hat{\theta}$ are the parameter estimators at the global maximum of the likelihood. 
%The condition $\hat{\mu} \geq 0 $ expresses that the rate of signal cannot be negative and $\hat{\mu} \leq \mu $ is constraint to consider only one-sided confidence interval.
	  
The observed value $\tilde{q}_{\mu}^{obs}$ of the profile likelihood ratio expressed in Eq.~\ref{eq:pLR} can be computed using the measured data and the maximum likelihood estimators of $\theta$ which best describe the observed data, it can be evaluated for both the $H_{0}$ and $H_{1}$ hypotheses. The $\mathrm{CL_{s}}$  method defines two p-values, the first one is $p_{\mu}$ testing the compatibility of the observation with $H_{1}$ and the second one, $1-p_{b}$, testing the compatibility of the observation with the background-only hypothesis~$H_{0}$. The $\mathrm{CL_{s}}$ for a given signal strength $\mu$ is then defined as

\eq{cls}
{
\mathrm{CL_{s}}(\mu) = \frac{p_{\mu}}{1-p_{b}},
}
where $p_{\mu}$ can be written as

\eq{pmu}
{
p_{\mu} = P(\tilde{q}_{\mu} \geq \tilde{q}_{\mu}^{obs}|s+b)
}
and $1-p_{b}$ as

\eq{pb}
{
1-p_{b} = P(\tilde{q}_{\mu} \geq \tilde{q}_{\mu}^{obs}|b).
}

For $\mu=1$, the $H_{1}$ is excluded at ($1-\alpha$) $\mathrm{CL_{s}}$ confidence level if $\mathrm{CL_{s}} \leq \alpha$. Typically, the exclusion limits are derived for $\alpha = 0.05$. In the case that the $\mu$ parameter is not fixed to one, the exclusion limits are set on the cross section given a fixed branching fraction of a given signal model. In such procedure the signal strength $\mu$ is adjusted in order to correspond to the $\mathrm{CL_{s}}$ = 95\%. If for a given signal point the cross section is limited to a lower value than the theoretical cross section, the signal point is excluded. A contour line is drawn at the place where the limit on the cross section is equal to the theoretical cross section.

\textbf{Expected limits}

The expected limits can be determined via a similar method using generated background-only pseudo-data instead of observed data. 

Thanks to the relation  

\eq{significance}
{
 p = \int_Z^{\infty} \frac{1}{\sqrt{2\pi}} e^{-x^{2}/2} dx.
}
the p-value computed from the test statistics can be converted to the significance $Z$ with $Z\sigma$ denoting the significance in terms of the standard deviation $\sigma$ of a Gaussian distribution. If considering the background-only hypothesis, the significance of 5 standard deviations corresponding to $p_{b} = 2.8 \times 10^{-7} $ suggests that a $H_{0}$ hypothesis is excluded at $5 \sigma$ in favor of the $H_{1}$ hypothesis, therefore the discovery is claimed. %TODO check that


\subsection{Triggers, data and simulated samples~\label{sec:trigger}}

In the full 2016 analysis several kinds of triggers are used:  \MET, single lepton, $e-\mu$ double lepton  and single photon triggers. As the online thresholds for the single lepton triggers are higher than the cut applied offline for the lepton, the use of the \MET triggers allows to select also events with a lower \pt lepton. To guarantee a high efficiency,  an ``OR'' of these triggers is used in the analysis. The trigger requirements are given in Table~\ref{tab:triggerobj}. The trigger efficiency in the search regions of interest is greater than 99\%. The data triggered by the double lepton trigger are used for checking the kinematics of the lost lepton background. Similarly the data triggered by the single photon trigger are used to cross-check the \MET resolution in  the $1\ell$ background. 

The SM background samples are generated by leading-order~(LO) or next-to-leading-order~(NLO) generators and processed by the full simulation of CMS. The LO samples generated by MADGRAPH\_aMC@NLO 2.2.2~\cite{Alwall:2014hca} generator configured to the LO with MLM matching~\cite{Alwall:2007fs} using the LO NNPDF3.0~\cite{Ball:2014uwa} parton distribution functions are for the  $t\bar{t}$, W+jets, $t\bar{t}Z$ and $\gamma$+jets processes. The single top samples are generated by the NLO POWHEG 2.0~\cite{Alioli:2010xd} generator. The MADGRAPH\_aMC@NLO 2.2.2 in the NLO mode with FxFx~\cite{Frederix:2012ps} matching and the NLO NNPDF3.0 parton distribution functions is used to generate the majority of the rare processes, such as $WZ$ and $ZZ$. The full hadronization, parton showering and modeling of the underlying events are then performed by PYTHIA 8.205~\cite{Sjostrand:2014zea}.

The description of the initial state radiation (ISR) jets in the $t\bar{t}$ and $t\bar{t}Z$ samples is improved by reweighting these samples based on the number of the ISR jets ($N_{ISR}$) in order that the jet multiplicity agrees between data and simulation. Measurements for a $N_{ISR}$ between one and six were done and the data-to-simulation ratio was found to be in the range between 0.92 and 0.51. The systematic uncertainty on this factor is taken to be half the deviation from the unity.

The signal samples are produced by the MADGRAPH\_aMC@NLO 2.2.2 generator at the LO and injected into the fast simulation of the CMS detector.  The same $N_{ISR}$ reweighting procedure as discussed above is applied on the signal samples. 

\begin{table}[h]
\begin{center}
   \begin{tabular}{| l | l |}
      \hline
      \MET trigger & \MET$>120\GeV$ and $H_{T}^{miss}=\abs{\sum(\ptvec^{\text{\,jets}})+\ptvec^{\mathrm{^{\text{\,lep}}}}}>120\GeV$ \\
      Single lepton trigger                         & isolated electron (muon):       $p_{T}^{\mathrm{lep}} > 25~(22) \GeV$, $|\eta| < 2.1~(2.4)$ \\
      \hline
      Selected lepton \rule{0pt}{4ex}   & electron (muon): $p_{T}^{\mathrm{lep}} > 20 \GeV$, $|\eta| <1.442~(2.4)$, medium ID \\
      Selected lepton isolation & $p_{T}^{\mathrm{sum}} < 0.1 \times
      p_{T}^{\mathrm{lep}}$,  $\Delta R = \min[0.2,\max(0.05, 10 \GeV / p_{T}^{\mathrm{lep}})]$ \\
      Jets and b-tagged jets \rule{0pt}{4ex}   & $p_{T} > 30 \GeV$, $|\eta| < 2.4$ \\
      \ \ \ b tagging efficiency & medium (tight) WP: 60-70 (35-50)\% for 30<jet \pt<400~$\GeV$ \\
      \ \ \ b tagging mistag rate & medium (tight) WP : $\sim1\%$ ($\sim0.2\%$) for light-flavor partons\\
      Veto lepton \rule{0pt}{4ex}   & muon or electron with $p_{T}^{\mathrm{lep}} > 5 \GeV$, $|\eta| <2.4$ \\
      \ \ \ Veto lepton isolation & $p_{T}^{\mathrm{sum}} < 0.1 \times p_{T}^{\mathrm{lep}}$,  $\Delta R = \min[0.2,\max(0.05, 10 \GeV / p_{T}^{\mathrm{lep}})]$ \\
      Veto track \rule{0pt}{4ex}   & charged particle-flow candidate, $p_{T} > 10 \GeV$, $|\eta| <2.4$ \\
      \ \ \ Veto track isolation & $p_{T}^{\mathrm{sum}} < \min\left(0.1 \times p_{T}^{\mathrm{lep}}, \mathrm{6} \GeV\right)$, $\Delta R = 0.3$ \\
      Veto hadronic tau \rule{0pt}{4ex}   & tau ID, \pt$ > 120 \GeV$, $|\eta| <2.4$ \\
      \ \ \ Veto hadronic tau isolation & $ \Delta R = 0.4$ \\
      \hline
    \end{tabular}

\caption[Table caption text]{ The triggers and the object definition. $p_{T}^{\mathrm{lep}}$ is the \pt of the lepton and the $p_{T}^{\mathrm{sum}}$ is the scalar sum of the \pt of all PF candidates which are located in a cone around the lepton, but excluding the lepton itself. $p_{T}^{\mathrm{sum}}$ for the selected and veto lepton is computed from the charged and neutral PF particles, while for the veto track only charged particles are used. The label ``light flavor'' denotes that the jet originates from u, d or s quarks or gluons~\cite{Sirunyan:2016jpr}. }
\label{tab:triggerobj}
\end{center}
\end{table}




%TODO start here

\subsection{Object definition~\label{sec:objects}}


In this section the object definition and the criteria on their selection for the full 2016 analysis are introduced.

\begin{description}
\item[Primary vertex]
The primary vertices in the event must pass good quality criteria. The primary vertex corresponding to the hard interaction is identified as the vertex which leads to the largest sum of $p_{T}^{2}$ of the physics objects belonging to this vertex.

\item[Lepton]
The ``selected'' lepton for this analysis is a large \pt electron or muon isolated from any activity in the detector. The selected lepton must originate from the primary vertex, both electron and muon have \pt>20~GeV, but a different $|\eta|$ coverage: $|\eta|<1.4442$ for the electron and $|\eta|<2.4$ for the muon. Within this acceptance criteria the selection efficiency is more than 85\% for electrons and 95\% for muons. This analysis also defines a second category of lepton, called ``veto'' lepton, which passes looser  \pt threshold, isolation criteria, or identification than the selected one. A common isolation definition used for the majority of the SUSY analyses, differs from the standard CMS one. It is defined as a function of the sum of the \pt of PF candidates which are within a cone around the lepton. The isolation cone size parameter depends on the \pt of the lepton and the cone size parameter  starts at 0.2 for $p_{T}^{\mathrm{lep}}$<50~GeV and then changes to 10.0/$p_{T}^{\mathrm{lep}}$ for  50<~$p_{T}^{\mathrm{lep}}$~<200~GeV. For $p_{T}^{\mathrm{lep}}$>200~GeV the cone size parameter is of 0.05. This procedure ensures, that in the case of boosted topologies when lepton and jet can be close to each other, the cone size decreases in order not to be affected by the jet fragments in the proximity of the lepton.   

\item[Jets and b-tagged jets]
The jets are reconstructed by the anti-kt~\cite{Cacciari:2008gp} algorithm with a cone distance parameter of 0.4. The jet \pt is required to be higher than 30~GeV and the jet must be in the range $|\eta|<2.4$. Either a medium or a tight working point~(WP) of the b-tagging CSVv2 algorithm is used for the definition of the signal regions.  

\item[Veto on isolated tracks]
The majority of hadronically decaying tau leptons decay into one charged track which is predominantly a charged hadron. Therefore the events with taus can be suppressed by vetoing events which have one isolated charged track.

\item[Veto on hadronic tau]
This additional tau veto, using tau identification criteria, helps to veto the taus which do not pass the isolated track selection. It mainly helps to reduce the lost lepton background.

\item[Missing transverse energy]
The missing transverse momentum referred as ``missing transverse energy'' is used as defined in Section~\ref{sec:MET}. Following the CMS recommendations, several additional \MET filters, e.g. filters for bad or duplicate muons, are applied.   
\end{description}

The summary of the object selection criteria is presented in Table~\ref{tab:triggerobj}.
%The presented analysis selects events with one single lepton, at least two jets one being b-tagged, and \MET in the final state. On the other hand the events which have in the final state hadronically decaying tau leptons or a second ``veto'' lepton passing looser lepton criterion are vetoed. 

\subsection{Search variables~\label{sec:variables}}

Discriminating variables are built from the objects defined in previous section and their properties, kinematics or multiplicities. The variables considered for the presented stop analyses to define the baseline search region (see Section~\ref{sec:baseline}) as well as the signal regions (Section~\ref{sec:sr}) are introduced in the following subsections.

\subsubsection{Number of leptons~($N_{\ell}$)}

The presented search focuses on final states with one charged lepton and therefore a requirement on the number of leptons to be equal to one must be imposed. The lepton is either an electron or a muon and the events with tau leptons are rejected. No additional lepton from the veto lepton category is allowed.

\subsubsection{Number of jets~($N_{J}$)}

As shown in blue in the example in Fig.~\ref{fig:figures/T2ttV}, at least four jets are expected in the considered signal final states. But because of the low mass difference between the chargino and the neutralino in the T2tb model, the two jets from the hadronically decaying W boson have low \pt and thus they may not be selected, therefore only two jets are expected. In the case of a high $\Delta m$ value, the decay products of the stops can be boosted and thus two or more jets can be merged into one, resulting to a  reduced number of selected jets in final state. The baseline selection requires to have at least two jets, but this variable is also later used for the definition of signal regions targeting different topologies.

%If the jets from hadronically decaying W boson are not selected, only the two b-jets are present in the final state of this signal topology, what is being one of the motivation to search for final states with less jets than expected four.
%TODO jets PT, eta, jet wp

    \insertFigure{figures/T2ttV}
                 {0.7}       % Width, in fraction of the whole page width
                 { Diagram of the T2tt signal model supporting the choice of $N_{J}$ (blue), \MET (red),  $M_{T}$ (magenta), min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ (brown) and $M_{\ell b}$ (orange) as discriminating variables~\cite{CMS:2016vew}. }

\subsubsection{Number of b-jets~($N_{b}$)}

In the diagram of Fig.~\ref{fig:figures/T2ttV} it can also be noticed that two jets in green are originating from b quarks. For this reason the number of selected b-tagged jets is one of the discriminating variables, which is in the baseline search region  chosen to be larger or equal to one. In the signal regions where the $W+jets \to 1\ell$  background is dominant, the tight b-tagging working point is used to further suppress it, as the jets are expected to be mainly composed of light flavors.

\subsubsection{Missing transverse energy~($E_{T}^{miss}$)}

The red circles in Fig.~\ref{fig:figures/T2ttV} show that there are two neutralinos and one neutrino originating from the leptonically decaying W boson  in the final states of the signal processes. These particles escape from the detector and lead to missing momentum in the transverse plane. \MET is very powerful in rejecting the SM backgrounds because they tend to have smaller values of \MET as the sources of \MET are more limited than in  the case of the signal processes. However, because of the presence of two neutrinos and one lost lepton in the lost lepton background, the \MET value of this background can be very large. This also applies for the processes of the $Z \to \nu \bar{\nu}$ background, where the source of \MET are three neutrinos.

Therefore the baseline selection requires \MET to be larger than 250~GeV. The \MET distribution in the baseline search region for all relevant backgrounds and three selected signal scenarios is shown in the left plot of Fig.~\ref{fig:figures/METMT}. The $1\ell$ background is decomposed into two processes which are later referred to as ``1$\ell$ from top'' populated mainly by the $t \bar{t} \to 1\ell$ events and ``1l not from top'' populated by the $W+jets \to 1\ell$  process. The \MET was, as expected, also found to be a good variable for the definition of the signal regions.


\subsubsection{Transverse mass of the lepton-\MET system~($M_{T}$)}

The $M_{T}$ variable is defined as

\eq{MT}
{
 M_{T} = \sqrt{2 p_{T}^{\mathrm{lep}} E_{T}^{miss} (1 - \mathrm{cos}(\phi)) } ,
}
where $p_{T}^{\mathrm{lep}}$ is the transverse momentum of the lepton and $\phi$ is the angle between the lepton and the direction of \MET in the transverse plane. The combination of a selection on the $M_{T}$ and \MET ensures a drastic reduction of the SM background. The $M_{T}$ variable is  designed to suppress backgrounds where both the lepton and \MET come from one W~boson. In that case the \MET corresponds to the neutrino of this W boson and the the $M_{T}$ has an endpoint at the W boson mass~($\sim$80~GeV). As shown in the diagram of Fig.~\ref{fig:figures/T2ttV}, the leptonically decaying W boson is not the only source of the \MET and therefore no endpoint is expected for the signal.


The baseline search region requires events with $M_{T}$ larger than 150~GeV, suppressing mainly the $1 \ell$ background, as in that case the lepton and \MET in both the  $W+jets \to 1\ell$  and $t \bar{t} \to 1\ell$ processes originate from one W boson.  It can be seen in the right plot of Fig.~\ref{fig:figures/METMT}, that indeed the bulk of the one lepton events have a low $M_{T}$ value, but a tail survives at high $M_{T}$. For the $t \bar{t} \to 1\ell$ process, the top quark constraints the kinematics of the W boson and therefore the tail in $M_{T}$  is mainly caused by the \MET resolution. The situation is different for the $W+jets \to 1\ell$ background, where the kinematics permits the off-shell production of W boson and consequently $m_{W^{*}}> 80$~GeV. For the lost lepton and $Z \to \nu \bar{\nu}$ backgrounds, there is no endpoint as the \MET does not originate from one W boson.

    \insertTwoFigures{figures/METMT}
                 {figures/LogMET2j} % Filename = label
                 {figures/LogMT2j} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { Data (black dots), background (full colored histograms) and signal (dashed lines) distributions of the \MET (left) and $M_{T}$ (right) variables after applying the baseline selection. The cut on $M_{T}$ is not applied in the the right plot, but only indicated by the black arrow~\cite{website:stopSupp}. }

\subsubsection{Minimal azimuthal angle between the direction of one of the two leading jets and the \MET~(min$\Delta \phi (j_{1,2}, E_{T}^{miss})$)}

For the background events where a neutrino is the only source of \MET, it is probable that this neutrino is close to the b quark  originating from the same top decay. This appears for $t\bar{t} \to 1\ell$ process as shown in the left plot of Fig.~\ref{fig:figures/DPHIMLB}. In the case of the signal, as depicted in magenta in the diagram of Fig.~\ref{fig:figures/T2ttV}, there are more sources of the \MET and therefore there is no constraint on min$\Delta \phi (j_{1,2}, E_{T}^{miss})$. The chosen baseline requirement on the min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ to be larger than 0.8 was found to considerably reduce the suggested $t\bar{t} \to 1\ell$ process but also the $t\bar{t} \to 2\ell$  background while leading to a relatively small reduction of the signal.

    \insertTwoFigures{figures/DPHIMLB}
                 {figures/LogMDPhi2j} % Filename = label
                 {figures/LogMlb2j} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { Data (black dots), background (full colored histograms) and signal (dashed lines) distributions of the min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ (left) and $M_{\ell b}$ (right) variables after applying the baseline selection. The cut on min$\Delta \phi (j_{1,2}, E_{T}^{miss})$  is not applied in the the right plot, but only indicated by the black arrow~\cite{website:stopSupp}. }

\subsubsection{Invariant mass of the system composed by the selected lepton and the closest b-tagged jet~($M_{\ell b}$)}

When a lepton and a b-jet originate from a one top quark as displayed in Fig.~\ref{fig:figures/T2ttV}, there is a bound on the $M_{\ell b}$ variable computed as an invariant mass of the system composed by the selected lepton and the closest b-tagged jet, which is

\eq{Mlb}
{
 m_{t} \sqrt{1 - \frac{m_{W}^{2}}{m_{t}^{2}} } \approx 153~\mathrm{GeV} ,
}
where $m_{t}$ is the mass of the top quark and $m_{W}$ the W boson mass. This limit is true for backgrounds coming from $t\bar{t}$ decays, but also for the T2tt signal. On the other hand there is no limit for the $W+jets \to 1\ell$  background and the T2bW signal. Therefore this variable is not suitable for the baseline region definition, as putting a constraint on it could considerably reduce one of the targeted signals but it can be used later to define signal regions which discriminate among the signal scenarios as well as suppress part of the backgrounds. The distribution of the  $M_{\ell b}$ variable for all relevant backgrounds and three different signals is shown in Fig.~\ref{fig:figures/DPHIMLB} after the baseline selection. It can be clearly noticed that the low $M_{\ell b}$ part of distribution is dominated by the lost lepton background and is more sensitive to the T2tt signal, while in  the high $M_{\ell b}$ part the relative fraction of backgrounds coming from the $W+jets \to 1\ell$ process is increased and the T2bW  signal shows a higher yield than the other signals.

\subsubsection{Modified topness~($t_{mod}$)}

The topness variable~\cite{Graesser:2012qy} is a $\chi^{2}$-like variable developed to suppress the background coming from the $t \bar{t} \to 2\ell$ process with one lost lepton. Its aim is to evaluate how well an event agrees with the $t \bar{t} \to 2\ell$ hypothesis, given that one lepton is lost. It was discovered that removing several terms from the topness variable, improves the signal discrimination in this analysis. The new variable called ``modified topness''~($t_{mod}$) is obtained by minimization of variable $S$ and is defined as 


\eq{tmod}
{
 t_{mod} = \mathrm{ln(\mathrm{min}S)},~\mathrm{where}~S(\vec{p}_{W}, p_{\nu, z} ) = \frac{(m_{W}^{2}- (p_{\nu}+p_{\ell})^2 )^2 }{a_{W}^{4}} + \frac{(m_{t}^{2}- (p_{b_{1}}+p_{W})^2 )^2 }{a_{t}^{4}},
}
with $m_{W}$ and  $m_{t}$ being the masses of the W boson and the top quark, ${p}_{W}$, ${p}_{\nu}$, ${p}_{\ell}$, ${p}_{b_{1}}$ being the momenta of the W boson supposed to be fully invisible due to the lost lepton, the neutrino, the selected lepton and the b-jet respectively. The parameters $a_{W} =5$~GeV and $a_{t}=15$~GeV are the resolution parameters. The first term in Eq.~\ref{eq:tmod} aims to reconstruct the mass of the W boson whose lepton was selected. The purpose of the second term is to reconstruct the top mass of leg, for which the lepton was lost. There are different options how to chose the  b-jet in the event. For this analysis the following procedure was chosen: The modified topness is computed for the three jets having the highest CSVv2 discriminator values, and then the jet which leads to the smallest modified topness is chosen.

Different backgrounds and signal models with different $\Delta m$ populate the $t_{mod}$ distribution shown in Fig.~\ref{fig:figures/Logtmod2j} differently, therefore this variable is valuable for the definition of signal regions. It can be noticed that the $t_{mod}$ variable has a double peak shape. The left peak is populated by the events where the objects are correctly identified to originate from the given W boson and top quark, while in the one on the right at least one a object is wrongly was taken for the computation of the modified topness.

    \insertFigure{figures/Logtmod2j}
                 {0.5}       % Width, in fraction of the whole page width
                 { Data (black dots), background (full colored histograms) and signal (dashed lines) distributions of the $t_{mod}$  variable after applying the baseline selection~\cite{website:stopSupp}. }

\subsubsection{An alternative variable discriminating against the lost lepton background~($M_{T2}^{W}$)~\label{sec:mt2w}}

The purpose of the $M_{T2}^{W}$ variable is similar to that of the modified topness, i.e. to reduce the lost lepton background. This variable similarly tries to reconstruct the event under the $t \bar{t} \to \ell \ell$ hypothesis with one lost lepton. Its definition is

\begin{equation}
\begin{split}
M_{T2}^{W} = \mathrm{min}\{m_{y},~\mathrm{consistent~with:}~[p_{1}^{2}=0,~(p_{1}+p_{\ell})^{2} = p_{2}^{2} =m_{W}^{2},~ \vec{p}_{1,T}+\vec{p}_{2,T}= \vec{E}_{T}^{miss} \\
(p_{1} + p_{\ell} + p_{b_{1}})^{2} =  (p_{2} + p_{b_{1}})^{2} = m_{y}^{2}]\},
\end{split}
\end{equation}
where $m_{y}$ is the fitted mass of the mother particle and the different $p_{i}$ represent the four-momenta of the neutrino ($p_{1}$) from the W boson whose lepton is selected, the invisible W boson ($p_{2}$), the two b-jets ($p_{b_{1}}$ and $p_{b_{2}}$) and the selected lepton $p_{\ell}$, respectively. The constraint on the mass $m_{W}$ of W boson is used. The b-jets $b_{1}$ and $b_{2}$ are selected, from three jets with the highest CSVv2 discriminator, as the jets which lead to the lowest $M_{T2}^{W}$.

Following the definition of $M_{T2}^{W}$,  an endpoint around the top mass is expected for the $t\bar{t}$ events with a lost lepton. It would be possible to use this variable for the baseline selection by requiring a high value of $M_{T2}^{W}$, but it was found out that signals with low $\Delta m$ also lead to small $M_{T2}^{W}$. In the 2015 and ICHEP2016 analyses~\cite{Sirunyan:2016jpr, CMS:2016vew}, the $M_{T2}^{W}$ variable was used in the definition of signal regions. It shows a  very similar behavior as the $t_{mod}$ variable and with more statistics collected it was found out that the $t_{mod}$ is more discriminating in the full analysis phase-space than the $M_{T2}^{W}$ variable. Therefore in the current analysis the $M_{T2}^{W}$ variable is not used anymore.


\subsubsection{Number of W-tagged jets~($N_{W}$)}

For signals with high  $\Delta m$, the top quarks originating from the stop decays are expected to be significantly boosted. The boost of the top quarks within the SM is less likely and therefore tagging of boosted objects could help the signal discrimination. As shown in Fig.~\ref{fig:figures/boostedTopologies} when the momentum of the hadronically decaying top quark is low, three resolved anti-kt jets with cone distance parameter of 0.4  are observed. With a growing boost of the top quark, the jets originating from the W boson or all three jets of the top quark can be merged into one ``fat'' jet. The merged jets can be tagged by special W-  or top-tagging techniques.

    \insertFigure{figures/boostedTopologies}
                 {0.99}       % Width, in fraction of the whole page width
                 { A schema of the jet configuration of a hadronically decaying top quark  depending on its boost. }


I worked on the evaluation of the potential increase in sensitivity that W-tagging techniques could bring to the stop analysis. The study is performed in the context of the 2015 single lepton stop analysis~\cite{Sirunyan:2016jpr} using data collected in 2015 at center-of-mass energy of 13~TeV  and corresponding to an integrated luminosity of 2.3~fb$^{-1}$. 


The standard jets used in the analysis are clustered by the ak4 algorithm. The boosted W jets are expected to be fat and therefore need to be re-clustered with a different cone distance parameter. In the study two possibilities were exploited, ak8 jets with a cone distance parameter  of 0.8 and ak10 jets with a cone distance parameter of 1.0. The merged W jet should have a large transverse momentum and a mass around the mass of W boson. Therefore a requirement on the jet mass helps to identify a merged jet originating from a W boson. The raw mass of the jet is contaminated by initial state radiations, underlying events and pileup and therefore it must be cleaned from these contributions. Several ``grooming techniques'' of the jet mass are exploited by CMS collaboration, but in this study only the ``pruned mass'' was used, as it was the technique recommended by CMS for the W-tagging~\cite{website:Wtagging}. The pruning procedure~\cite{Ellis:2009su} rejects large angle and soft constituents during the re-clustering iterations. Another variable used in W-tagging is called the N-subjettiness~($\tau_{N}$) which aims to discriminate how much the jet is consistent with the N-jet hypothesis. For example the merged jet of a W boson should contain two jets and therefore be consistent with the 2-jet hypothesis, but  not the with 3-jet one. The N-subjettiness ratio $\tau_{21} = \tau_{2}/\tau_{1}$ has a high discriminating power for merged W jets and therefore in this study a requirement on $\tau_{21}$ was used to tag merged W-jets. Typical distributions of the pruned mass and  $\tau_{21}$ are shown in Fig.~\ref{fig:figures/subjet} taken from~\cite{Khachatryan:2014vla}. In the plots it can be noticed that  W+jets events mostly contain unmerged W bosons and therefore they have a low pruned mass and lead to a broad $\tau_{21}$ distribution. The fraction of the pruned jets around 80~GeV, therefore merged W-jets, is enhanced in the case of $t\bar{t}$ events, leading to lower values of $\tau_{21}$. Taking into account the discriminating variables, I design two selections of the W-tagged jets referred to as ``ak8-W'' and ``ak10-W'', presented in Table~\ref{tab:Wtags} and used in the following.


    \insertFigure{figures/subjet}
                 {0.9}       % Width, in fraction of the whole page width
                 {Data and simulation distribution of the jet pruned mass (left) and the N-subjettiness ratio $\tau_{21}$ (right) for W+jets events. The data-to-simulation ratios are shown at the bottom of the plots~\cite{Khachatryan:2014vla}.}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Name            & Clustering algorithm &      pruned mass $m_{W,p}$ [GeV]  &        $\tau_{21}$  & \pt [GeV]  \\
\hline
\hline
ak8-W  &        ak8                     &   60<$m_{W,p}$<100               & <0.5   & >200  \\
\hline
ak10-W  &        ak10                     &   60<$m_{W,p}$<100               & <0.5   & >200  \\
\hline
\end{tabular}
\caption[Table caption text]{ The definition of the ``ak8-W'' tagged and ``ak10-W'' tagged jet. }
\label{tab:Wtags}
\end{center}
\end{table}


The 2015 analysis defined four categories of signal regions targeting signals with different kinematics. In this study, I focus only on the region groups addressing the high $\Delta m $ phase space and referred to as ``High $\Delta m$'' and ``Boosted High $\Delta m$'' in Table~\ref{tab:SRnoW}. I redefine the signal regions based on the number of W-tagged jets~($N_{W}$) instead of \MET, but kept the number of the signal regions the same. The new proposed signal regions are shown in Table~\ref{tab:SRW}.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Name            & $N_{J}$  & $M_{T2}^{W}$~[GeV] & $E_{T}^{miss}$~[GeV]  \\
\hline
\hline
                & $\geq$4  & >200                & 250<$E_{T}^{miss}$<350   \\
High $\Delta m$ & $\geq$4  & >200                & 350<$E_{T}^{miss}$<450   \\
                & $\geq$4  & >200                & $E_{T}^{miss}$>450   \\
\hline
Boosted High $\Delta m$ & =3  & >200                & 250<$E_{T}^{miss}$<350   \\
                        & =3  & >200                & $E_{T}^{miss}$>350   \\
\hline
\end{tabular}
\caption[Table caption text]{ The signal regions of the 2015 analysis~\cite{Sirunyan:2016jpr}. }
\label{tab:SRnoW}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Name            & $N_{J}$  & $M_{T2}^{W}$~[GeV]            & $E_{T}^{miss}$~[GeV]    & $N_{W}$ \\
\hline
\hline
                          & $\geq$4  & >200                & 250<$E_{T}^{miss}$<350   & --   \\
High $\Delta m$ W-tagging & $\geq$4  & >200                & $E_{T}^{miss}$>350       & =0    \\
                          & $\geq$4  & >200                & $E_{T}^{miss}$>350       & =1    \\
\hline
Boosted High $\Delta m$ W-tagging & =3  & >200              & $E_{T}^{miss}$>250 & =0  \\
                                  & =3  & >200              & $E_{T}^{miss}$>250 & =1  \\
\hline
\end{tabular}
\caption[Table caption text]{ Proposed signal regions using the W-tagged jets multiplicity. }
\label{tab:SRW}
\end{center}
\end{table}

To evaluate the benefits of the W-tagging, the expected significances in the default signal regions of Table~\ref{tab:SRnoW} are compared to the expected significances of the proposed W-tagging signal regions in Table.~\ref{tab:SRW}. The significance, described in Section~\ref{sec:stats}, is the expected significance when using generated pseudo-data with a signal strength of one, instead of the observed data. For this comparison all relevant backgrounds as well as three different T2tt signal points with different stop and neutralino masses $(m_{\tilde{t}_{1}},~m_{\tilde{\chi}^{0}_{1}})$ are used. These signal point masses in GeV are (900,~1), (800,~300) and (650,~450). In this study the significances are computed for two different luminosity options (2.3~fb$^{-1}$ and 10~fb$^{-1}$) and the two different selections of W-tagging definition in Table~\ref{tab:Wtags}. The default signal regions and the newly proposed W-tagged signal regions are differently combined, the best performing combination of the signal regions is  the combination of the ``High $\Delta m$ W-tagging'' category with the ``Boosted High $\Delta m$'' category.

In Table~\ref{tab:taggingResults}, the significances for both ak8-W and ak10-W are shown together with the significances of the default signal regions for three signal points and an integrated luminosity of 2.3~fb$^{-1}$. It can be noticed, that using ak10-W jets leads to a larger significance than ak8-W jets. Although there is an increase in the significance up to around 20~\% in some cases, the increase is not large enough to support the usage of complicated W-tagger, highly consuming in computing resources and providing a significant gain only for high $\Delta m$ signals.  It is important to note, that compared to what is at the end achieved with the full analysis, many simplifications are done in this study, an uncorrelated systematic uncertainty of 30\% is used in each SR and per process, and the background yields are taken directly from simulated samples. Moreover it would be necessary to compute the W-tagging efficiency and eventually  derive scale factors correcting differences between data and simulation.  Therefore also a large effort would be needed to provide data-driven estimations of the backgrounds and the corresponding systematics. 

When studying the significances for an integrated luminosity of  10~fb$^{-1}$,  it has been found that the gain in significance raises up to around 30\% when using the W-tagging and therefore the W-tagging can be an interesting option for analyses based on data with larger integrated luminosity. 

%Table results
\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
$\mathcal{L} =$ 2.3~fb$^{-1}$             & ak4~\cite{Sirunyan:2017xse}      & ak8-W & ak10-W \\
\hline
Signal points                       & High $\Delta m$ +           &  High $\Delta m$ W-tagging +  &   High $\Delta m$ W-tagging +  \\
$(m_{\tilde{t}_{1}},~m_{\tilde{\chi}^{0}_{1}})$    &  Boosted High $\Delta m$    & Boosted High $\Delta m$       &   Boosted High $\Delta m$  \\
\hline
\hline
(900,1) &     1.43 & 1.35 & 1.70  \\
\hline
(800,300) &   1.93 & 1.96 & 2.34  \\
\hline
(650,450) &   0.21 & 0.21 & 0.23  \\
\hline
\end{tabular}
\caption[Table caption text]{ Significances for three different T2tt signal points (first column) using the default SRs (second column) and then a mix of SRs defined by using ``ak8-W'' (third column) and ``ak10-W'' (fourth column) tagged jet multiplicity for an integrated luminosity of 2.3~fb$^{-1}$. }
\label{tab:taggingResults}
\end{center}
\end{table}

%(900,1) &     2.68 & 2.63 & 3.43  \\
%\hline
%(800,300) &   3.58 & 3.76 & 4.61  \\
%\hline
%(650,450) &   0.38 & 0.39 & 0.44  \\

%-expected significance, reject backfround hypothesis

%leptonic diagram~\cite{CMS:2016vew}
%MT2W - formula; explanation; supress lost lepton (already suppressed by requiring no additional lepton, but not enough); tries to reconstruct the event under tt2l and one undetected lepton assumption; for signal large delta M leads to alrge MT2W, while small delta M has lage MT2W; endpoint at top mass
%tmod - formula; chi2 like variable how well the event agrees with tt2l hypothesis, similar behavior to Mt2W; removing some of the terms from oifficial topness helps the discrimination; works better at low jet multiplicities than MT2W


\subsection{The baseline selection~\label{sec:baseline}}

Based on knowledge how the objects are selected or vetoed and  which kinematic variables help to suppress the SM background while keeping high signal yields, the baseline search region of the full 2016 analysis is defined as follows:

\begin{itemize}
\item One good selected primary vertex
\item Exactly one selected electron or muon
\item No veto lepton, isolated track or hadronic tau
\item At least two selected jets
\item At least on b-jet passing the medium WP 
\item \MET>250~GeV
\item $M_{T}$>150~GeV
\item min$\Delta \phi (j_{1,2}, E_{T}^{miss})$ > 0.8
\end{itemize}


\subsection{Definition of the signal regions~\label{sec:sr}}

In order to increase the sensitivity of the analysis to the signal, the baseline search region can be further divided into signal regions with the help of the discriminating variables defined in Section~\ref{sec:variables}. For the nominal search there are 8 signal region groups denoted from A to H, leading to 27 exclusive signal regions in total, which are based on selections on $N_{J}$, $t_{mod}$, $M_{\ell b}$ and \MET. To further suppress $W+jets \to 1\ell$ background, the tight b-tagging WP is used in regions where $M_{\ell b}>$175~GeV.  An additional group I of four signal regions is created for the T2tt compressed spectrum where 100<$\Delta m$<225~GeV. The definition of the signal regions is shown in Table~\ref{tab:SR}.


\begin{table}[h]
\begin{center}
\begin{tabular}{|l|lll|l|}
\hline
Group  &  $N_{J}$  & $t_{mod}$    &  $M_{\ell b}$ [GeV]     & \MET [GeV]                       \\
\hline
A      &  2-3      &   >10        &  $\leq$175              &  250-350, ~350-450, ~450-600, ~>600  \\
B      &  2-3      &   >10        &       >175              &  250-450, ~450-600, ~>600  \\
C      &  $\geq$4  &   $\leq$0    &  $\leq$175              &  250-350, ~350-450, ~450-550, ~550-650, ~>650  \\
D      &  $\geq$4  &   $\leq$0    &       >175              &  250-350, ~350-450, ~450-550, ~>550  \\
E      &  $\geq$4  &   0-10       &  $\leq$175              &  250-350, ~350-550, ~>550  \\
F      &  $\geq$4  &   0-10       &       >175              &  250-450, ~>450  \\
G      &  $\geq$4  &   >10        &  $\leq$175              &  250-350, ~350-450, ~450-600, ~>600  \\
H      &  $\geq$4  &   >10        &       >175              &  250-450, ~>450  \\
\hline
I      &  $\geq$5  &   --         &   --                    &  250-350, ~350-450, ~450-550, ~>550  \\
\hline
\end{tabular}
\caption[Table caption text]{The 27 signal regions of the nominal analysis divided into groups A to H. In the high $M_{\ell b}$ signal region groups B, D, F and H the b-tagged jet must pass the tight working point instead of the medium one. The group I is composed of 4 exclusive signal regions for the T2tt compressed analysis, satisfying additional cuts described in the text. }
\label{tab:SR}
\end{center}
\end{table}

In the compressed T2tt region the top quarks are produced off-shell or with a very low momentum, limiting the available momentum,  and therefore their decay products have very low \pt consequently leading to  low \MET. Obviously, such kind of signal has different kinematics and the baseline selection and categorization into SRs must be adjusted to account for it. To pass the \MET selection characteristic for SUSY signatures, the stop pair must be boosted against an initial state radiation~(ISR), leading to the requirement of  an additional jet in the event. The ISR jet should have the largest \pt, but such requirement does not help the overall signal discrimination as \MET>250~GeV is already required. The jet with the highest \pt must fail the medium b-tagging working point in order to increase the probability that it comes from the ISR. The stop system is expected to be boosted in the opposite direction with respect to the ISR jet and thus the direction of objects originating from the stop pair decay, like the \MET and the lepton, should have a similar direction. As the \pt of the objects in the compressed region is not large, putting a limit on lepton \pt is found to be useful for the signal discrimination. Taking these considerations into account the following additional criteria are imposed for the compressed regions I: 

\begin{itemize}
\item At least five jets
\item One lepton with $p_{T}^{\mathrm{lep}}$<150~GeV
\item The leading jet is not b-tagged
\item $\Delta \phi(E_{T}^{miss}, \ell)$<2.0 
\item min$\Delta \phi (j_{1,2}, E_{T}^{miss})$>0.5
\end{itemize}

In the previous versions of the analysis (2015~\cite{Sirunyan:2016jpr} and ICHEP2016~\cite{CMS:2016vew}), there was a lower number of signal regions, due to the smaller available data statistics. Also as discussed before in Section~\ref{sec:mt2w}, the $M_{T2}^{W}$ variable was used instead of $t_{mod}$ in the definition of the majority of the signal regions. In the presented full 2016 analysis, a new discriminating variable $M_{\ell b}$ is introduced. % and the signal regions were re-optimized with respect to the $N_{J}$ and \MET .


\section{Background estimations~\label{sec:estimations}}

In this section, the estimations of all relevant background yields in the signal regions together with their systematic uncertainties are discussed. The lost lepton background, coming from the $t\bar{t} \to 2 \ell$, single top ($tW$), $t\bar{t}V$ and diboson processes where one of the two leptons is missing, is the largest contribution in the signal regions with low $M_{\ell b}$ values and at least four jets. This background is estimated in control regions with two leptons~(dilepton CR). The one lepton background, represented by the $W+jets \to 1\ell$, $t\bar{t} \to 1\ell$, and single top (s- and t-channel) processes, is largely suppressed by the \MET and $M_{T}$ cuts. After these cuts, the yield of the $t\bar{t} \to 1\ell$ process in the signal regions is small and can be estimated from simulations. The $W+jets \to 1\ell$  process leads to a larger contribution especially in regions with high $M_{\ell b}$ and it is estimated in the control regions with zero b-tagged jets (0 b-tag CR). The $Z \to \nu \bar{\nu}$ background is the last relevant background group composed by the $t\bar{t}Z$ and $WZ$ processes, in which the Z boson decays to neutrinos and one W boson decays leptonically. The yields of $Z \to \nu \bar{\nu}$ in the signal regions are estimated from simulation, and corrected by an overall normalization factor measured in a three lepton control region.  

For the full 2016 version of analysis I was in charge to provide the estimation of the $Z \to \nu \bar{\nu}$ background. In this section the estimation of this background is therefore described first and then built on the knowledge of the $Z \to \nu \bar{\nu}$ background estimation method, the other background estimates are presented.


\subsection{The $Z \to \nu \bar{\nu}$ background estimation}

The $Z \to \nu \bar{\nu}$ background,  represents overall the smallest relevant group of backgrounds, but it is largely enhanced in signal regions with high \MET and high $t_{mod}$ values. The main contribution to the $Z \to \nu \bar{\nu}$ background comes from the $t\bar{t}Z$ process. However at low jet multiplicities, the $WZ$ process becomes an important part of this background.  The previous versions of this analysis estimated this background purely from simulation, because of the low statistics in the three lepton control region. Now with a larger dataset, the global normalization of these two processes is derived from a data-driven method, but the relative splitting of events between signal regions (shape) is taken from simulation. Nevertheless it is important to mention that in case that the shapes of the discriminating variables differ between data and simulation, the background estimation can be influenced by migration of events between the signal regions. Therefore the target is to define as many control regions and the signal ones, once there is sufficient statistics, which is unfortunately not the case for $Z \to \nu \bar{\nu}$ background. 

\subsubsection{Principle of the data driven estimate}

To explain the data-driven background estimation technique, let's consider only one signal region and one control region. The CR is a region enriched in events of the process under study and is typically defined similarly as the signal region, just changing a certain requirement. In the case of the $Z \to \nu \bar{\nu}$ background, the selection on the lepton multiplicity is modified: a three lepton control region is chosen to consider the Z~boson decay into two leptons instead of two neutrinos. The definition of control region is a trade-off between selection as close as possible to the SR to avoid extrapolation and the need to collect enough statistics. The relationship between the data~($N^{Data}$) and simulation~($N^{MC}$) yields in the SR and control CR can be written as follows

\eq{bkgEst}
{
\frac{N^{Data,~SR}}{N^{Data,~CR}}  = \frac{N^{MC,~SR}}{N^{MC,~CR}}.
}
Therefore the estimated background yield in the SR ($N^{Data_{Est},~SR}$) can be obtained from the data yield in control region ($N^{Data,~CR}$) and the simulation based transfer factor ($TF^{MC}$) transferring the yield from control to signal region. To achieve this, the Eq.~\ref{eq:bkgEst} can be rewritten as

\eq{bkgEstTF}
{
N^{Data_{Est},~SR}  = N^{Data,~CR} \times  \frac{N^{MC,~SR}}{ N^{MC,~CR}} = N^{Data,~CR} \times  TF^{MC}.
}

Alternatively, the Eq.~\ref{eq:bkgEst} can be rearranged in terms of simulated yield in the signal region ($N^{MC,~SR}$) and the normalization factor ($NF^{CR}$) determined in the CR
\eq{bkgEstNF}
{
N^{Data_{Est},~SR}  = N^{MC,~SR} \times  \frac{N^{Data,~CR}}{ N^{MC,~CR}} = N^{MC,~SR} \times  NF^{CR}.
}
However, in the analysis there is not only one single signal region and thus the background yield estimations in Eq.~\ref{eq:bkgEstTF} and \ref{eq:bkgEstNF} need to be performed for each signal region separately. 

Ideally the same number of control regions as of signal regions should be used to keep the kinematics as close as possible between the SR and CR. Consequently there should be the same number of normalization factors as of signal regions.  But the three lepton control regions suffer from a lack of statistics and therefore  only one control region can be used in the estimation of the $Z \to \nu \bar{\nu}$ background. In addition, ideally, one designed  CR should address the background estimate of only one process. The situation is a bit more complicated for the $Z \to \nu \bar{\nu}$ background as both the  $t\bar{t}Z$ and $WZ$ processes populate the CR.  Therefore in that case, two normalization factors, one for each process, are determined separately  from a template fit of the $N_{b}$ distribution in the three lepton control region as described in the following subsection. For the $Z \to \nu \bar{\nu}$ background the Eq.~\ref{eq:bkgEstNF} can be rewritten as

\eq{ZnunuEst}
{
N_{proc}^{Data_{Est},~SR}  = N_{proc}^{MC,~SR} \times NF_{proc,~3\ell}^{CR},
}
where $proc$ = $t\bar{t}Z$ or $WZ$.

%TODO start here

\subsubsection{Scale factors from the three lepton control region}

The normalization factors for the $t\bar{t}Z$ and  $WZ$ processes are taken from the  paper~\cite{Sirunyan:2017uyt} searching for BSM physics in final states with two leptons of same sign, \MET, and jets. For this purpose a three lepton control region is defined by the requirements:
\begin{itemize}
\item Two leptons passing the nominal isolation and identification criteria as defined in~\cite{Sirunyan:2017uyt}, the leading lepton should have \pt>25~GeV and the trailing one a \pt>20~GeV
\item A third lepton of \pt>10~GeV should pass the tight identification and must have an opposite sign and the same flavor as one of the leptons above; the formed lepton pair must have a mass within a $\pm$15~GeV window around the Z boson mass
\item $H_{T}$ = $\displaystyle\sum_{jets} p_{T}$>80~GeV
\item $N_{J} \geq 2$ with jet \pt>40~GeV
\item No requirement on b-tagging
\item \MET>~30~GeV.
\end{itemize}

The $t\bar{t}Z$ and  $WZ$ processes are expected to have a different number of b-jets and  therefore a fit of the b-jet multiplicity distribution allows to extract the normalization factors of $t\bar{t}Z$ and $WZ$. The CR bin with zero b-jet  is composed at $\sim$70\% by the $WZ$ process, while the $t\bar{t}Z$ process represents around 70\% of the rest of the b-jet distribution. In addition to these two processes, there is also a contribution to the control region from non-prompt leptons, the $t\bar{t}H$,   $t\bar{t}W$, and $WW$ processes as well as processes collectively denoted as ``rare SM'' and ``$X+\gamma$''. The rare SM contribution arises mainly from diboson (ZZ), triboson (WWW, WWZ, WZZ, ZZZ), Higgs (HZZ, VH where V=Z,W), same-sign WW from double-parton scattering (DPS WW), and rare top (tZq and $\mathrm{t\bar{t}t\bar{t}}$) processes decaying to three leptons. The ``$X+\gamma$'' group is composed by processes where one of the selected leptons is an electron from an unidentified photon conversion, such processes being predominantly $W\gamma$, $Z\gamma$, $t\bar{t}\gamma$ and $t\gamma$.

The distribution of the number of b-jets in data is simultaneously fitted by the discussed simulated components, letting only the normalization of different contributions free to vary. This fit procedure results in a normalization factor for $t\bar{t}Z$ of $1.14 \pm 0.30$ and for $WZ$ of $1.26 \pm 0.09$. The uncertainty on the normalization factors includes both the statistical uncertainty and an uncertainty on the shape of the b-jet distribution in the simulation. The post-fit distribution of number of b-jets in the three lepton control region is shown in Fig.~\ref{fig:figures/nbtagspostfit}.

    \insertFigure{figures/nbtagspostfit} % Filename = label
                 {0.4}       % Width, in fraction of the whole page width
                 { The post-fit distribution of the number of b-tagged jets in the three lepton control region~\cite{Sirunyan:2017uyt}. The distribution in data is simultaneously fitted by the SM background components to obtain the normalization factors for the $t\bar{t}Z$ and $WZ$ processes. The top panel shows the data-to-simulation ratio with an uncertainty composed by the statistical uncertainty and an uncertainty on the shape of the b-jet distribution in the simulation.  }

\subsubsection{Estimation of the $Z \to \nu \bar{\nu}$ background }

The final estimation of the $t\bar{t}Z$ and $WZ$ yields computed according to Eq.~\ref{eq:ZnunuEst} is shown in Table~\ref{tab:YZnunu}. The simulated events are corrected by the object scale factors for the different lepton and b-tagging efficiencies in data and simulation, which is a standard procedure for all background estimates. The  pileup distributions in data and  generated samples largely differ and therefore the simulated samples should be reweighted for the different pileup profiles in data and simulation. The reweighting procedure is applied for the lost lepton and $W+jets \to 1\ell$ backgrounds as these processes have sufficient MC statistics in the signal regions. However for signal and the backgrounds with small yields in the signal regions it would only unreasonably enlarge the statistical uncertainty on the yield.   Because the $WZ$ process is generated in the NLO, its yields in the signal regions can be negative. In this analysis the negative yields are set to zero. On top of this, the events in the $t\bar{t}Z$ sample are reweighted in order to improve the modeling of jets coming from the ISR. The uncertainties on the estimates include both the statistical and systematic uncertainties, which are described later in the following subsection.


\begin{table}[h]
\centering
%\noindent\hrulefill
%\smallskip\noindent
%\resizebox{0.7\linewidth}{!}{%
\scalebox{0.8}{
\begin{tabular}{|l|ccc|}
\hline
&
\textbf{ttZ}    &
\textbf{WZ}     &
\textbf{Total}  \\
\hline
\textbf{ A 250$<E_T^{miss}<$350}         & 3.33 $\pm$ 0.96       & 1.38 $\pm$ 0.58       & 4.71 $\pm$ 1.21       \\
\textbf{ A 350$<E_T^{miss}<$450}         & 1.85 $\pm$ 0.53       & 0.21 $\pm$ 0.53       & 2.05 $\pm$ 0.75       \\
\textbf{ A 450$<E_T^{miss}<$600}         & 1.15 $\pm$ 0.33       & 0.47 $\pm$ 0.39       & 1.62 $\pm$ 0.53       \\
\textbf{ A $E_T^{miss}>$600}     & 0.36 $\pm$ 0.11       & 0.36 $\pm$ 0.41       & 0.71 $\pm$ 0.40       \\
\textbf{ B 250$<E_T^{miss}<$450}         & 0.61 $\pm$ 0.18       & 0.93 $\pm$ 0.47       & 1.54 $\pm$ 0.52       \\
\textbf{ B 450$<E_T^{miss}<$600}         & 0.15 $\pm$ 0.05       & 0.20 $\pm$ 0.34       & 0.35 $\pm$ 0.33       \\
\textbf{ B $E_T^{miss}>$600}     & 0.09 $\pm$ 0.03       & 0.02 $\pm$ 0.26       & 0.11 $\pm$ 0.26       \\
\textbf{ C 250$<E_T^{miss}<$350}         & 14.28 $\pm$ 3.84      & 0.10 $\pm$ 0.62       & 14.38 $\pm$ 3.92      \\
\textbf{ C 350$<E_T^{miss}<$450}         & 3.83 $\pm$ 1.06       & 0.60 $\pm$ 0.48       & 4.43 $\pm$ 1.23       \\
\textbf{ C 450$<E_T^{miss}<$550}         & 1.06 $\pm$ 0.31       & 0.73 $\pm$ 0.39       & 1.79 $\pm$ 0.52       \\
\textbf{ C 550$<E_T^{miss}<$650}         & 0.40 $\pm$ 0.12       & 0.00 $\pm$ 0.00       & 0.40 $\pm$ 0.12       \\
\textbf{ C $E_T^{miss}>$650}     & 0.20 $\pm$ 0.06       & 0.00 $\pm$ 0.00       & 0.20 $\pm$ 0.06       \\
\textbf{ D 250$<E_T^{miss}<$350}         & 2.06 $\pm$ 0.56       & 0.95 $\pm$ 0.63       & 3.01 $\pm$ 0.91       \\
\textbf{ D 350$<E_T^{miss}<$450}         & 0.62 $\pm$ 0.18       & 0.55 $\pm$ 0.40       & 1.18 $\pm$ 0.41       \\
\textbf{ D 450$<E_T^{miss}<$550}         & 0.24 $\pm$ 0.07       & 0.21 $\pm$ 0.22       & 0.45 $\pm$ 0.24       \\
\textbf{ D $E_T^{miss}>$550}     & 0.09 $\pm$ 0.03       & 0.00 $\pm$ 0.00       & 0.09 $\pm$ 0.03       \\
\textbf{ E 250$<E_T^{miss}<$350}         & 7.88 $\pm$ 2.14       & 0.40 $\pm$ 0.44       & 8.27 $\pm$ 2.21       \\
\textbf{ E 350$<E_T^{miss}<$550}         & 3.34 $\pm$ 0.94       & 0.52 $\pm$ 0.39       & 3.87 $\pm$ 1.07       \\
\textbf{ E $E_T^{miss}>$550}     & 0.26 $\pm$ 0.08       & 0.03 $\pm$ 0.25       & 0.29 $\pm$ 0.26       \\
\textbf{ F 250$<E_T^{miss}<$450}         & 0.98 $\pm$ 0.27       & 0.13 $\pm$ 0.17       & 1.11 $\pm$ 0.33       \\
\textbf{ F $E_T^{miss}>$450}     & 0.21 $\pm$ 0.06       & 0.00 $\pm$ 0.00       & 0.21 $\pm$ 0.06       \\
\textbf{ G 250$<E_T^{miss}<$350}         & 2.89 $\pm$ 0.78       & 0.14 $\pm$ 0.15       & 3.03 $\pm$ 0.81       \\
\textbf{ G 350$<E_T^{miss}<$450}         & 2.51 $\pm$ 0.71       & 0.16 $\pm$ 0.18       & 2.67 $\pm$ 0.77       \\
\textbf{ G 450$<E_T^{miss}<$600}         & 1.80 $\pm$ 0.50       & 0.16 $\pm$ 0.16       & 1.96 $\pm$ 0.54       \\
\textbf{ G $E_T^{miss}>$600}     & 0.70 $\pm$ 0.20       & 0.00 $\pm$ 0.00       & 0.70 $\pm$ 0.23       \\
\textbf{ H 250$<E_T^{miss}<$450}         & 0.37 $\pm$ 0.11       & 0.00 $\pm$ 0.00       & 0.37 $\pm$ 0.11       \\
\textbf{ H $E_T^{miss}>$450}     & 0.33 $\pm$ 0.10       & 0.16 $\pm$ 0.17       & 0.49 $\pm$ 0.20       \\
\hline
\textbf{ I 250$<E_T^{miss}<$350}         & 3.66 $\pm$ 1.01       & 0.66 $\pm$ 0.43       & 4.33 $\pm$ 1.16       \\
\textbf{ I 350$<E_T^{miss}<$450}         & 1.57 $\pm$ 0.45       & 0.35 $\pm$ 0.34       & 1.93 $\pm$ 0.59       \\
\textbf{ I 450$<E_T^{miss}<$550}         & 0.58 $\pm$ 0.17       & 0.19 $\pm$ 0.20       & 0.77 $\pm$ 0.27       \\
\textbf{ I $E_T^{miss}>$550}     & 0.41 $\pm$ 0.13       & 0.17 $\pm$ 0.17       & 0.58 $\pm$ 0.22       \\
\hline
\end{tabular}}
\caption[Table caption text]{The estimated $t\bar{t}Z$ and $WZ$ yields in the signal regions A to I with their uncertainties. }
\label{tab:YZnunu}
\end{table}

\subsubsection{Systematic uncertainties on the $Z \to \nu \bar{\nu}$ background estimation}


In general, there are three kinds uncertainties entering into the background estimate. First is the uncertainty on the theoretical quantities used for the production of the simulated samples, second arise, for example, from the differences in modeling of the physics objects in data and simulation. The third category comprises the uncertainty due to the limited statistics in simulated and control data sample. Many uncertainties are common to all background estimates and therefore here again, due to my personal involvement, the systematic uncertainties for the $Z \to \nu \bar{\nu}$ background is described in a detail. The differences for other backgrounds are discussed later.

\textbf{Theoretical uncertainties}

The  ``theoretical uncertainties'' consists of uncertainties arising from the choice of the factorization and renormalization scale, particle distribution functions (PDFs) and the strong coupling constant ($\alpha_{S}$). As for the $Z \to \nu  \bar{\nu}$ processes the background yields in signal regions are taken from simulation and only normalized by an overall factor determined by the data-driven method, this background estimate is sensitive to the migrations of events between signal regions due to imprecise modeling of the theoretical parameters in the simulation. Because of the limited statistics in the signal regions, large fluctuations driven by the statistics are observed when varying the theoretical parameters. To avoid this, for the evaluation of the systematic uncertainties, the binning of the signal regions in $M_{\ell b}$ and $t_{mod}$ is removed and only a binning in \MET and $N_{J}$ is kept. Effectively, we group signal regions across the groups A--B and C--H, the compressed region I is treated separately. To be sure that the uncertainty is not underestimated due to the grouping in $M_{\ell b}$ and $t_{mod}$, the distributions of these variables are studied for different values of the theoretical parameters. Only small variations (less than 2\%) in the shape of $M_{\ell b}$ and $t_{mod}$ are observed when varying the theoretical parameters, while the variations are large in the shapes of \MET and $N_{J}$.  First the relative uncertainties are evaluated in the grouped regions for $t\bar{t}Z$ and $WZ$ separately, they are then used to compute the absolute uncertainties per process in the corresponding signal regions. The absolute uncertainties per process are propagated to evaluate the total absolute and relative uncertainty on the $Z \to \nu \bar{\nu}$ estimate in the signal regions.  The theoretical uncertainties are discussed one by one in the following paragraphs.


\begin{description}
\item[Factorization and renormalization scales]
The cross section of a given process has a remaining dependence on the choice of the factorization and renormalization scales. The theoretical $t\bar{t}Z$ cross section at 13~TeV is 839.3~fb~\cite{deFlorian:2016spz}, with a quoted scale uncertainty around 10\%. As the normalization of the background estimate was derived from data, the scale uncertainty should only influence the shapes and consequently the migration of events between the signal regions. In this analysis both renormalization and factorization scales are varied simultaneously, while keeping the cross section constant. The uncertainty was then evaluated from the change of background estimate when varying the scales. The scale uncertainty on the $Z \to \nu \bar{\nu}$ estimation in each signal region is shown in Table~\ref{tab:SysZnunu}, overall it is lower than 10\%.

%The central value for renormalization and factorization scales is set to 0=Mt+MV/2.
%For the NLO QCD part of the calculation scale uncertainties are estimated by independent variations of
%renormalization and factorization scales in the range /2R,F2, with 1/2R/F2,while for PDF and s uncertainties the PDF4LHC15 prescription is used. The resulting uncertainties are applied also to NLO EW correction effects
%str 156 handbook

\item[Parton distribution functions]
The NNPDF3.0~\cite{Ball:2014uwa} set of parton distribution functions~(PDF) was used for the production of simulated samples. To evaluate the uncertainty arising from the PDFs, 100 parameters of the PDFs were varied and each variation is used to calculate an event weight $y_{i,n}$, where $i$ denotes the i-th event and $n$ the n-th variation. Then the standard deviation of the PDF variations for an event $i$, computed as

\eq{pdfVar}
{
\sigma_{i} = \sqrt{\frac{1}{99} \sum_{n=1}^{100} (y_{i,n} - \bar{y}_{i})^{2}}, ~\mathrm{with}~ \bar{y}_{i} = \frac{1}{100} \sum_{n=1}^{100} y_{i,n},
}
is used to evaluate the PDF uncertainties. Here again only the shape uncertainties are taken into account by keeping the normalization constant. The uncertainties due to PDFs go up to 10\% as shown in Table~\ref{tab:SysZnunu} for the $Z \to \nu \bar{\nu}$ estimate in all signal regions.

\item[Strong coupling constant $\alpha_{S}$]
The uncertainties on the background estimate coming from $\alpha_{S}$ are as well evaluated with the help of event weights. The constant $\alpha_{S}$ is was varied from its default value of 0.118  within its uncertainty to 0.117 and 0.119~\cite{Patrignani:2016xqp}.  The interest is in the shape uncertainties and therefore again the normalization is forced to be constant when varying $\alpha_{S}$. The final uncertainties are of the order of 3\% and are show in Table~\ref{tab:SysZnunu} for $Z \to \nu \bar{\nu}$ background estimate.
\end{description}

Figure~\ref{fig:figures/ttZUnc} shows the  \MET distribution for the $t\bar{t}Z$ process in the grouped regions A--B~(left) and C--H(right) with a band corresponding to combined uncertainties on the scales, PDFs and $\alpha_{S}$ . It can be noticed that the uncertainties are very small for low \MET, but grow with \MET up to around 10\%.

    \insertTwoFigures{figures/ttZUnc}
                 {figures/AB250lessMETlessInfCanFrank} % Filename = label
                 {figures/CDEFGH250lessMETlessInfCanFrank} % Filename = label
                 {0.49}       % Width, in fraction of the whole page width
                 { The predicted \MET distribution of the $t\bar{t}Z$ process with a band coming from the combined uncertainties on scales, PDFs and $\alpha_{S}$. The regions A--B (left) and C--H (right) are grouped in the $M_{\ell b}$ and $t_{mod}$ variables. The ratio plots depict the maximal variations from the default. }

\textbf{Experimental uncertainties}

The second group of systematic uncertainties is referred to as the ``experimental uncertainties''. Part of the uncertainties originate from a non-perfect modeling in the simulation of the efficiency to tag a jet (coming from a b quark or from a light parton) as a b-jet, or of the lepton ID/isolation efficiencies. These mis-modelings are corrected in simulation by the use of scale factors delivered by the CMS collaboration.  The uncertainties on the background yield are evaluated from the variations of these scale factors within their uncertainties. The impact on the yields is presented in Table.~\ref{tab:SysZnunu} for the $Z \to \nu \bar{\nu}$ background estimate. The uncertainties on the b-tagging and lepton scale factors are typically of the order of a few percent, except for a few outliers caused by limited statistics in the simulated samples.

Another source of uncertainties arises from the Jet Energy Scale~(JES). Jet Energy Corrections~(JEC) must be applied on the reconstructed jets to obtain the correct energy of the jets. The uncertainties from the variations of the JES are of the order of a few percent, although in few signal regions with small statistics they can reach up to three or four tens of percent. 

The uncertainty arising from differences in the pileup profile in data and simulation is found to be extremely sensitive to the limited statistics and therefore, for the $Z \to  \nu \bar{\nu}$ processes, one global uncertainty is evaluated by varying the PU in the baseline search region only. The systematic uncertainty on the PU is determined to be lower than one percent in the baseline region, but a conservative estimate of 3\% is used to take into account variations due to categorization. 

As, for the $t\bar{t}Z$ process, a ISR reweighting is applied, the uncertainty arising from this procedure has to be evaluated by varying the ISR weight. The uncertainty introduced by the ISR reweighting is evaluated to be lower than 10\%. 

The systematic uncertainty on the luminosity is provided by the CMS and is  2.5\%~\cite{CMS-PAS-LUM-17-001}. The normalization uncertainty is computed by varying the normalization factors $NF_{proc,~3\ell}^{CR}$ within their uncertainty.

The dominant uncertainty comes from the limited statistics in the simulated samples. The statistical uncertainty varies from around 5\% to 90\%.

The break-down of the systematic uncertainties on the $Z \to \nu \bar{\nu}$  background estimate is shown in Table~\ref{tab:SysZnunu} for all signal regions. The summarized typical order of magnitude of these uncertainties is presented in Table~\ref{tab:SysZnunuSum}. As only the normalization factors were derived by a data-driven method and the shapes are taken from simulation, the uncertainties are very sensitive to the modeling of shape in the simulation. With one CR per each SR, a large part of the uncertainties would largely cancel in the $N^{MC,~SR}/ N^{MC,~CR}$ ratio. For now, as the statistics in the three lepton control region is small, only one CR can be used, but with a higher integrated luminosity, a splitting of the three lepton control regions should be possible.

%Like all experimentally-reconstructed objects, jets need to be calibrated in order to have the correct energy scale: this is the aim of the jet energy corrections (JEC). 

\begin{table}[h]
\begin{center}
\noindent\hrulefill
\smallskip\noindent
\resizebox{\linewidth}{!}{%
\begin{tabular}{|l|ccccccccccc|c|}
\hline
&
\textbf{MC stat.}  &
\textbf{$SF_{l}$}        &
\textbf{$SF_{b}$(light)}    &
\textbf{$SF_{b}$(heavy)}    &
\textbf{PU}         &
\textbf{PDFs}  &
\textbf{$\alpha_{S}$}       &
\textbf{Scales}   &
\textbf{ISR}   &
\textbf{JES}  &
\textbf{Norm.}        &
\textbf{Total}  \\
\hline
\textbf{ A 250$<E_T^{miss}<$350}         & 11.88         & 0.31          & 1.58          & 1.23          & 3.00          & 0.23          & 0.86          & 3.86          & 7.12          & 2.55          & 20.70         & 25.60         \\
\textbf{ A 350$<E_T^{miss}<$450}         & 25.04         & 0.35          & 3.63          & 0.77          & 3.00          & 3.33          & 0.61          & 1.31          & 9.09          & 0.37          & 24.38         & 36.61         \\
\textbf{ A 450$<E_T^{miss}<$600}         & 23.87         & 0.37          & 1.34          & 1.34          & 3.00          & 0.45          & 0.39          & 2.47          & 7.02          & 0.46          & 20.77         & 32.71         \\
\textbf{ A $E_T^{miss}>$600}    & 51.43         & 0.41          & 2.87          & 1.74          & 3.00          & 8.04          & 0.36          & 2.37          & 5.13          & 10.54         & 16.77         & 56.17         \\
\textbf{ B 250$<E_T^{miss}<$450}         & 29.98         & 0.50          & 1.67          & 1.23          & 3.00          & 1.17          & 0.76          & 2.86          & 3.97          & 1.01          & 14.74         & 34.01         \\
\textbf{ B 450$<E_T^{miss}<$600}         & 86.04         & 0.77          & 5.28          & 1.26          & 3.00          & 0.45          & 0.40          & 2.47          & 4.00          & 32.62         & 15.46         & 93.63         \\
\textbf{ B $E_T^{miss}>$600}    & 228.31        & 2.65          & 19.40         & 1.64          & 3.00          & 8.03          & 0.36          & 2.37          & 8.12          & 44.04         & 22.72         & 234.76        \\
\textbf{ C 250$<E_T^{miss}<$350}         & 4.56          & 0.45          & 0.06          & 0.68          & 3.00          & 0.81          & 0.60          & 1.77          & 0.40          & 4.78          & 26.18         & 27.26         \\
\textbf{ C 350$<E_T^{miss}<$450}         & 10.85         & 0.35          & 0.48          & 0.66          & 3.00          & 2.01          & 1.23          & 5.00          & 0.99          & 6.85          & 23.73         & 27.73         \\
\textbf{ C 450$<E_T^{miss}<$550}         & 20.70         & 0.31          & 4.53          & 0.26          & 3.00          & 1.89          & 1.34          & 4.86          & 2.03          & 4.52          & 18.54         & 29.25         \\
\textbf{ C 550$<E_T^{miss}<$650}         & 9.20          & 0.38          & 0.77          & 0.38          & 3.00          & 4.90          & 1.86          & 3.67          & 3.67          & 2.14          & 26.32         & 29.09         \\
\textbf{ C $E_T^{miss}>$650}    & 12.73         & 0.45          & 0.20          & 0.40          & 3.00          & 8.11          & 3.08          & 4.23          & 4.92          & 5.74          & 26.32         & 31.85         \\
\textbf{ D 250$<E_T^{miss}<$350}         & 19.82         & 0.31          & 0.72          & 0.41          & 3.00          & 0.81          & 0.60          & 1.76          & 0.82          & 9.76          & 20.24         & 30.20         \\
\textbf{ D 350$<E_T^{miss}<$450}         & 27.58         & 0.32          & 1.82          & 1.08          & 3.00          & 2.02          & 1.23          & 4.99          & 1.63          & 10.79         & 17.33         & 34.98         \\
\textbf{ D 450$<E_T^{miss}<$550}         & 47.40         & 0.33          & 4.96          & 0.22          & 3.00          & 1.88          & 1.33          & 4.85          & 2.24          & 17.87         & 17.31         & 54.17         \\
\textbf{ D $E_T^{miss}>$550}    & 18.94         & 0.44          & 0.06          & 0.83          & 3.00          & 7.53          & 2.44          & 2.55          & 7.31          & 7.15          & 26.32         & 35.14         \\
\textbf{ E 250$<E_T^{miss}<$350}         & 5.55          & 0.42          & 0.65          & 0.74          & 3.00          & 0.81          & 0.60          & 1.77          & 2.52          & 3.80          & 25.40         & 26.66         \\
\textbf{ E 350$<E_T^{miss}<$550}         & 10.40         & 0.40          & 0.01          & 0.83          & 3.00          & 1.97          & 1.26          & 4.96          & 0.66          & 7.23          & 23.72         & 27.63         \\
\textbf{ E $E_T^{miss}>$550}    & 87.72         & 0.77          & 7.05          & 0.02          & 3.00          & 7.55          & 2.47          & 2.51          & 2.26          & 1.66          & 24.61         & 91.86         \\
\textbf{ F 250$<E_T^{miss}<$450}         & 16.02         & 0.42          & 0.06          & 0.31          & 3.00          & 1.16          & 0.78          & 2.70          & 2.10          & 4.41          & 24.07         & 29.63         \\
\textbf{ F $E_T^{miss}>$450}    & 12.82         & 0.47          & 0.40          & 0.57          & 3.00          & 3.72          & 1.70          & 3.91          & 2.22          & 3.37          & 26.33         & 30.26         \\
\textbf{ G 250$<E_T^{miss}<$350}         & 5.75          & 0.39          & 0.01          & 1.71          & 3.00          & 0.81          & 0.60          & 1.77          & 2.09          & 3.44          & 25.42         & 26.68         \\
\textbf{ G 350$<E_T^{miss}<$450}         & 7.06          & 0.43          & 0.05          & 1.35          & 3.00          & 2.01          & 1.23          & 5.00          & 1.79          & 9.62          & 25.14         & 28.63         \\
\textbf{ G 450$<E_T^{miss}<$600}         & 9.01          & 0.37          & 0.19          & 1.27          & 3.00          & 2.63          & 1.54          & 4.72          & 0.51          & 5.15          & 24.77         & 27.64         \\
\textbf{ G $E_T^{miss}>$600}    & 7.00          & 0.47          & 0.19          & 1.01          & 3.00          & 8.01          & 2.34          & 1.74          & 0.68          & 15.62         & 26.32         & 32.70         \\
\textbf{ H 250$<E_T^{miss}<$450}         & 9.94          & 0.49          & 0.05          & 0.07          & 3.00          & 1.17          & 0.79          & 2.72          & 2.06          & 4.60          & 26.32         & 28.91         \\
\textbf{ H $E_T^{miss}>$450}    & 34.01         & 0.43          & 0.04          & 0.88          & 3.00          & 3.71          & 1.70          & 3.91          & 0.93          & 2.41          & 19.94         & 40.03         \\
\hline
\textbf{ I 250$<E_T^{miss}<$350}         & 9.76          & 0.38          & 0.78          & 0.47          & 3.00          & 0.21          & 0.32          & 3.91          & 4.30          & 5.32          & 23.38         & 26.72         \\
\textbf{ I 350$<E_T^{miss}<$450}         & 17.91         & 0.35          & 0.76          & 0.60          & 3.00          & 1.99          & 1.41          & 5.88          & 4.56          & 6.05          & 22.78         & 30.79         \\
\textbf{ I 450$<E_T^{miss}<$550}         & 25.04         & 0.51          & 4.83          & 0.35          & 3.00          & 1.89          & 1.62          & 4.90          & 5.38          & 4.89          & 21.64         & 34.80         \\
\textbf{ I $E_T^{miss}>$550}    & 29.62         & 0.40          & 0.22          & 0.68          & 3.00          & 1.82          & 1.55          & 7.48          & 4.76          & 6.34          & 20.76         & 37.98         \\
\hline
\end{tabular}}
\caption[Table caption text]{The break-down of the relative systematic uncertainties in \% on the $Z \to \nu \bar{\nu}$ background estimation in each signal region (first column). The columns two to twelve report the uncertainties on MC statistics, the scale factor for lepton, for b-tagging applied on light jets, on heavy-flavor jets (i.e. originating from  b or a c quark), pileup, PDFs, $\alpha_{S}$, scales, ISR jets, JES and normalization, respectively.   }
\label{tab:SysZnunu}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Source}       & \textbf{Typical range of values} [\%]                       \\
\hline
\textbf{MC statistics}      &  5-88   \\
\hline
\textbf{Lepton SF}              &  <1  \\
\textbf{b-tagging SF (light flavor)}      &  1-7  \\
\textbf{b-tagging SF (heavy flavor)}      &  <2  \\
\textbf{Pileup}                          &  3  \\
\textbf{PDFs}                 &  1-8  \\
\textbf{$\mathbf{\alpha_{S}}$}                 &  1-3  \\
\textbf{Scales}                 &  1-8  \\
\textbf{ISR}                 &  1-9  \\
\textbf{Jet energy scale}                 &  1-33  \\
\textbf{Normalization}                 &  15-26  \\
\hline
\textbf{Total}                 &  26-94  \\
\hline
\end{tabular}
\caption[Table caption text]{The typical order of magnitude of a given uncertainty for the $Z \to \nu \bar{\nu}$ background.  }
\label{tab:SysZnunuSum}
\end{center}
\end{table}

\newpage
\subsection{The lost lepton background estimation}

\subsubsection{Data-driven estimation}

The lost lepton background,  is the dominant background due to the large tails in the \MET and $M_{T}$ distributions caused by the two neutrinos and the missing lepton. The majority of the lost lepton background arises from the $t\bar{t} \to 2\ell$ process. The background yields in the signal regions are estimated according to Eq.~\ref{eq:bkgEstTF} from dilepton control regions. These control regions have a purity above 95\% in the $t\bar{t}$, $tW$, $t\bar{t}V$ and diboson processes and therefore no correction for contamination from other processes is needed. 

Event in the control regions pass the same selection as in the signal regions, except the veto on the second lepton which is reversed. Because of the small yields in the control regions with high \MET, several control regions differing only by the \MET range are merged together. Due to this combination of control regions, the background estimate in a given signal region has to be corrected by a corresponding interpolation factor. The lost lepton background estimate in the signal region can then be written as

\eq{bkgEstLL}
{
N^{Data_{Est},~SR}_{lost~\ell}  =  N^{Data,~CR,~E_{T}^{miss}~ext.}_{2\ell} \times  \frac{N^{MC,~SR}_{lost~\ell}}{ N^{MC,~CR}_{2\ell}} \times \frac{ N^{MC,~SR}_{lost~\ell}}{N^{MC,~SR,~E_{T}^{miss}~ext.}_{lost~\ell}},
}
where the $E_{T}^{miss~ext.}$ denotes regions extrapolated in \MET. The first term on the right side is the observed yield in the control region, the second term corresponds to the transfer factor from the CR to the SR  and the third factor is the simulation based \MET interpolation factor, which is equal to one if a given control region is not merged with another one.

The described extrapolation of control regions in the \MET could lead to an imprecise estimate if the \MET shapes in data and simulation differ. For this reason the shape of \MET is checked in an region enriched top quark events. Contrary to the control regions using single lepton triggers, data used for the study of the \MET shapes are triggered by a  double lepton trigger with an electron-muon pair in the final state. Then the data-to-simulation ratio in \MET bins is used as a correction of the simulated \MET spectra. 

The final estimates of the lost lepton background per signal region are shown in Table~\ref{tab:resultsAll}, together with their uncertainties including both the statistical and systematic uncertainties as described in following subsection.

\subsubsection{Systematic uncertainties on the lost lepton background estimation}

As for the $Z \to \nu \bar{\nu}$ background, the uncertainties on the lost lepton background estimate include the uncertainties due to limited statistics in simulation, lepton and b-tagging scale factors, PU,  PDFs, $\alpha_{S}$, scales, ISR reweighting and JES, although many of them largely cancel in the $N^{MC,~SR}_{lost \ell}/ N^{MC,~CR}_{2\ell}$ ratio used for the estimation of the lost lepton background yields in the SRs.

In addition to these uncertainties, the following uncertainties are evaluated:
\begin{itemize} 
\item The uncertainty arising from the trigger efficiency in the dilepton CR
\item The uncertainty on the \MET resolution which can cause a migration of events between signal regions: It cancels in the $N^{MC,~SR}_{lost \ell}/ N^{MC,~CR}_{2\ell}$ ratio in case no \MET extrapolation is needed. However in the case that the \MET extrapolation is applied, the corresponding interpolation factor is computed purely from the simulation and is directly sensitive to the \MET resolution.
\item The uncertainty arising from the different \MET shapes in data and simulation: Because of the extrapolation in \MET, the differences in \MET shapes  can also cause migration of events between the signal regions as described for the \MET resolution
\item The uncertainty on the efficiency of the hadronic tau and the isolated track vetos: As the control region is defined by requiring a second lepton, the differences of efficiency for the hadronically decaying tau leptons between signal and control regions must be taken into account. Any variation on these  efficiencies is propagated to the background yield to quantify its corresponding uncertainty.
\end{itemize}
As previously, the dominant uncertainty arises from the limited statistics in data and simulations which can go up to around 60\%.
%Additional correction factor obtained from studies of $\gamma$+jets processes is applied on background estimates to take into account differeneces in \MET resolution in data and simulation causing different migration between 
%An additional correction factor is used to take into account  ??!!
%gamma plus jets -> MET resolution > bin migrations - gamma pt spectrum reveighted to match neutrino pt spectrum, from reweighted events METmodified = reconstructed-MET + pTgamma -check that
%->ask about that -> not really clear from the papaer

\subsection{The one lepton background estimation}

\subsubsection{Data-driven estimation}

The $W+jets \to 1\ell$ background  represents the largest part of the $1\ell$ background and is estimated from control regions with zero b-tagged jets. The control regions are defined in the same way as the signal regions, but no b-tagged jet is allowed in the event. When the medium b-tagging WP is used in the SR, this requirement is simply inverted in the CR. For the SRs requesting a tight b-tagging WP, a veto against tight b-tagged jet is applied in the majority of cases.  But in some control regions of high $M_{\ell b}$, a veto against medium b-tagged jets is used to reduce the systematics uncertainty and increase the purity. The  $t\bar{t} \to 1\ell$ represents only a small contribution to the one lepton background and therefore is taken directly from the simulation.

In the zero b-tagged jet control regions, the $M_{\ell b}$ variable needs to be redefined as there is no b-tagged jet in the event. Instead of the closest b-tagged jet, the jet with highest CSVv2 discriminator is used to define $M_{\ell b}$ in this case. Contrary to the previous background estimations there is enough statistics to define a corresponding zero b-tag control region to each signal region. Unfortunately, the control regions are not pure in the $W+jets \to 1\ell$  process and the background estimate have to account for the contamination by other  processes in the control regions. The $W+jets \to 1\ell$  background estimate in the signal regions can be then written as

\eq{bkgEstWJ}
{
N^{Data_{Est},~SR}_{W+jets,~\geq~1btag}  =  N^{Data,~CR}_{W+jets,~0btag}\times  \frac{N^{MC,~SR}_{W+jets,~\geq~1btag}}{ N^{MC,~CR}_{W+jets,~0btag}},
}
where the first term on right side denotes the observed yield in the control region and the second term corresponds to the transfer factor from CR to SR for the W+jets process. Because of the contamination in the control regions by other processes, the $N^{Data,~CR}_{W+jets,~0btag}$ obtained from the yield measured in the CR ($N^{Data,~CR}_{all,~0btag}$) is corrected by a simulation based purity factor: 


\eq{bkgEstWJT2}
{
N^{Data,~CR}_{W+jets,~0btag} = N^{Data,~CR}_{all,~0btag} \times \frac{N^{MC,~CR}_{W+jets,~0btag}}{ N^{MC,~CR}_{all,~0btag}} ,
}
where $ N^{MC,~CR}_{all,~0btag} = N^{MC,~CR}_{W+jets,~0btag}$+ the contaminating processes.

Because of the different $M_{\ell b}$ definition, study based on simulation, comparing the shapes of this modified $M_{\ell b}$ between control and signal regions, confirms that these two shapes are in agreement. 

As the contribution to the total background from the $t\bar{t} \to 1\ell$ process is lower than 10\%, it is therefore reasonable to estimate the background yields directly from simulation. The $t\bar{t} \to 1\ell$ process appears in the signal regions mainly due to the \MET resolution. As this background is purely estimated from simulation, a mis-modeling of the \MET resolution could cause an imprecise estimate. The \MET resolution is checked with the help of $\gamma$+jets data and simulated samples. The \pt spectrum of the photon in data and simulation is reweighted to match the \pt spectrum of the neutrino in the $t\bar{t} \to 1\ell$ process. Then a modified \MET is computed as the sum of reweighted $p_{T}^{\gamma}$ and the originally reconstructed \MET. The data-to-simulation ratio for the modified \MET distribution can then reveal any \MET resolution mis-modeling. It was found out that the difference between data and simulation for the modified \MET is large, up to 40\%, but no scale factor is derived and this mis-modeling is only taken into account as a systematic uncertainty which is conservatively assessed to be 100\%. %really true? 
 
The final estimates of the one lepton background in the signal regions are shown in Table~\ref{tab:resultsAll}, together with their uncertainties including both the statistical and systematic uncertainties as described in the following subsection.

\subsubsection{Systematic uncertainties on the one lepton background estimation}

The uncertainties on the $W+jets \to 1\ell$  background estimate are very similar to the previous cases as they contain uncertainties due to the limited statistics in simulation, PU, JES, PDFs, scales, $\alpha_{S}$, b-tagging and lepton scale factors, although here as well many of them largely cancel in $N^{MC,~SR}_{W+jets,~\geq 1btag}/ N^{MC,~CR}_{W+jets,~0btag}$ ratio used for the estimation of $1\ell$ background yields in the SRs.

In addition these, already mentioned uncertainties, the following ones are evaluated:
\begin{itemize} 
\item The uncertainty on the W+heavy flavors ($W+b,~W+c,~W+bb$, ...) cross section: For the background estimation, the simulation is used to determine a factor which transfers the data yield from the control to the signal region as can be seen in Eq.~\ref{eq:bkgEstWJ}. Therefore the background estimate is sensitive on the modeling of the W+heavy flavors fraction in the simulation.
\item The uncertainty on the contamination of control regions by other processes.
\end{itemize}

In this case, the dominant uncertainties arise not only from the limited statistics in data and simulation but also from the contamination in  control regions by other  processes than $W+jets \to 1\ell$. These uncertainties can reach up to 90\%.

The uncertainty on the $t\bar{t} \to 1\ell$ estimate is evaluated to be 100\% from the mentioned studies of the \MET resolution. All the other sources of uncertainties are negligible.
%\subsection{Systematic uncertainties~\label{sec:systematics}}

%There are several uncertainties entering into the background estimations, one is the uncertainty due to limited statistics in simulated and control data samples and others arise for example from the differences in modeling of the physics objects in data and simulation. Many uncertainties are common to all background estimations and therefore here again, due to my personal involvement, the systematic uncertainties for the $Z \to \nu \bar{\nu}$ background are described first. Then the differences for other backgrounds are discussed.


\section{Results and interpretation~\label{sec:results}}

\subsection{Results}

The results on the background estimates together with the observed data in the signal regions are shown in Table~\ref{tab:resultsAll} for the full 2016 analysis. These results are graphically presented in Fig.~\ref{fig:figures/resultPlot}. In this figure the yields of three different signal models for a chosen signal point are also shown. 

\begin{table}[htb]
\footnotesize
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{$N_{J}$} & \multirow{2}{*}{$t_{mod}$} & $M_{\ell b}$ & \MET & Lost  & \multirow{2}{*}{1$\ell$ (top)} & 1$\ell$ (not & \multirow{2}{*}{$Z\rightarrow\nu\bar{\nu}$} & Total & \multirow{2}{*}{Data} \\
  &  &  [GeV] &  [GeV] &  lepton &  &  top) &  & background &  \\
\hline
$\leq3$ &    $>10$ & $\leq175$ & $250-350$ & 53.9$\pm$6.2 & $<0.1$ & 7.2$\pm$2.5 & 4.7$\pm$1.2 & 65.8$\pm$6.8 & 72 \\
$\leq3$ &    $>10$ & $\leq175$ & $350-450$ & 14.2$\pm$2.4 & 0.2$\pm$0.2 & 4.1$\pm$1.4 & 2.1$\pm$0.8 & 20.5$\pm$2.9 & 24 \\
$\leq3$ &    $>10$ & $\leq175$ & $450-600$ & 2.9$\pm$0.9 & 0.1$\pm$0.1 & 1.7$\pm$0.7 & 1.6$\pm$0.5 & 6.4$\pm$1.3 & 6 \\
$\leq3$ &    $>10$ & $\leq175$ &    $>600$ & 0.6$\pm$0.5 & 0.3$\pm$0.3 & 0.8$\pm$0.3 & 0.7$\pm$0.4 & 2.4$\pm$0.8 & 2 \\
\hline
$\leq3$ &    $>10$ &     $>175$ & $250-450$ & 1.7$\pm$0.8 & $<0.1$ & 5.6$\pm$2.2 & 1.5$\pm$0.5 & 8.9$\pm$2.4 & 6 \\
$\leq3$ &    $>10$ &     $>175$ & $450-600$ & 0.02$\pm$0.01 & $<0.1$ & 1.6$\pm$0.6 & 0.4$\pm$0.3 & 1.9$\pm$0.7 & 3 \\
$\leq3$ &    $>10$ &     $>175$ &    $>600$ & 0.01$\pm$0.01 & $<0.1$ & 0.9$\pm$0.4 & 0.1$\pm$0.3 & 1.0$\pm$0.5 & 2 \\
\hline
$\geq4$ & $\leq0$ & $\leq175$ & $250-350$ & 346$\pm$30 & 13.2$\pm$13.2 & 9.7$\pm$8.6 & 14.4$\pm$3.9 & 383$\pm$34 & 343 \\
$\geq4$ & $\leq0$ & $\leq175$ & $350-450$ & 66.3$\pm$7.9 & 2.3$\pm$2.3 & 2.5$\pm$1.7 & 4.4$\pm$1.2 & 75.5$\pm$8.5 & 68 \\
$\geq4$ & $\leq0$ & $\leq175$ & $450-550$ & 12.1$\pm$2.8 & 0.6$\pm$0.6 & 0.5$\pm$0.5 & 1.8$\pm$0.5 & 15.0$\pm$2.9 & 13 \\
$\geq4$ & $\leq0$ & $\leq175$ & $550-650$ & 3.4$\pm$1.5 & 0.1$\pm$0.1 & 0.3$\pm$0.2 & 0.4$\pm$0.1 & 4.1$\pm$1.5 & 6 \\
$\geq4$ & $\leq0$ & $\leq175$ &    $>650$ & 5.9$\pm$2.8 & $<0.1$ & 0.4$\pm$0.4 & 0.2$\pm$0.1 & 6.6$\pm$2.9 & 2 \\
\hline
$\geq4$ & $\leq0$ &     $>175$ & $250-350$ & 26.0$\pm$4.3 & 3.1$\pm$3.1 & 7.5$\pm$3.0 & 3.0$\pm$0.9 & 39.7$\pm$6.2 & 38 \\
$\geq4$ & $\leq0$ &     $>175$ & $350-450$ & 10.4$\pm$2.6 & 0.6$\pm$0.6 & 1.6$\pm$0.7 & 1.2$\pm$0.4 & 13.7$\pm$2.8 & 8 \\
$\geq4$ & $\leq0$ &     $>175$ & $450-550$ & 1.7$\pm$0.9 & 0.4$\pm$0.4 & 0.6$\pm$0.3 & 0.5$\pm$0.2 & 3.1$\pm$1.1 & 2 \\
$\geq4$ & $\leq0$ &     $>175$ &    $>550$ & 1.1$\pm$0.8 & $<0.1$ & 1.0$\pm$0.6 & 0.09$\pm$0.03 & 2.2$\pm$1.0 & 1 \\
\hline
$\geq4$ &   $0-10$ & $\leq175$ & $250-350$ & 43.0$\pm$5.9 & 1.7$\pm$1.7 & 5.7$\pm$3.0 & 8.3$\pm$2.2 & 58.7$\pm$7.2 & 65 \\
$\geq4$ &   $0-10$ & $\leq175$ & $350-550$ & 9.1$\pm$2.0 & 0.5$\pm$0.5 & 1.2$\pm$0.5 & 3.9$\pm$1.1 & 14.7$\pm$2.4 & 23 \\
$\geq4$ &   $0-10$ & $\leq175$ &    $>550$ & 0.6$\pm$0.3 & 0.3$\pm$0.3 & 0.3$\pm$0.2 & 0.3$\pm$0.3 & 1.5$\pm$0.6 & 1 \\
\hline
$\geq4$ &   $0-10$ &     $>175$ & $250-450$ & 4.4$\pm$1.4 & 0.3$\pm$0.3 & 3.1$\pm$1.3 & 1.1$\pm$0.3 & 8.9$\pm$1.9 & 9 \\
$\geq4$ &   $0-10$ &     $>175$ &    $>450$ & 0.10$\pm$0.17 & $<0.1$ & 0.2$\pm$0.3 & 0.2$\pm$0.1 & 0.6$\pm$0.2 & 0 \\
\hline
$\geq4$ &    $>10$ & $\leq175$ & $250-350$ & 9.5$\pm$2.3 & 0.8$\pm$0.8 & 1.1$\pm$0.9 & 3.0$\pm$0.8 & 14.3$\pm$2.7 & 12 \\
$\geq4$ &    $>10$ & $\leq175$ & $350-450$ & 5.9$\pm$1.8 & 0.7$\pm$0.7 & 0.7$\pm$0.5 & 2.7$\pm$0.8 & 10.0$\pm$2.1 & 9 \\
$\geq4$ &    $>10$ & $\leq175$ & $450-600$ & 3.8$\pm$1.3 & 0.1$\pm$0.1 & 0.4$\pm$0.3 & 2.0$\pm$0.5 & 6.3$\pm$1.5 & 3 \\
$\geq4$ &    $>10$ & $\leq175$ &    $>600$ & 0.8$\pm$0.6 & 0.7$\pm$0.7 & 0.3$\pm$0.4 & 0.7$\pm$0.3 & 2.4$\pm$1.0 & 0 \\
\hline
$\geq4$ &    $>10$ &     $>175$ & $250-450$ & 0.5$\pm$0.3 & $<0.1$ & 1.0$\pm$0.6 & 0.4$\pm$0.1 & 1.9$\pm$0.7 & 0 \\
$\geq4$ &    $>10$ &     $>175$ &    $>450$ & 0.2$\pm$0.2 & 0.1$\pm$0.1 & 0.5$\pm$0.3 & 0.5$\pm$0.2 & 1.3$\pm$0.4 & 2 \\
\hline
\hline
\multicolumn{3}{|l|}{Compressed region} & $250-350$ & 67.5$\pm$8.9 & 5.3$\pm$5.3 & 5.0$\pm$1.8 & 4.3$\pm$1.2 & 82$\pm$11 & 72 \\
\multicolumn{3}{|l|}{Compressed region} & $350-450$ & 15.1$\pm$3.5 & 1.0$\pm$1.0 & 0.8$\pm$0.3 & 1.9$\pm$0.6 & 18.9$\pm$3.7 & 30 \\
\multicolumn{3}{|l|}{Compressed region} & $450-550$ & 2.4$\pm$1.3 & 0.1$\pm$0.1 & 0.4$\pm$0.2 & 0.8$\pm$0.3 & 3.7$\pm$1.4 & 2 \\
\multicolumn{3}{|l|}{Compressed region} &    $>550$ & 3.9$\pm$2.0 & 0.1$\pm$0.1 & 0.2$\pm$0.2 & 0.6$\pm$0.2 & 4.8$\pm$2.0 & 2 \\
\hline
\end{tabular}
\caption{\label{tab:resultsAll} Observed data yields and background estimations  in the 31 signal regions of the analysis, corresponding to an  integrated luminosity of 35.9~fb$^{-1}$. The uncertainties of background include both the statistical and systematic parts~\cite{Sirunyan:2017xse}. }
\end{table}


\subsection{Uncertainties on the signal yields}

\subsubsection{Uncertainties for the full 2016 analysis}

The signal uncertainties are evaluated similarly as the background ones.  As previously, the largest source of uncertainty is the limited statistics in the simulated samples which varies from 5 to 25\%. The signal uncertainties with their typical values are summarized in Table~\ref{tab:systAll}.

\begin{table}[htb]
\centering
\begin{tabular}{lc}%c}
\hline\hline
Source & Typical range of values [\%] \\%& shape effect \\
\hline
MC statistics & 5--25 \\
b-tagging SF & 1--7 \\
Pileup & 1--4 \\
Scales & 2--4 \\
ISR & 2--15 \\
Jet energy scale & 1--20 \\
Integrated luminosity & 2.5 \\
Trigger & 2--4 \\
Lepton SF & 1--4 \\
\MET modeling & 1--10 \\
\hline
Total  & 7--38 \\
\hline\hline
\end{tabular}
\caption{\label{tab:systAll} The typical order of magnitude of systematic uncertainties on the signal yields~\cite{Sirunyan:2017xse}.}
\end{table}


\subsubsection{Discussion on a pileup uncertainty estimate for the 2015 analysis}

For the 2015 version of the analysis~\cite{Sirunyan:2016jpr} I performed studies of the dependency of the $M_{T}$ and \MET distributions in signal samples on the pileup. The results of this study were used to assess one of the systematic uncertainties on the signal yields. 


The difference in the $M_{T}$ and \MET distributions as a function of pileup was evaluated starting for events passing the baseline selection except the requirement on the $M_{T}$ and \MET variables, respectively. The total number of events $N$ was divided in bins according to  the number of primary vertices  ($N_{vtx}$) present in the event. In each  bin $i$ of the $N_{vtx}$ distribution, the number of events passing the criterion $M_{T}$>150~GeV~($N_{i,M_{T}>150}$) was divided by the total number of events~($N_{i}$) in bin $i$ and scaled by $N$. The same computation was performed for the number of events in bin $i$ passing \MET>250~GeV~($N_{i,E_{T}^{miss}>250}$). The evolution of the variables $N \cdot N_{i,M_{T}>150}/N_{i}$ and $N \cdot N_{i,E_{T}^{miss}>250}/N_{i}$ is shown in  Fig.~\ref{fig:figures/pileupUnc} as a function of $N_{vtx}$ for the T2tt signal points with $m_{\tilde{t}_{1}}$=225~GeV and 25<$M_{\tilde{\chi}_{1}^{0}}$<150~GeV. In the plots it can be noticed that  the distributions are not flat. Varying $N_{vtx}$ by five units, which is the difference between the maxima of data and simulation pileup profiles, leads to an uncertainty up to around 7\%. This value was used as an uncertainty for the low $\Delta m$ signals. The variations are smaller for high $\Delta m$ signals, leading to an uncertainty up to 2\%.

    \insertTwoFigures{figures/pileupUnc}
                 {figures/tailnrMT22525to150nNEW} % Filename = label
                 {figures/tailnrMET22525to150nNEW} % Filename = label
                 {0.49}       % Width, in fraction of the whole page width
                 { The evolution  of the fraction of events with $M_{T}>150$~GeV (left) and \MET>250~GeV (right), multiplied by the total number of events, as a function of the reconstructed primary vertex multiplicity. The study was performed in the context of the 2015 analysis. }


 In the full 2016 analysis, the pileup reweighting and uncertainty is applied according to standard CMS SUSY group recommendations~\cite{website:SUSYrec}.

\subsection{Interpretation}

In the signal regions of the full 2016 analysis, no significant excess is observed in data with respect to the SM expectation.  Therefore the $\mathrm{CL_{s}}$ method described in Section~\ref{sec:stats} is used to interpret the results as exclusion limits on the considered SMS signal models. These limits on model production cross-sections are given as a function of the SUSY particles masses, in this case the stop and neutralino masses. They are obtained by combining all signal regions for a given signal point. A contour line is drawn when the upper limit on the NLO+NLL cross section is equal to the theoretical cross section computed in~\cite{Borschensky:2014cia}. The signal points with an upper limit on the cross section which is lower than the theoretical cross section are considered as excluded. The signal yields entering the statistical procedure are corrected for a possible contamination in the control regions by SUSY signal, these corrections are around 5\% to 10\%.



    \insertFigure{figures/resultPlot} % Filename = label
                 {0.75}       % Width, in fraction of the whole page width
                 { The presentation of the observed data (black dots), background estimates and expected signal for three different signal models in the 31 signal regions. The lost lepton background is shown in green, the $1 \ell$ background from $W+jets \to 1\ell$ (i.e. not from top) in yellow, the $Z \to \nu \bar{\nu}$ background in violet and the $t\bar{t} \to 1\ell$ contribution in red. The T2tt model ($\tilde{t}_{1} \to t \tilde{\chi}^{0}_{1}$ ) with a stop mass of 900~GeV and an LSP mass of 300~GeV is shown in blue dashed line. The T2tb ($\tilde{t}_{1} \to t \tilde{\chi}^{0}_{1} /\tilde{t}_{1} \to b \tilde{\chi}^{\pm}_{1}$) and T2bW ($\tilde{t}_{1} \to b \tilde{\chi}^{\pm}_{1}$) scenarios for a stop mass of 600~GeV and a neutralino mass of 300~GeV are shown in pink and green dashed lines, respectively. The red dashed line separates the signal regions of the nominal and optimized compressed analyses~\cite{Sirunyan:2017xse}. }

The 95\% confidence level~(CL) upper limits  on the  T2tt ($ pp \to \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t \bar{t} \tilde{\chi}^{0}_{1} \tilde{\chi}^{0}_{1}$ )  and T2tb ($ pp \to \tilde{t}_{1} \bar{\tilde{t}}_{1} \to t b \tilde{\chi}^{0}_{1} \tilde{\chi}^{+}_{1}$ ) models are shown in Fig.~\ref{fig:figures/limitsT2tt} and \ref{fig:figures/limitsT2tb} respectively. The  results are displayed for both the 2015 analysis~\cite{Sirunyan:2016jpr} corresponding to the integrated luminosity of 2.3~fb$^{-1}$  (left) and the full 2016 analysis ~\cite{Sirunyan:2017xse} corresponding to the integrated luminosity of 35.9~fb$^{-1}$  (right). Both models assume unpolarized top quarks originating from stops.  

The analysis of 2015 data combines searches for the stop quark pair production in all-hadronic final states and in the $1\ell$ final state. Therefore in the left plots of Figs.~\ref{fig:figures/limitsT2tt} and \ref{fig:figures/limitsT2tb}, the expected limit from the $0\ell$~(magenta dashed line) and $1\ell$~(blue dashed line) are depicted. In the 2015 analysis, both observed~(black line) and expected~(red dashed line) limits are computed from the combination of the signal regions of both analyses, which are mutually exclusive. In the case of the T2tt model in Fig.~\ref{fig:figures/limitsT2tt}, the  $0\ell$ analysis outperforms the $1\ell$ analysis in the part of the mass plane where the stop is heavy and the LSP light, because of the larger signal acceptance. On the other hand the $1\ell$ analysis is more sensitive in the region of a heavier LSP and an intermediate stop mass. The combined analysis of data corresponding to an integrated luminosity of 2.3~fb$^{-1}$ excludes the stop masses between 280 and 830~GeV when the LSP is massless and LSP masses up to 260~GeV for a stop mass of 675~GeV. 

The full 2016 analysis corresponding to an integrated luminosity of 35.9~fb$^{-1}$ combines the four signal regions of the compressed analysis in the region 100 $\leq \Delta m \leq $ 225~GeV and the 27 signal regions of the nominal analysis everywhere else. The exclusion limits purely based on the $1\ell$ search exclude  stop masses up to 1120~GeV for a massless LSP and LSP masses up to 515~GeV for a stop mass of 950~GeV. 

In both analyses in case of the T2tt model, the region where the LSP is light and $\Delta m$ is around the mass of the top quark, appears as white in the T2tt plane and it is not interpreted because of a problematic modeling of quickly changing kinematics in this region. As mentioned, the signal samples are produced with FastSim and in this kinematic regime, large differences between FastSim and FullSim were observed. Therefore we have decided not to interpret in this region. Along the diagonal where the $\Delta m$ is around the mass of the top quark, the neutralinos are produced almost at rest, leading to  small \MET and therefore the signal final state is very similar to these of the SM backgrounds. The 2015 analysis has almost no sensitivity there. The full 2016 analysis improved highly its sensitivity in this region by requiring a boost of the stop pair against an ISR jet which is enhancing the \MET value of the event, but also due to an increase in the luminosity. A similar situation happens in the region where $\Delta m$ is around the mass of W boson, but moreover this region suffers from a  large contribution from the $W+jets \to 1\ell$ background. The sensitivity in this is also improved in the full 2016 analysis compared to the 2015 analysis due to an increase in the luminosity and  thanks to the compressed selection. 

Different signal regions bring different sensitivities in the $\Delta m$ plane. For example, the signal regions with high \MET are sensitive mainly to the signals with high $\Delta m$ and push the limits for high stop and low LSP masses. The signal regions are also differently sensitive to different signal decay modes, the signal regions with a low modified topness are more sensitive to the T2tt signals,  while the regions with a high modified topness  target signals with intermediate charginos as in the T2bW scenario. 

In the full 2016 analysis, the observed limit is stronger than the expected one. This is caused by upward fluctuations of the predicted SM backgrounds in the signal regions. In case of downward fluctuations, the expected exclusion limit would be stronger than the observed one.

    \insertTwoFigures{figures/limitsT2tt}
                 {figures/limitT2ttOLD} % Filename = label
                 {figures/limitT2tt} % Filename = label
                 {0.49}       % Width, in fraction of the whole page width
                 { The exclusion limits at 95\% CL on the T2tt model corresponding to an integrated luminosity of 2.3~fb$^{-1}$~\cite{Sirunyan:2016jpr} (left) and   35.9~fb$^{-1}$ ~\cite{Sirunyan:2017xse} (right). The interpretation is provided in the context of SMS, in the plane of the stop mass vs the LSP mass. The color code shows the 95\% CL upper limit on the cross section assuming a branching ratio of 100\% in $ \tilde{t}_{1} \to t  \tilde{\chi}^{0}_{1} $. The red and black contours represent the expected and observed exclusion limits, respectively. The blue and magenta lines in the left plot show the expected limits of $1 \ell$ and $0 \ell$ analyses, respectively.  The area below the black thick curve indicates the excluded region of the signal points.  }

The exclusion limits of the 2015 \cite{Sirunyan:2016jpr} and full 2106 analysis  \cite{Sirunyan:2017xse} are shown in Fig.~\ref{fig:figures/limitsT2tb} for the T2tb signal, for which the branching ratios of $ \tilde{t}_{1} \to t  \tilde{\chi}^{0}_{1} $ and $ \tilde{t}_{1} \to b  \tilde{\chi}^{\pm}_{1} $  are 50\% each and $m_{\tilde{\chi}_{1}^{\pm}} = m_{\tilde{\chi}_{1}^{0}} + 5~\mathrm{GeV}$. The combined analysis  corresponding to an integrated luminosity of 2.3~fb$^{-1}$ excludes stop masses up to 700~GeV for intermediate LSP masses, and  LSP masses up to 210~GeV for a stop mass of around 500~GeV. This exclusion is mainly driven by the fully hadronic analysis due to its larger signal acceptance. The full 2016 analysis~\cite{Sirunyan:2017xse}, corresponding to integrated luminosity of 35.9~fb$^{-1}$, excludes stop masses up to 980~GeV for a massless LSP and LSP masses up to 400~GeV for a stop mass of 825~GeV. There is a loss of sensitivity for regions of low $\Delta m$  where the neutralinos are produced at rest and which might suffer by large SM backgrounds.

    \insertTwoFigures{figures/limitsT2tb}
                 {figures/limitT2tbOLD} % Filename = label
                 {figures/limitT2tb} % Filename = label
                 {0.49}       % Width, in fraction of the whole page width
                 { The exclusion limits at 95\% CL on the T2tb model corresponding to an integrated luminosity of 2.3~fb$^{-1}$~\cite{Sirunyan:2016jpr} (left) and   35.9~fb$^{-1}$ ~\cite{Sirunyan:2017xse} (right). The interpretation is provided in the context of SMS, in the plane of the stop mass vs the LSP mass. The color code shows the 95\% CL upper limit on the cross section assuming a branching ratio of 50\% in $ \tilde{t}_{1} \to t  \tilde{\chi}^{0}_{1} $ and 50\% in $ \tilde{t}_{1} \to b  \tilde{\chi}^{\pm}_{1} $. The red and black contours represent the expected and observed exclusion limits, respectively. The blue and magenta lines in the left plot show the expected limits of $1 \ell$ and $0 \ell$ analyses, respectively.  The area below the black thick curve indicates the excluded region of the signal points.  }

The last considered model is T2bW where $ pp \to \tilde{t}_{1} \bar{\tilde{t}}_{1} \to b \tilde{\chi}^{+}_{1} \bar{b} \tilde{\chi}^{-}_{1} \to b W^{+} \tilde{\chi}^{0}_{1} \bar{b} W^{-} \tilde{\chi}^{0}_{1}$ in 100\% of the cases and  for which the chargino mass is fixed by the condition $m_{\tilde{\chi}_{1}^{\pm}} = ( m_{\tilde{t}} +  m_{\tilde{\chi}_{1}^{0}} )/2$. The 95\% CL upper limit on this model obtained by the full analysis~\cite{Sirunyan:2017xse} is shown in Fig.~\ref{fig:figures/limitT2bW}. In this model stop masses are excluded up to 1000~GeV for a massless LSP and LSP masses up to 450~GeV for a stop mass of 800~GeV.

    \insertFigure{figures/limitT2bW} % Filename = label
                 {0.5}       % Width, in fraction of the whole page width
                 { The exclusion limits at 95\% CL on the T2bW model corresponding to an integrated luminosity of 35.9~fb$^{-1}$~\cite{Sirunyan:2017xse}. The interpretation is provided in the context of the SMS, in the plane of the stop mass vs LSP mass. The color code shows the 95\% CL upper limit on the cross section. The red and black contours represent the expected and observed exclusion limits, respectively. The area below the black thick curve indicates the excluded region of the signal points.  }

The ATLAS searches for the stop pair production in the single lepton final state~\cite{Aaboud:2017aeu} based on data corresponding to an integrated luminosity of 36.1~fb$^{-1}$ exclude the stop masses up to 940~GeV for a massless LSP in the T2tt model. 

\section{Conclusion}

The excluded stop mass in the T2tt model was increased by around of 50~GeV in early Run~2 based on 2015 data compared to the strongest exclusion results of Run~1 presented in Fig.~\ref{fig:figures/T2tt2015}, despite the lower integrated luminosity. This increase in sensitivity is caused by the change of the center-of-mass energy from 8~TeV to 13~TeV as well as by an improvement in the optimization of the analyses. The analysis of data corresponding to an integrated luminosity of 35.9~fb$^{-1}$ outperforms the combined analysis of data corresponding to an integrated luminosity of 2.3~fb$^{-1}$ by almost 300~GeV in the stop mass exclusion of the T2tt and T2tb models for a massless LSP. The exclusion limit on the stop mass reaches the TeV range which is usually considered close to the naturalness boundary. But the exclusion of the stop masses around 1~TeV happens only for the low LSP masses and there are still configurations in the $\Delta m$ plane, which are uncovered and where the stops can have relatively low masses. Moreover, this limit is interpreted in terms of simplified model spectra and when considering more complex models the limits can become considerably weaker. For example Ref.~\cite{Baer:2012uy} suggests that natural SUSY models might lead to final states with very soft visible particle spectra which would therefore be very difficult to be extracted from the SM backgrounds, making the search for them at the LHC very challenging.%TODO remarks of Eric


%tables and plots
%Up to 2~TeV stops at HL-LHC
\newpage

\section{Perspectives}

As the exclusion limits on the stop mass are reaching the naturalness bound lot of effort is being recently made to revisit this bound and find natural SUSY scenarios which are not excluded by the experiments. The Ref.~\cite{Baer:2016bwh} shows that the mass of the lighter stop $(\tilde{t}_{1})$ of up to the 3~TeV can lead to the natural SUSY. This reference discusses that if an observable $O$ can be written as

\eq{finetuning}
{
O = a + b + f(b) + c,
}
the $O$ is claimed to be natural in the case that all terms on the right hand side are of comparable values or smaller than $O$. If they would be larger than the observable $O$, some degree of fine-tuning would be needed to cancel the large terms between each other in order to end up with small value of $O$. But in the Eq.~\ref{eq:finetuning} two terms which are $b$ and $f(b)$ are correlated and they can cancel each other without any fine-tuning, e.g. if $f(b) = -b$, the $b+f(b)$ is always zero independently on the value of $b$. Therefore when evaluating the fine-tuning, only independent terms should be compared between each other. The naturalness measure of the observable $O$ can be defined as 

\eq{ftm}
{
\Delta = \frac{\abs{\mathrm{largest~term~on~right~hand~side}}}{\abs{O}}.
}

The two terms on the right hand side of  Eq.~\ref{eq:HiggsMass} giving the Higgs boson mass, which are $m_{H,0}^{2}$ and $\Delta m_{H}^{2}$ are independent and to avoid the fine-tuning, each of them should be around the measured mass of the Higgs boson. Within the MSSM, the SM Higgs boson mass denoted as $m_{h}$ is given by relation

\eq{sustHiggs}
{
m_{h}^{2} \approx -2 \mu^{2} (\Lambda)+ m_{H_{u}}^{2} (\Lambda)+ \delta m_{H_{u}}^{2}(\Lambda),
}
where $\Lambda$ is the energy cut-off, $\mu$ is the Higgs mass parameter, $m_{H_{u}}((\Lambda))$ is the up-Higgs mass and $\delta m_{H_{u}}^{2}((\Lambda))$ is the radiative correction to the up-Higgs mass. In the Eq.~\ref{eq:sustHiggs} terms $m_{H_{u}}^{2} ((\Lambda))$ and  $\delta m_{H_{u}}^{2} ((\Lambda))$ are dependent, larger the $m_{H_{u}}^{2} ((\Lambda))$, larger is the canceling correction  $\delta m_{H_{u}}^{2} ((\Lambda))$. Therefore in evaluating the naturalness as in Eq.~\ref{eq:ftm}, only the combination of the dependent terms should appear in the numerator.

In the context of the MSSM the mass of the Z boson, when assuming $\beta \gtrsim 5 $, can be expressed as 

\eq{Zmass}
{
\frac{m_{Z}^{2}}{2} \simeq -m_{H_{u}}^{2} - \sum_{u}^{u} - \mu^{2},
}
where $\sum_{u}^{u}$ denotes 1-loop corrections and the three terms on the right hand side are mutually independent. The $\Delta_{EW}$ defined as

\eq{deltaEW}
{
\Delta_{EW} = \frac{\abs{\mathrm{largest~term~on~right~hand~side~of~Eq.~\ref{eq:Zmass}}}}{m_{Z}^{2}/2}.
}
gives a measure for fine-tuning of SUSY at the electroweak scale. Requiring the $\Delta_{EW}$<30, i.e. 3\% fine-tuning or better, the upper bound on the gluino mass is up to 4~TeV and the lighter stop mass up to 3~TeV. Two  lightest neutralinos and the lightest chargino are higgsino-like and their mass can go up to 300~GeV. These two neutralinos and the chargino are expected to be almost mass degenerate. The masses of the other charginos and neutralinos are expected to be at the TeV scale.

To compute the spectrum of the possible lighter stop vs. LSP masses, the two-extra-parameter non-universal Higgs model~(NUH2M)~\cite{Matalliotakis:1994ft, Nath:1997qm, Ellis:2002iu, Baer:2005bu} was used. The theoretical parameters in this model were varied within given boundary: $\Delta_{EW} <30$, the lightest chargino is heavier than 103.5~GeV, the masses of gluino and all other squarks,  except of $\tilde{t}_{1}$, are higher than excluded masses by the LHC during the Run~1, and the Higgs boson mass is of $125 \pm 2$~GeV. The possible configurations of the $\tilde{t}_{1}$ and LSP masses obtained by the described model are shown in Fig.~\ref{fig:figures/mz1mt1delew}. In this figure the possible scenarios with $\Delta_{EW} <15$ and $\Delta_{EW} <30$ are shown in red and blue crosses, respectively, together with the existing exclusion limits existing in 2016 which were derived by the CMS~\cite{CMS:2016hxa} (red line) and the ATLAS~\cite{ATLAS:2016jaa, ATLAS:2016ljb, ATLAS:2016xcm, Aaboud:2016tnv} (black line) collaborations. The predicted exclusion limit and the discovery potential to be reached by the ATLAS collaboration in the fully hadronic search assuming the integrated luminosity of $3000$fb$^{-1}$, which is expected at the HL-LHC, is shown in an orange curve. The black dotted line corresponds to the situation when the lighter stop and LSP have the same masses. In this figure it can be seen, that many signal scenarios were already excluded, but as well many scenarios will remain uncovered even analyzing the HL-LHC data. The predicted reach of the HL-LHC is up to around 1.4~TeV in the stop masses, however the natural SUSY scenarios predict stop masses up to 3~TeV. Also in can be noticed that this model does not predict many scenarios in the compressed region.  

    \insertFigure{figures/mz1mt1delew} % Filename = label
                 {0.7}       % Width, in fraction of the whole page width
                 { The plane of the $\tilde{t}_{1}$ vs LSP masses. The natural SUSY scenarios with $\Delta_{EW} <15$ and $\Delta_{EW} <30$ are shown in the red and blue crosses, respectively. In red, black and orange curves, the existing and predicted exclusion limits are depicted. The green curves shows discovery potential for the HL-LHC. The dotted line is drawn for the situation when the stop and the LSP have same masses~\cite{Baer:2016bwh}.   }

The Ref.~\cite{Baer:2016bwh} also examines prospects for the stop discovery and presents the exclusion potential for current and future colliders as depicted in Fig.~\ref{fig:figures/collidersstop}. The figure reveals, that as shown previously, the stop mass of 3~TeV will not be probed at current center-of-mass energies of 13~TeV. To probe such high stop masses, a machine delivering collisions at center-of-mass energy of 33~TeV would be needed. 

    \insertFigure{figures/collidersstop} % Filename = label
                 {0.45}       % Width, in fraction of the whole page width
                 { Potential for discovery (5$\sigma$) and exclusion (95\%) of the current LHC with center-of-mass energy of 13~TeV, HL-LHC with the center-of-mass energy of 13~TeV, HE-LHC with the center-of-mass energy of 33~TeV,  and a pp collider with the center-of-mass energy of 100~TeV (FCC) of the stop $\tilde{t}_{1}$ masses~\cite{Baer:2016bwh}.  }


One may also examine what are the expectations on the SUSY mass spectrum from the other measurements than the direct searches for the SUSY particles. One of the options is to study the branching ratio of the b quark to the s-quark and the photon BR($b \to s\gamma$).  This decay is possible in the SM via loops with W boson and top quark, within the SUSY other loops are possible as well. These are mainly loops with the  stop and the chargino, and loops with the b quark and the charged Higgs boson. The measured BR$(b \to s\gamma) = (3.01 \pm 0.22)$~\cite{Belle:2016ufb} can be compared with the prediction of the BR for each scenario in the $\tilde{t}_{1}$ vs LSP plane. The compatibility of the measured and predicted branching ratios excludes stop masses lower than 500~GeV. In the majority of scenarios with the stop mass higher than 1.5~TeV, the predicted and measured BR are in agreement within $2\sigma$~\cite{Baer:2016bwh}.

One can also obtain constraints on the masses of SUSY particles from the observed average cold dark matter density $\rho_{DM} \approx 1.2 \times 10^{-6} \mathrm{GeV/cm^{3}}$~\cite{Ade:2015xua, Patrignani:2016xqp}.  In the early universe there was a thermal equilibrium between the SM and SUSY particles. As the universe cooled down, heavier sparticles could not be produced anymore, they only annihilated into the SM particles or decayed to the LSP which is in this text considered to be a neutralino. Eventually LSPs also annihilated into the SM particles. With continuing expansion of the universe, the annihilation rate became small and the LSPs experienced a ``freeze-out''. If the LSP is mostly higgsino or wino, in order to reproduce the observed dark matter density, the LSP needs to be very heavy, of order of 1~TeV and more. This model is not yet excluded by the LHC, but on the other hand it is not natural. Another option is a bino-like LSP, but the mass of such LSP is in the majority of parameter space, given the constraint on the cold dark matter density,  already excluded by the LHC. The non-excluded bino-like LSP lead to too large cold dark matter density, which can be reduced for example by assuming that one of the sleptons is not too heavy.  But in this case the slepton is too light and large part of such light slepton models is already excluded at the LHC. The cold dark matter density for the bino-like LSP scenario can be also reduced in the compressed scenario, when the LSP mass is around at least mass of the top quark or is maximum 100~GeV heavier. The $\tilde{t}_{1}$ mass is required to be around 150~GeV larger then the mass of the LSP. In order to make this  model natural, the gluino and wino mass parameters ratio must be smaller than the prediction giving the unification of interactions~\cite{Martin:2008aw}. Although as described, many possible scenarios are already ruled out by the requirements on the cold dark matter density, there are still parts of the parameter space which are uncovered. Moreover, the described considerations assume only simple thermal mechanism for production of the LSP. If, for example, there is other production mechanisms or the R-parity is broken, the conclusions do not have to hold.

There is also a possibility that the MSSM is not the realization of the SUSY chosen by nature and we should examine the theories beyond the MSSM. For example in the MSSM the R-parity is conserved, which protects the proton to decay. But there is no fundamental reason that R-parity should be conserved and can be replaced by another symmetry, which also protects proton to decay. For example, such symmetry conserves baryon number but not the lepton one. In the case that the R-parity is violated, the LSP can decay and therefore the signal production and final states differ from the ones assuming the R-parity conservation. Both pair and single SUSY particle production is expected. Depending on the R-parity violation scenario, the LSP can decay to leptons or/and jets or not decay in the detector, resulting in the final states with large number of charged leptons, jets or \MET, or combination of these.

As discussed in the previous paragraphs, there are many models of SUSY which are already excluded. But on the other hand there are still many possibilities where to search for the SUSY. In was shown, that the lighter stop can have mass up to 3~TeV and be beyond the reach of the LHC. Or it might be needed to go beyond MSSM and focus our searches on more complicated models.

